Using folder_path = /work/gbadarac/MonoJet_NPLM/MonoJet_NPLM_analysis/Train_Ensembles/Train_Models/Sparker_kernels/EstimationKernels_outputs/2_dim/2d_bimodal_gaussian_heavy_tail/N_100000_dim_2_kernels_SparKer_models20_L5_K75_M270_Nboot100000_lr0.05_clip_10000000_different_seeds
Storing WiFi fit results in: /work/gbadarac/MonoJet_NPLM/MonoJet_NPLM_analysis/Uncertainty_Modeling/wifi/Fit_Weights/results_fit_weights_kernel/N_100000_dim_2_kernels_SparKer_models20_L5_K75_M270_Nboot100000_lr0.05_clip_10000000_different_seeds_2d_bimodal_gaussian_heavy_tail_latest_Sean_corrections_and_no_penalty_2
Loaded target data: (100000, 2)
Precomputing model_probs on CPU ...
model_probs stats: min 1.1678009998243512e-21 max 1.9011804923777558 finite True
model_probs_cpu computed: shape=(100000, 20)
w_init: [0.07538033 0.03930047 0.0505919  0.04967963 0.05823561 0.05028928
 0.03538485 0.04528684 0.05389446 0.04105662 0.04842162 0.04335925
 0.04620492 0.03315677 0.04668105 0.07271648 0.04922877 0.06129414
 0.04030427 0.06454291]
w_init sum: 1.0050101729364156
epoch 100 loss 3.959660e-01 |dL/du| 1.975e-05
epoch 100 sumw 1.000000
epoch 100 weights [ 0.1140973   0.23177236  0.1220064   0.0738938  -0.00615506  0.08089984
  0.04066028  0.10590581 -0.0594463  -0.003022    0.03476072 -0.03777032
  0.09246441  0.04249566 -0.10274273  0.08135119  0.08050524  0.01216231
  0.1547704  -0.05860931]
epoch 200 loss 3.959660e-01 |dL/du| 1.635e-07
epoch 200 sumw 1.000000
epoch 200 weights [ 0.11421442  0.23101138  0.12228033  0.07396016 -0.00624526  0.08102648
  0.04056616  0.10589334 -0.05985703 -0.00287549  0.03466909 -0.03796207
  0.09243422  0.04261514 -0.10257018  0.08127641  0.08099712  0.01227273
  0.15479709 -0.05850403]
epoch 300 loss 3.959660e-01 |dL/du| 3.688e-10
epoch 300 sumw 1.000000
epoch 300 weights [ 0.11421621  0.23101746  0.12228191  0.07396164 -0.00624444  0.08102838
  0.04056624  0.10589175 -0.05985859 -0.00287688  0.03466737 -0.03796286
  0.09243392  0.04261525 -0.10257337  0.08127614  0.0809981   0.01227156
  0.15479481 -0.0585046 ]
epoch 400 loss 3.959660e-01 |dL/du| 2.642e-12
epoch 400 sumw 1.000000
epoch 400 weights [ 0.11421621  0.23101746  0.12228191  0.07396163 -0.00624445  0.08102838
  0.04056624  0.10589176 -0.05985858 -0.0028769   0.03466739 -0.03796286
  0.09243391  0.04261525 -0.10257337  0.08127615  0.08099809  0.01227156
  0.15479482 -0.0585046 ]
epoch 500 loss 3.959660e-01 |dL/du| 1.396e-14
epoch 500 sumw 1.000000
epoch 500 weights [ 0.11421621  0.23101746  0.12228191  0.07396163 -0.00624445  0.08102838
  0.04056624  0.10589176 -0.05985858 -0.0028769   0.03466739 -0.03796286
  0.09243391  0.04261525 -0.10257337  0.08127615  0.08099809  0.01227156
  0.15479482 -0.0585046 ]
epoch 600 loss 3.959660e-01 |dL/du| 2.302e-15
epoch 600 sumw 1.000000
epoch 600 weights [ 0.11421621  0.23101746  0.12228191  0.07396163 -0.00624445  0.08102838
  0.04056624  0.10589176 -0.05985858 -0.0028769   0.03466739 -0.03796286
  0.09243391  0.04261525 -0.10257337  0.08127615  0.08099809  0.01227156
  0.15479482 -0.0585046 ]
epoch 700 loss 3.959660e-01 |dL/du| 4.881e-15
epoch 700 sumw 1.000000
epoch 700 weights [ 0.11421621  0.23101746  0.12228191  0.07396163 -0.00624445  0.08102838
  0.04056624  0.10589176 -0.05985858 -0.0028769   0.03466739 -0.03796286
  0.09243391  0.04261525 -0.10257337  0.08127615  0.08099809  0.01227156
  0.15479482 -0.0585046 ]
epoch 800 loss 3.959660e-01 |dL/du| 3.045e-15
epoch 800 sumw 1.000000
epoch 800 weights [ 0.11421621  0.23101746  0.12228191  0.07396163 -0.00624445  0.08102838
  0.04056624  0.10589176 -0.05985858 -0.0028769   0.03466739 -0.03796286
  0.09243391  0.04261525 -0.10257337  0.08127615  0.08099809  0.01227156
  0.15479482 -0.0585046 ]
epoch 900 loss 3.959660e-01 |dL/du| 7.448e-16
epoch 900 sumw 1.000000
epoch 900 weights [ 0.11421621  0.23101746  0.12228191  0.07396163 -0.00624445  0.08102838
  0.04056624  0.10589176 -0.05985858 -0.0028769   0.03466739 -0.03796286
  0.09243391  0.04261525 -0.10257337  0.08127615  0.08099809  0.01227156
  0.15479482 -0.0585046 ]
epoch 1000 loss 3.959661e-01 |dL/du| 3.699e-04
epoch 1000 sumw 1.000000
epoch 1000 weights [ 0.11435372  0.23107119  0.12240955  0.07412917 -0.0061886   0.08118863
  0.04069054  0.10600801 -0.05970472 -0.00272889  0.0348114  -0.03781476
  0.09263191  0.04277339 -0.10245252  0.081448    0.08115379  0.01229227
  0.15488564 -0.06095772]
epoch 1100 loss 3.959660e-01 |dL/du| 1.346e-06
epoch 1100 sumw 1.000000
epoch 1100 weights [ 0.11421723  0.2310177   0.12228283  0.07396279 -0.00624411  0.08102961
  0.04056711  0.10589277 -0.05985745 -0.0028758   0.03466842 -0.03796205
  0.09243513  0.04261632 -0.10257255  0.08127705  0.08099895  0.01227166
  0.15479562 -0.05852123]
epoch 1200 loss 3.959660e-01 |dL/du| 5.408e-09
epoch 1200 sumw 1.000000
epoch 1200 weights [ 0.11421623  0.23101746  0.12228193  0.07396165 -0.00624444  0.08102841
  0.04056626  0.10589178 -0.05985856 -0.00287688  0.03466741 -0.03796284
  0.09243394  0.04261527 -0.10257335  0.08127618  0.08099811  0.01227157
  0.15479484 -0.05850496]
epoch 1300 loss 3.959660e-01 |dL/du| 7.750e-11
epoch 1300 sumw 1.000000
epoch 1300 weights [ 0.11421621  0.23101746  0.12228191  0.07396163 -0.00624445  0.08102838
  0.04056624  0.10589176 -0.05985858 -0.0028769   0.03466739 -0.03796286
  0.09243391  0.04261525 -0.10257337  0.08127615  0.08099809  0.01227156
  0.15479482 -0.0585046 ]
epoch 1400 loss 3.959660e-01 |dL/du| 4.855e-13
epoch 1400 sumw 1.000000
epoch 1400 weights [ 0.11421621  0.23101746  0.12228191  0.07396163 -0.00624445  0.08102838
  0.04056624  0.10589176 -0.05985858 -0.0028769   0.03466739 -0.03796286
  0.09243391  0.04261525 -0.10257337  0.08127615  0.08099809  0.01227156
  0.15479482 -0.0585046 ]
epoch 1500 loss 3.959661e-01 |dL/du| 4.398e-04
epoch 1500 sumw 1.000000
epoch 1500 weights [ 0.11396419  0.23092433  0.12207962  0.07373483 -0.00635081  0.08080619
  0.04039814  0.10569389 -0.06008777 -0.00312191  0.03443568 -0.03820344
  0.09222476  0.04237599 -0.10276281  0.08103265  0.0807659   0.01222196
  0.15462195 -0.05475335]
epoch 1600 loss 3.959660e-01 |dL/du| 3.652e-06
epoch 1600 sumw 1.000000
epoch 1600 weights [ 0.11422228  0.2310199   0.12228763  0.07396764 -0.00624135  0.08103492
  0.04057239  0.10589762 -0.05985298 -0.00287077  0.03467328 -0.03795683
  0.09244048  0.04262136 -0.1025679   0.08128262  0.08100435  0.01227282
  0.1547997  -0.05860717]
epoch 1700 loss 3.959660e-01 |dL/du| 2.139e-08
epoch 1700 sumw 1.000000
epoch 1700 weights [ 0.11421619  0.23101745  0.12228189  0.07396161 -0.00624446  0.08102836
  0.04056621  0.10589174 -0.0598586  -0.00287692  0.03466736 -0.03796289
  0.09243389  0.04261522 -0.10257339  0.08127613  0.08099806  0.01227156
  0.1547948  -0.0585042 ]
epoch 1800 loss 3.959660e-01 |dL/du| 7.739e-11
epoch 1800 sumw 1.000000
epoch 1800 weights [ 0.11421621  0.23101746  0.12228191  0.07396163 -0.00624445  0.08102838
  0.04056624  0.10589176 -0.05985858 -0.0028769   0.03466739 -0.03796286
  0.09243391  0.04261525 -0.10257337  0.08127615  0.08099809  0.01227156
  0.15479482 -0.0585046 ]
epoch 1900 loss 3.959660e-01 |dL/du| 2.296e-11
epoch 1900 sumw 1.000000
epoch 1900 weights [ 0.11421621  0.23101746  0.12228191  0.07396163 -0.00624445  0.08102838
  0.04056624  0.10589176 -0.05985858 -0.0028769   0.03466739 -0.03796286
  0.09243391  0.04261525 -0.10257337  0.08127615  0.08099809  0.01227156
  0.15479482 -0.0585046 ]
epoch 2000 loss 3.959661e-01 |dL/du| 1.340e-04
epoch 2000 sumw 1.000000
epoch 2000 weights [ 0.11394743  0.23087355  0.12201675  0.0736842  -0.00640728  0.08074213
  0.04030238  0.10562024 -0.0601396  -0.00315869  0.03438805 -0.03822294
  0.09215657  0.04234273 -0.10282414  0.08100651  0.08073098  0.01220328
  0.15453196 -0.05379412]
Saved final_weights.npy and loss_history_wifi.npy
p_raw min/med/max: 8.202e-06 / 8.706e-01 / 1.718e+00
p     min/med/max: 6.932e-01 / 1.220e+00 / 1.883e+00
sum(w): 1.000000
L1(w):  1.529094e+00
max|w|: 2.308735e-01
fraction p_raw < 0: 0.0000
Hessian eigenvalues are [4.25448554e-03 4.69083837e-03 4.93048935e-03 5.44606093e-03
 5.63278153e-03 6.06039981e-03 6.56392387e-03 7.08354730e-03
 7.81028187e-03 8.24725916e-03 9.18817765e-03 9.75358974e-03
 1.06083756e-02 1.19668528e-02 1.51728683e-02 2.66008276e-02
 4.58015285e-02 4.95463560e-02 1.59372971e-01 2.00288221e+01]
All done.
