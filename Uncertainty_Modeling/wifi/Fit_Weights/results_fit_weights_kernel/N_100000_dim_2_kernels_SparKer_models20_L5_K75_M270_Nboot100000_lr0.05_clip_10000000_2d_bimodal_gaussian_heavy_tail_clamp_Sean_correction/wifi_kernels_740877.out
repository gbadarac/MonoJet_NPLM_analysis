Using folder_path = /work/gbadarac/MonoJet_NPLM/MonoJet_NPLM_analysis/Train_Ensembles/Train_Models/Sparker_kernels/EstimationKernels_outputs/2_dim/2d_bimodal_gaussian_heavy_tail/N_100000_dim_2_kernels_SparKer_models20_L5_K75_M270_Nboot100000_lr0.05_clip_10000000
Storing WiFi fit results in: /work/gbadarac/MonoJet_NPLM/MonoJet_NPLM_analysis/Uncertainty_Modeling/wifi/Fit_Weights/results_fit_weights_kernel/N_100000_dim_2_kernels_SparKer_models20_L5_K75_M270_Nboot100000_lr0.05_clip_10000000_2d_bimodal_gaussian_heavy_tail_clamp_valid_weights_mixture_no_non_negativity_Sean_correction
Loaded target data: (100000, 2)
Precomputing model_probs on CPU ...
model_probs stats: min 2.1394534041273488e-14 max 1.8003042606422504 finite True
model_probs_cpu computed: shape=(100000, 20)
epoch 100 loss -3.615109e+00 |dL/dw| 8.087e-02
epoch 100 loss -3.615109 weights [2.78079537 2.7807956  2.7807959  2.78079573 2.78079446 2.78079577
 2.78079549 2.78079591 2.78079599 2.78079612 2.78079476 2.78079502
 2.78079455 2.78079466 2.78079534 2.78079551 2.78079577 2.78079513
 2.78079598 2.78079564] sumw 55.615909
epoch 200 loss -4.071972e+00 |dL/dw| 5.118e-02
epoch 200 loss -4.071972 weights [4.38064866 4.38064932 4.3806502  4.38064969 4.38064599 4.38064983
 4.38064901 4.38065024 4.38065045 4.38065085 4.38064685 4.38064761
 4.38064623 4.38064656 4.38064856 4.38064905 4.38064981 4.38064794
 4.38065044 4.38064945] sumw 87.612977
epoch 300 loss -4.359215e+00 |dL/dw| 3.838e-02
epoch 300 loss -4.359215 weights [5.83388187 5.83388312 5.83388477 5.83388381 5.83387685 5.83388407
 5.83388253 5.83388485 5.83388524 5.83388599 5.83387847 5.83387991
 5.83387732 5.83387793 5.83388168 5.83388262 5.83388404 5.83388052
 5.83388522 5.83388336] sumw 116.677644
epoch 400 loss -4.570106e+00 |dL/dw| 3.106e-02
epoch 400 loss -4.570106 weights [7.20153548 7.20153744 7.20154004 7.20153853 7.20152759 7.20153894
 7.20153651 7.20154015 7.20154078 7.20154195 7.20153014 7.20153239
 7.20152833 7.20152929 7.20153518 7.20153665 7.20153889 7.20153336
 7.20154074 7.20153781] sumw 144.030720
epoch 500 loss -4.737516e+00 |dL/dw| 2.626e-02
epoch 500 loss -4.737516 weights [8.51326838 8.51327117 8.51327487 8.51327272 8.51325716 8.51327331
 8.51326985 8.51327504 8.51327592 8.5132776  8.51326078 8.51326399
 8.5132582  8.51325958 8.51326796 8.51327005 8.51327324 8.51326537
 8.51327587 8.51327171] sumw 170.265383
epoch 600 loss -4.876851e+00 |dL/dw| 2.283e-02
epoch 600 loss -4.876851 weights [9.78632074 9.78632447 9.78632943 9.78632655 9.78630572 9.78632733
 9.78632271 9.78632964 9.78633083 9.78633307 9.78631057 9.78631487
 9.78630712 9.78630896 9.78632018 9.78632297 9.78632724 9.78631671
 9.78633077 9.78632519] sumw 195.726435
epoch 700 loss -4.996573e+00 |dL/dw| 2.025e-02
epoch 700 loss -4.996573 weights [11.0318918  11.03189659 11.03190294 11.03189926 11.03187254 11.03190025
 11.03189433 11.03190322 11.03190475 11.03190762 11.03187876 11.03188427
 11.03187433 11.03187669 11.03189108 11.03189466 11.03190014 11.03188663
 11.03190466 11.03189751] sumw 220.637862
epoch 800 loss -5.101828e+00 |dL/dw| 1.822e-02
epoch 800 loss -5.101828 weights [12.25783192 12.25783787 12.25784577 12.25784119 12.25780798 12.25784243
 12.25783506 12.25784612 12.25784802 12.25785159 12.2578157  12.25782256
 12.2578102  12.25781313 12.25783103 12.25783548 12.25784229 12.25782549
 12.25784791 12.25783902] sumw 245.156671
epoch 900 loss -5.195981e+00 |dL/dw| 1.657e-02
epoch 900 loss -5.195981 weights [13.46996078 13.46996801 13.4699776  13.46997204 13.4699317  13.46997354
 13.46996459 13.46997803 13.46998033 13.46998467 13.46994109 13.46994941
 13.46993441 13.46993797 13.4699597  13.4699651  13.46997337 13.46995298
 13.4699802  13.4699694 ] sumw 269.399255
epoch 1000 loss -5.281351e+00 |dL/dw| 1.521e-02
epoch 1000 loss -5.281351 weights [14.6727839  14.67279252 14.67280396 14.67279732 14.67274924 14.67279911
 14.67278845 14.67280446 14.6728072  14.67281237 14.67276043 14.67277035
 14.67275247 14.67275671 14.67278261 14.67278905 14.67279891 14.6727746
 14.67280705 14.67279418] sumw 293.455725
epoch 1100 loss -5.359607e+00 |dL/dw| 1.406e-02
epoch 1100 loss -5.359607 weights [15.86991337 15.86992349 15.86993693 15.86992914 15.86987268 15.86993124
 15.86991871 15.86993752 15.86994073 15.86994681 15.86988581 15.86989746
 15.86987646 15.86988144 15.86991186 15.86991942 15.869931   15.86990245
 15.86994056 15.86992544] sumw 317.398323
epoch 1200 loss -5.431987e+00 |dL/dw| 1.307e-02
epoch 1200 loss -5.431987 weights [17.0643301  17.06434184 17.06435742 17.06434838 17.0642829  17.06435082
 17.06433629 17.0643581  17.06436184 17.06436888 17.06429813 17.06431165
 17.06428729 17.06429306 17.06432834 17.06433711 17.06435055 17.06431743
 17.06436163 17.0643441 ] sumw 341.286666
epoch 1300 loss -5.499437e+00 |dL/dw| 1.221e-02
epoch 1300 loss -5.499437 weights [18.25855517 18.25856864 18.25858653 18.25857616 18.25850098 18.25857896
 18.25856228 18.25858732 18.2585916  18.25859969 18.25851847 18.25853398
 18.25850602 18.25851265 18.25855315 18.25856322 18.25857864 18.25854063
 18.25859136 18.25857124] sumw 365.171177
epoch 1400 loss -5.562694e+00 |dL/dw| 1.146e-02
epoch 1400 loss -5.562694 weights [19.45476629 19.45478162 19.45480198 19.45479017 19.45470462 19.45479336
 19.45477438 19.45480287 19.45480775 19.45481696 19.45472452 19.45474218
 19.45471036 19.4547179  19.454764   19.45477545 19.454793   19.45474974
 19.45480748 19.45478457] sumw 389.095409
epoch 1500 loss -5.622347e+00 |dL/dw| 1.079e-02
epoch 1500 loss -5.622347 weights [20.65487948 20.6548968  20.65491979 20.65490646 20.65480983 20.65491006
 20.65488862 20.65492081 20.65492631 20.65493672 20.65483231 20.65485225
 20.65481631 20.65482483 20.65487689 20.65488983 20.65490965 20.65486079
 20.65492601 20.65490014] sumw 413.097684
epoch 1600 loss -5.678870e+00 |dL/dw| 1.019e-02
epoch 1600 loss -5.678870 weights [21.86060796 21.86062739 21.86065319 21.86063823 21.86052979 21.86064227
 21.86061821 21.86065433 21.86066051 21.86067218 21.86055502 21.8605774
 21.86053706 21.86054662 21.86060505 21.86061957 21.86064181 21.86058698
 21.86066017 21.86063113] sumw 437.212265
epoch 1700 loss -5.732652e+00 |dL/dw| 9.651e-03
epoch 1700 loss -5.732652 weights [23.07350558 23.07352727 23.07355605 23.07353936 23.07341835 23.07354387
 23.07351702 23.07355732 23.07356422 23.07357724 23.0734465  23.07347148
 23.07342646 23.07343714 23.07350233 23.07351854 23.07354336 23.07348217
 23.07356384 23.07353144] sumw 461.470230
epoch 1800 loss -5.784016e+00 |dL/dw| 9.164e-03
epoch 1800 loss -5.784016 weights [24.29499959 24.29502367 24.29505563 24.29503709 24.29490274 24.2950421
 24.29501229 24.29505704 24.2950647  24.29507916 24.294934   24.29496173
 24.29491175 24.2949236  24.29499598 24.29501398 24.29504153 24.2949736
 24.29506427 24.2950283 ] sumw 485.900123
epoch 1900 loss -5.833233e+00 |dL/dw| 8.719e-03
epoch 1900 loss -5.833233 weights [25.5264157  25.52644232 25.52647765 25.52645716 25.52630865 25.52646269
 25.52642975 25.5264792  25.52648767 25.52650365 25.5263432  25.52637385
 25.52631861 25.52633171 25.52641172 25.52643161 25.52646207 25.52638697
 25.5264872  25.52644744] sumw 510.528459
epoch 2000 loss -5.880533e+00 |dL/dw| 8.312e-03
epoch 2000 loss -5.880533 weights [26.76899763 26.76902694 26.76906584 26.76904328 26.76887977 26.76904937
 26.7690131  26.76906755 26.76907687 26.76909447 26.76891781 26.76895156
 26.76889074 26.76890516 26.76899325 26.76901514 26.76904868 26.768966
 26.76907636 26.76903258] sumw 535.380112
Saved final_weights.npy and loss_history_wifi.npy
All done.
