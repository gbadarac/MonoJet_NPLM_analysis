Using folder_path = /work/gbadarac/MonoJet_NPLM/MonoJet_NPLM_analysis/Train_Ensembles/Train_Models/Sparker_kernels/EstimationKernels_outputs/2_dim/2d_bimodal_gaussian_heavy_tail/N_100000_dim_2_kernels_SparKer_models32_L5_K75_M270_Nboot100000_lr0.05_clip_10000000_no_masking_narrower_widths
Storing WiFi fit results in: /work/gbadarac/MonoJet_NPLM/MonoJet_NPLM_analysis/Uncertainty_Modeling/wifi/Fit_Weights/results_fit_weights_kernel/N_100000_dim_2_kernels_SparKer_models32_L5_K75_M270_Nboot100000_lr0.05_clip_10000000_no_masking_narrower_widths_2d_bimodal_gaussian_heavy_tail_ensemblecomponents20
Loaded target data: (100000, 2)
Precomputing model_probs on CPU ...
model_probs stats: min 1.2664885127734468e-30 max 2.0028813697912793 finite True
model_probs_cpu computed: shape=(100000, 20)
w_init: [0.03298892 0.04502728 0.04247756 0.0674408  0.05303844 0.04258434
 0.04612188 0.05106761 0.04561584 0.04716426 0.05522437 0.03815803
 0.06333796 0.05089653 0.06465203 0.05026012 0.02445017 0.03885695
 0.0541257  0.04825004]
w_init sum: 0.9617388364178626
epoch 100 loss nan |dL/du| 1.399e-01
epoch 100 sumw 1.000000
epoch 100 weights [ 0.05209076  0.19008381  0.25604514  0.13111042 -0.06228354  0.12030625
  0.16169211  0.09371106  0.10778586  0.19956054  0.2049991   0.1135846
  0.09852144  0.16951741  0.06003778  0.10158467  0.22773396  0.14259614
  0.21297531 -1.58165283]
epoch 200 loss nan |dL/du| 4.420e+00
epoch 200 sumw 1.000000
epoch 200 weights [ 0.00112625  0.15741591  0.29013554  0.05721648 -0.28415864  0.0650348
  0.06647834  0.02028784  0.02741028  0.17281501  0.14880806  0.17030556
 -0.1101893   0.13454937 -0.00651678  0.07819176  0.17763174  0.14804663
  0.18869399 -0.50328283]
epoch 300 loss nan |dL/du| 7.466e-03
epoch 300 sumw 1.000000
epoch 300 weights [ 0.01967634  0.19422061  0.31627263  0.06365476 -0.29066142  0.07746991
  0.09830846  0.04245662  0.04986692  0.19192087  0.15628688  0.19019721
 -0.10539583  0.15835526  0.01576946  0.11658338  0.18507747  0.16337263
  0.21908768 -0.86251985]
epoch 400 loss nan |dL/du| 4.109e-03
epoch 400 sumw 1.000000
epoch 400 weights [ 0.02458386  0.20335106  0.31551973  0.06057454 -0.28886968  0.081526
  0.1061583   0.04474103  0.0440796   0.18979777  0.1548151   0.17639619
 -0.09834936  0.16149682  0.01874331  0.12342725  0.17543879  0.15128114
  0.21687551 -0.86158694]
epoch 500 loss nan |dL/du| 3.173e-03
epoch 500 sumw 1.000000
epoch 500 weights [ 0.02787431  0.21271077  0.3134425   0.05851538 -0.29180221  0.08535172
  0.1109793   0.04856106  0.03859388  0.18758194  0.15153413  0.16776373
 -0.09542742  0.16803916  0.01907862  0.12573435  0.16857625  0.14609054
  0.21711092 -0.86030891]
epoch 600 loss nan |dL/du| 2.524e-03
epoch 600 sumw 1.000000
epoch 600 weights [ 0.03023572  0.22168077  0.31184242  0.05752037 -0.2948405   0.08845417
  0.11351913  0.05134881  0.03253757  0.18627969  0.14874166  0.16102238
 -0.09296583  0.17496844  0.01826717  0.12700262  0.16248183  0.14341727
  0.21692539 -0.85843908]
epoch 700 loss nan |dL/du| 2.052e-03
epoch 700 sumw 1.000000
epoch 700 weights [ 0.03202901  0.23008723  0.31091327  0.05738426 -0.29755391  0.0910484
  0.11469956  0.0528241   0.02639435  0.18584805  0.14658666  0.15597308
 -0.09075641  0.18183613  0.01681688  0.12750159  0.15710632  0.14176011
  0.2161223  -0.85662099]
epoch 800 loss nan |dL/du| 1.676e-03
epoch 800 sumw 1.000000
epoch 800 weights [ 0.03347397  0.2378332   0.31064106  0.0578312  -0.29974427  0.0931297
  0.11520637  0.05322108  0.02060774  0.1858034   0.14481852  0.15218621
 -0.08872096  0.18837655  0.01510559  0.1274761   0.15253221  0.14037208
  0.21493212 -0.85508187]
epoch 900 loss nan |dL/du| 1.361e-03
epoch 900 sumw 1.000000
epoch 900 weights [ 0.03468371  0.24485463  0.31088793  0.05862176 -0.30142398  0.09470645
  0.11546459  0.05284431  0.01546648  0.18581138  0.1432477   0.1492477
 -0.08686404  0.19439733  0.01338783  0.12710314  0.14875358  0.13904864
  0.21358121 -0.85382037]
epoch 1000 loss nan |dL/du| 1.091e-03
epoch 1000 sumw 1.000000
epoch 1000 weights [ 0.03569679  0.25110039  0.31145337  0.05958325 -0.30269069  0.09588904
  0.11566658  0.05195963  0.0111149   0.18576047  0.1418054   0.14687432
 -0.08519788  0.19977231  0.01181654  0.1264923   0.14566917  0.13783177
  0.21220844 -0.85280609]
epoch 1100 loss nan |dL/du| 8.620e-04
epoch 1100 sumw 1.000000
epoch 1100 weights [ 0.036526    0.25654348  0.31215102  0.06060205 -0.30364136  0.09681319
  0.11586282  0.05078689  0.00759753  0.18563846  0.14047837  0.1448917
 -0.08373463  0.20443457  0.01047182  0.12574302  0.14314843  0.13679939
  0.21089735 -0.8520101 ]
epoch 1200 loss nan |dL/du| 6.696e-04
epoch 1200 sumw 1.000000
epoch 1200 weights [ 0.03718419  0.26118745  0.31284689  0.06160706 -0.30434675  0.09757199
  0.11604739  0.04949617  0.00488345  0.18546007  0.13926938  0.14320983
 -0.08248307  0.20836933  0.00937669  0.12495214  0.14107638  0.13599448
  0.20969864 -0.85140171]
epoch 1300 loss nan |dL/du| 5.118e-04
epoch 1300 sumw 1.000000
epoch 1300 weights [ 0.03769251  0.26506468  0.31346187  0.06255496 -0.3048582   0.09821145
  0.11620568  0.04820901  0.00288158  0.18524619  0.13818509  0.14179185
 -0.08144154  0.21160397  0.0085182   0.12419921  0.13936668  0.13541486
  0.20864141 -0.85094948]
epoch 1400 loss nan |dL/du| 3.850e-04
epoch 1400 sumw 1.000000
epoch 1400 weights [ 0.03807808  0.26823221  0.31395962  0.06341848 -0.30521759  0.09874923
  0.11633073  0.04700644  0.00146247  0.18501949  0.13723236  0.14062278
 -0.08059644  0.21419745  0.00786693  0.12353597  0.13795743  0.13502708
  0.20774031 -0.85062304]
epoch 1500 loss nan |dL/du| 2.850e-04
epoch 1500 sumw 1.000000
epoch 1500 weights [ 3.83683684e-02  2.70765279e-01  3.14333207e-01  6.41799938e-02
 -3.05461209e-01  9.91918044e-02  1.16424057e-01  4.59375432e-02
  4.86326035e-04  1.84800879e-01  1.36415585e-01  1.39688429e-01
 -7.99262505e-02  2.16228847e-01  7.38871592e-03  1.22985930e-01
  1.36803309e-01  1.34784883e-01  2.06998687e-01 -8.50394384e-01]
epoch 1600 loss nan |dL/du| 2.075e-04
epoch 1600 sumw 1.000000
epoch 1600 weights [ 3.85869630e-02  2.72749172e-01  3.14593892e-01  6.48297776e-02
 -3.05619560e-01  9.95447298e-02  1.16491551e-01  4.50261803e-02
 -1.72683027e-04  1.84605797e-01  1.35734354e-01  1.38966669e-01
 -7.94062853e-02  2.17785655e-01  7.04984907e-03  1.22550795e-01
  1.35869066e-01  1.34643177e-01  2.06409819e-01 -8.50238918e-01]
epoch 1700 loss nan |dL/du| 1.484e-04
epoch 1700 sumw 1.000000
epoch 1700 weights [ 3.87519446e-02  2.74271368e-01  3.14762188e-01  6.53659958e-02
 -3.05717032e-01  9.98166840e-02  1.16539793e-01  4.42769573e-02
 -6.13737156e-04  1.84442438e-01  1.35182477e-01  1.38427493e-01
 -7.90118185e-02  2.18954233e-01  6.81904354e-03  1.22219095e-01
  1.35125055e-01  1.34565530e-01  2.05958540e-01 -8.50136247e-01]
epoch 1800 loss nan |dL/du| 1.041e-04
epoch 1800 sumw 1.000000
epoch 1800 weights [ 0.03887634  0.27541553  0.3148612   0.06579409 -0.30577244  0.10001941
  0.11657419  0.043681   -0.00090813  0.18431254  0.13474851  0.13803713
 -0.07871957  0.21981366  0.00666853  0.12197362  0.13454446  0.13452604
  0.20562429 -0.8500704 ]
epoch 1900 loss nan |dL/du| 7.149e-05
epoch 1900 sumw 1.000000
epoch 1900 weights [ 0.03896947  0.27625771  0.31491234  0.06612501 -0.30580002  0.10016601
  0.11659865  0.04322125 -0.00110438  0.1842135   0.13441733  0.13776252
 -0.07850836  0.22043274  0.00657493  0.12179634  0.13410193  0.13450795
  0.20538457 -0.85002946]
epoch 2000 loss nan |dL/du| 4.805e-05
epoch 2000 sumw 1.000000
epoch 2000 weights [ 0.03903821  0.2768644   0.31493308  0.06637292 -0.3058103   0.10026911
  0.1166159   0.04287685 -0.00123476  0.18414057  0.13417203  0.13757438
 -0.07835958  0.22086917  0.00651965  0.12167104  0.13377329  0.13450113
  0.2052178  -0.85000488]
Saved final_weights.npy and loss_history_wifi.npy
p_raw min/med/max: -1.152e-01 / 8.626e-01 / 1.712e+00
p     min/med/max: 6.372e-01 / 1.215e+00 / 1.878e+00
sum(w): 1.000000
L1(w):  3.470819e+00
max|w|: 8.500049e-01
fraction p_raw < 0: 0.0018
Hessian eigenvalues are [1.70865861e-02 2.25940542e-02 2.39572077e-02 2.71910385e-02
 3.57263678e-02 3.71262729e-02 4.37928111e-02 4.49960959e-02
 5.30667741e-02 5.85635882e-02 6.77437918e-02 9.32227893e-02
 1.24693600e-01 1.52075719e-01 2.28966974e-01 3.35640231e-01
 4.76102147e-01 6.41498297e-01 8.04467800e-01 4.29429670e+01]
All done.
