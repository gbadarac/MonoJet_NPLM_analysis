Using folder_path = /work/gbadarac/MonoJet_NPLM/MonoJet_NPLM_analysis/Train_Ensembles/Train_Models/Sparker_kernels/EstimationKernels_outputs/2_dim/2d_bimodal_gaussian_heavy_tail/N_100000_dim_2_kernels_SparKer_models32_L5_K75_M270_Nboot100000_lr0.05_clip_10000000_no_masking
Storing WiFi fit results in: /work/gbadarac/MonoJet_NPLM/MonoJet_NPLM_analysis/Uncertainty_Modeling/wifi/Fit_Weights/results_fit_weights_kernel/N_100000_dim_2_kernels_SparKer_models32_L5_K75_M270_Nboot100000_lr0.05_clip_10000000_no_masking_2d_bimodal_gaussian_heavy_tail_ensemblecomponents20_new_plotting
Loaded target data: (100000, 2)
Precomputing model_probs on CPU ...
model_probs stats: min 3.6376149562588196e-12 max 1.8383188729183293 finite True
model_probs_cpu computed: shape=(100000, 20)
w_init: [0.06281654 0.04696136 0.05039177 0.04317807 0.035991   0.06395573
 0.05231683 0.04012177 0.03423019 0.03666783 0.05740303 0.06404301
 0.06646442 0.04646489 0.05744541 0.04067582 0.04749132 0.06059939
 0.0589871  0.04660246]
w_init sum: 1.0128079505777436
epoch 100 loss 3.966149e-01 |dL/du| 9.587e-06
epoch 100 sumw 1.000000
epoch 100 weights [ 0.07339851  0.09973193  0.03661087  0.05784396 -0.01272131 -0.04896427
  0.07109765  0.08054414  0.11745684  0.08824416 -0.00089448  0.0125059
  0.06568817  0.05178441 -0.00267022  0.05560629  0.13177915  0.01018022
  0.16796723 -0.05518915]
epoch 200 loss 3.966149e-01 |dL/du| 4.369e-08
epoch 200 sumw 1.000000
epoch 200 weights [ 0.07357833  0.09965435  0.03687745  0.05789573 -0.01296072 -0.0488336
  0.07104869  0.08012989  0.11806861  0.08847471 -0.00099008  0.01248609
  0.06596815  0.05139331 -0.00282216  0.05547359  0.13172809  0.00979542
  0.16825217 -0.05521802]
epoch 300 loss 3.966149e-01 |dL/du| 2.255e-10
epoch 300 sumw 1.000000
epoch 300 weights [ 0.07357618  0.09965341  0.03687688  0.05789718 -0.01296206 -0.04883268
  0.07104877  0.08012988  0.11806991  0.08847546 -0.00098994  0.0124851
  0.06596803  0.0513927  -0.00282254  0.05547225  0.13172927  0.0097953
  0.16825494 -0.05521803]
epoch 400 loss 3.966149e-01 |dL/du| 1.565e-12
epoch 400 sumw 1.000000
epoch 400 weights [ 0.07357618  0.09965341  0.03687689  0.05789718 -0.01296205 -0.04883268
  0.07104877  0.08012987  0.1180699   0.08847545 -0.00098993  0.0124851
  0.06596803  0.0513927  -0.00282254  0.05547226  0.13172926  0.00979531
  0.16825492 -0.05521802]
epoch 500 loss 3.966149e-01 |dL/du| 8.412e-15
epoch 500 sumw 1.000000
epoch 500 weights [ 0.07357618  0.09965341  0.03687689  0.05789718 -0.01296205 -0.04883268
  0.07104877  0.08012987  0.1180699   0.08847545 -0.00098993  0.0124851
  0.06596803  0.0513927  -0.00282254  0.05547226  0.13172926  0.00979531
  0.16825492 -0.05521802]
epoch 600 loss 3.966149e-01 |dL/du| 2.300e-15
epoch 600 sumw 1.000000
epoch 600 weights [ 0.07357618  0.09965341  0.03687689  0.05789718 -0.01296205 -0.04883268
  0.07104877  0.08012987  0.1180699   0.08847545 -0.00098993  0.0124851
  0.06596803  0.0513927  -0.00282254  0.05547226  0.13172926  0.00979531
  0.16825492 -0.05521802]
epoch 700 loss 3.966149e-01 |dL/du| 2.704e-15
epoch 700 sumw 1.000000
epoch 700 weights [ 0.07357618  0.09965341  0.03687689  0.05789718 -0.01296205 -0.04883268
  0.07104877  0.08012987  0.1180699   0.08847545 -0.00098993  0.0124851
  0.06596803  0.0513927  -0.00282254  0.05547226  0.13172926  0.00979531
  0.16825492 -0.05521802]
epoch 800 loss 3.966149e-01 |dL/du| 2.964e-04
epoch 800 sumw 1.000000
epoch 800 weights [ 0.07344907  0.09951617  0.03672167  0.05778835 -0.01310131 -0.04889029
  0.07092373  0.07995472  0.11798263  0.08833984 -0.00110308  0.01240795
  0.06583035  0.05127823 -0.00293108  0.0553173   0.13161657  0.00964556
  0.16815348 -0.05289987]
epoch 900 loss 3.966149e-01 |dL/du| 1.571e-06
epoch 900 sumw 1.000000
epoch 900 weights [ 0.07357349  0.09965003  0.03687386  0.05789452 -0.0129649  -0.04883407
  0.07104608  0.08012676  0.11806809  0.0884729  -0.00099275  0.01248326
  0.06596515  0.05139048 -0.00282532  0.05546957  0.13172735  0.0097923
  0.16825311 -0.05516992]
epoch 1000 loss 3.966149e-01 |dL/du| 5.997e-09
epoch 1000 sumw 1.000000
epoch 1000 weights [ 0.07357618  0.09965341  0.03687689  0.05789718 -0.01296205 -0.04883267
  0.07104877  0.08012987  0.1180699   0.08847545 -0.00098993  0.0124851
  0.06596803  0.0513927  -0.00282254  0.05547226  0.13172926  0.00979531
  0.16825492 -0.05521805]
epoch 1100 loss 3.966149e-01 |dL/du| 6.260e-11
epoch 1100 sumw 1.000000
epoch 1100 weights [ 0.07357618  0.09965341  0.03687689  0.05789718 -0.01296205 -0.04883268
  0.07104877  0.08012987  0.1180699   0.08847545 -0.00098993  0.0124851
  0.06596803  0.0513927  -0.00282254  0.05547226  0.13172926  0.00979531
  0.16825492 -0.05521802]
epoch 1200 loss 3.966149e-01 |dL/du| 5.532e-12
epoch 1200 sumw 1.000000
epoch 1200 weights [ 0.07357618  0.09965341  0.03687689  0.05789718 -0.01296205 -0.04883268
  0.07104877  0.08012987  0.1180699   0.08847545 -0.00098993  0.0124851
  0.06596803  0.0513927  -0.00282254  0.05547226  0.13172926  0.00979531
  0.16825492 -0.05521802]
epoch 1300 loss 3.966149e-01 |dL/du| 1.633e-04
epoch 1300 sumw 1.000000
epoch 1300 weights [ 0.0737678   0.09983772  0.03707343  0.05807396 -0.01277332 -0.04870766
  0.07125954  0.08032145  0.1182021   0.08866986 -0.00080158  0.01264254
  0.06615531  0.05156129 -0.00265291  0.05564987  0.13187978  0.00997885
  0.1684032  -0.05854125]
epoch 1400 loss 3.966149e-01 |dL/du| 7.991e-07
epoch 1400 sumw 1.000000
epoch 1400 weights [ 0.07357578  0.09965285  0.03687639  0.05789675 -0.01296256 -0.04883297
  0.07104833  0.08012939  0.1180695   0.08847503 -0.00099046  0.01248471
  0.06596748  0.05139234 -0.00282294  0.05547182  0.13172892  0.00979478
  0.1682546  -0.05520975]
epoch 1500 loss 3.966149e-01 |dL/du| 5.030e-09
epoch 1500 sumw 1.000000
epoch 1500 weights [ 0.07357617  0.0996534   0.03687688  0.05789718 -0.01296206 -0.04883268
  0.07104876  0.08012987  0.11806989  0.08847544 -0.00098994  0.0124851
  0.06596802  0.05139269 -0.00282254  0.05547225  0.13172926  0.0097953
  0.16825492 -0.05521793]
epoch 1600 loss 3.966149e-01 |dL/du| 3.387e-11
epoch 1600 sumw 1.000000
epoch 1600 weights [ 0.07357618  0.09965341  0.03687689  0.05789718 -0.01296205 -0.04883268
  0.07104877  0.08012987  0.1180699   0.08847545 -0.00098993  0.0124851
  0.06596803  0.0513927  -0.00282254  0.05547226  0.13172926  0.00979531
  0.16825492 -0.05521803]
epoch 1700 loss 3.966149e-01 |dL/du| 2.703e-07
epoch 1700 sumw 1.000000
epoch 1700 weights [ 0.07357544  0.09965261  0.03687611  0.05789646 -0.01296281 -0.04883315
  0.07104799  0.08012908  0.11806933  0.08847472 -0.00099068  0.01248451
  0.06596726  0.05139202 -0.00282329  0.05547153  0.13172865  0.00979453
  0.16825433 -0.05520464]
epoch 1800 loss 3.966149e-01 |dL/du| 1.946e-05
epoch 1800 sumw 1.000000
epoch 1800 weights [ 0.07353826  0.09961588  0.03683944  0.0578606  -0.01300102 -0.0488604
  0.07101198  0.08008983  0.11803666  0.08843688 -0.00102952  0.01245247
  0.06592766  0.05135646 -0.00286354  0.05543457  0.1316943   0.00975419
  0.16822133 -0.05451605]
epoch 1900 loss 3.966149e-01 |dL/du| 5.777e-08
epoch 1900 sumw 1.000000
epoch 1900 weights [ 0.07357597  0.09965319  0.03687668  0.05789697 -0.01296226 -0.04883284
  0.07104853  0.08012966  0.11806972  0.08847524 -0.00099014  0.0124849
  0.06596782  0.0513925  -0.00282276  0.05547205  0.13172907  0.00979508
  0.16825474 -0.05521413]
epoch 2000 loss 3.966149e-01 |dL/du| 7.803e-10
epoch 2000 sumw 1.000000
epoch 2000 weights [ 0.07357618  0.09965341  0.03687688  0.05789718 -0.01296205 -0.04883268
  0.07104877  0.08012987  0.1180699   0.08847545 -0.00098994  0.0124851
  0.06596803  0.0513927  -0.00282254  0.05547226  0.13172926  0.00979531
  0.16825492 -0.055218  ]
Saved final_weights.npy and loss_history_wifi.npy
p_raw min/med/max: 8.205e-09 / 8.737e-01 / 1.707e+00
p     min/med/max: 6.931e-01 / 1.223e+00 / 1.874e+00
sum(w): 1.000000
L1(w):  1.241650e+00
max|w|: 1.682549e-01
fraction p_raw < 0: 0.0000
Hessian eigenvalues are [3.77365739e-03 4.21362959e-03 4.97499164e-03 5.25811932e-03
 5.57508541e-03 5.78837500e-03 6.47566210e-03 6.97928253e-03
 7.62318432e-03 8.78540728e-03 9.69408447e-03 1.01243467e-02
 1.08791444e-02 1.35135246e-02 1.56975439e-02 1.81970141e-02
 1.95172912e-02 2.66651145e-02 4.18155992e-02 2.00111443e+01]
All done.
