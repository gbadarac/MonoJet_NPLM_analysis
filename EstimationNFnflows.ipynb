{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing flow using nflows package and toy data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "from scipy.stats import rel_breitwigner\n",
    "import torch\n",
    "#instead of manually defining bijectors and distributions, \n",
    "#import necessary components from nflows\n",
    "from nflows import distributions, flows, transforms\n",
    "import nflows.transforms as transforms\n",
    "from nflows.flows import Flow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: \n",
    "- #### bkg: exponential falling distribution\n",
    "- #### signal: Breit-Wigner at certain mass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate background and signal data\n",
    "n_bkg = 400000\n",
    "n_sig = 20\n",
    "bkg = np.random.exponential(scale=100.0, size=n_bkg)\n",
    "sig = rel_breitwigner.rvs(450, size=n_sig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot histogram\n",
    "plt.hist([bkg, sig], bins=50, stacked=True)\n",
    "plt.yscale('log')\n",
    "plt.axvline(400.0, color='black', zorder=100)  # mass threshold or cut\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.32341661 0.40921988 0.8897746  ... 0.35350044 0.03109549 0.21362902]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Adding b-tagging information (a form of event classification)\n",
    "bkg_btag = np.random.uniform(low=0.0, high=1.0, size=n_bkg)\n",
    "sig_btag = np.random.normal(0.85, 0.05, n_sig)\n",
    "\n",
    "print(bkg_btag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400000, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Combining energy and b-tagging score for both bkg and signal \n",
    "#Convert background coordinates to tensor\n",
    "bkg_coord = np.column_stack((bkg_btag, bkg))  # Combine btag and bkg for training\n",
    "bkg_coord = bkg_coord.astype('float32') #bkg coordinates converted to float32 for compatibility with python \n",
    "\n",
    "#sig_coord = np.column_stack((sig, sig_btag))\n",
    "#sig_coord = sig_coord.astype('float32')\n",
    "\n",
    "print(bkg_coord.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100000, 2])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#sample points from target distribution \n",
    "y = torch.from_numpy(bkg_coord[:100000])  # Take the first 100,000 samples\n",
    "\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the bkg distribution is the posterior/target distribution which the Normalizing Flow should learn to approximate. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing flow model:\n",
    "#### Set up simple normalizing flow with arbitrary inputs and outputs just to test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base distribution\n",
    "# base distribution = prior distribution that the Normalizing Flow will transform \n",
    "base_distribution = distributions.StandardNormal(shape=(2,))\n",
    "\n",
    "# Define transformations (bijectors)\n",
    "# you don't need to define a customed bijector anymore \n",
    "transformations = transforms.MaskedAffineAutoregressiveTransform(features=2, hidden_features=16, num_blocks=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: \n",
    "- features=2: dimensionality of the data being transformed, which in this case is 2 since you are combining the b-tagging score and background energy.\n",
    "- hidden_features=16: size of the hidden layers in the neural network that defines the transformation. The higher the number of hidden features, the more expressive the transformation will be, allowing it to capture more complex relationships in the data. In this case, each transformation in the flow is backed by a neural network with 16 hidden units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network basically has the base distribution values as inputs and gets to the parameters of the target distribution (via the network). Then those parameters are inserted in the target distribution to get the ouputs in correspondence to the inputs. In this case, the neural network has 16 layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a neural network inside the transformations in normalizing flows does make the training loop \"deeper\" in the sense that you're not just applying a single transformation but a series of transformations that are learned through the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the normalizing flow and the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss tensor(4841.8975, grad_fn=<NegBackward0>)\n",
      "epoch 1 loss tensor(4755.9517, grad_fn=<NegBackward0>)\n",
      "epoch 2 loss tensor(4671.2612, grad_fn=<NegBackward0>)\n",
      "epoch 3 loss tensor(4587.5845, grad_fn=<NegBackward0>)\n",
      "epoch 4 loss tensor(4504.8862, grad_fn=<NegBackward0>)\n",
      "epoch 5 loss tensor(4423.1592, grad_fn=<NegBackward0>)\n",
      "epoch 6 loss tensor(4342.4033, grad_fn=<NegBackward0>)\n",
      "epoch 7 loss tensor(4262.6182, grad_fn=<NegBackward0>)\n",
      "epoch 8 loss tensor(4183.8076, grad_fn=<NegBackward0>)\n",
      "epoch 9 loss tensor(4105.9707, grad_fn=<NegBackward0>)\n",
      "epoch 10 loss tensor(4029.1084, grad_fn=<NegBackward0>)\n",
      "epoch 11 loss tensor(3953.2202, grad_fn=<NegBackward0>)\n",
      "epoch 12 loss tensor(3878.3018, grad_fn=<NegBackward0>)\n",
      "epoch 13 loss tensor(3804.3469, grad_fn=<NegBackward0>)\n",
      "epoch 14 loss tensor(3731.3499, grad_fn=<NegBackward0>)\n",
      "epoch 15 loss tensor(3659.3159, grad_fn=<NegBackward0>)\n",
      "epoch 16 loss tensor(3588.2400, grad_fn=<NegBackward0>)\n",
      "epoch 17 loss tensor(3518.1155, grad_fn=<NegBackward0>)\n",
      "epoch 18 loss tensor(3448.9353, grad_fn=<NegBackward0>)\n",
      "epoch 19 loss tensor(3380.6909, grad_fn=<NegBackward0>)\n",
      "epoch 20 loss tensor(3313.3750, grad_fn=<NegBackward0>)\n",
      "epoch 21 loss tensor(3246.9766, grad_fn=<NegBackward0>)\n",
      "epoch 22 loss tensor(3181.4707, grad_fn=<NegBackward0>)\n",
      "epoch 23 loss tensor(3116.8582, grad_fn=<NegBackward0>)\n",
      "epoch 24 loss tensor(3053.1348, grad_fn=<NegBackward0>)\n",
      "epoch 25 loss tensor(2990.2893, grad_fn=<NegBackward0>)\n",
      "epoch 26 loss tensor(2928.3159, grad_fn=<NegBackward0>)\n",
      "epoch 27 loss tensor(2867.2046, grad_fn=<NegBackward0>)\n",
      "epoch 28 loss tensor(2806.9446, grad_fn=<NegBackward0>)\n",
      "epoch 29 loss tensor(2747.5315, grad_fn=<NegBackward0>)\n",
      "epoch 30 loss tensor(2688.9565, grad_fn=<NegBackward0>)\n",
      "epoch 31 loss tensor(2631.2119, grad_fn=<NegBackward0>)\n",
      "epoch 32 loss tensor(2574.2903, grad_fn=<NegBackward0>)\n",
      "epoch 33 loss tensor(2518.1929, grad_fn=<NegBackward0>)\n",
      "epoch 34 loss tensor(2462.9148, grad_fn=<NegBackward0>)\n",
      "epoch 35 loss tensor(2408.4504, grad_fn=<NegBackward0>)\n",
      "epoch 36 loss tensor(2354.7942, grad_fn=<NegBackward0>)\n",
      "epoch 37 loss tensor(2301.9409, grad_fn=<NegBackward0>)\n",
      "epoch 38 loss tensor(2249.8855, grad_fn=<NegBackward0>)\n",
      "epoch 39 loss tensor(2198.6238, grad_fn=<NegBackward0>)\n",
      "epoch 40 loss tensor(2148.1492, grad_fn=<NegBackward0>)\n",
      "epoch 41 loss tensor(2098.4565, grad_fn=<NegBackward0>)\n",
      "epoch 42 loss tensor(2049.5400, grad_fn=<NegBackward0>)\n",
      "epoch 43 loss tensor(2001.3965, grad_fn=<NegBackward0>)\n",
      "epoch 44 loss tensor(1954.0208, grad_fn=<NegBackward0>)\n",
      "epoch 45 loss tensor(1907.4083, grad_fn=<NegBackward0>)\n",
      "epoch 46 loss tensor(1861.5581, grad_fn=<NegBackward0>)\n",
      "epoch 47 loss tensor(1816.4651, grad_fn=<NegBackward0>)\n",
      "epoch 48 loss tensor(1772.1248, grad_fn=<NegBackward0>)\n",
      "epoch 49 loss tensor(1728.5360, grad_fn=<NegBackward0>)\n",
      "epoch 50 loss tensor(1685.6968, grad_fn=<NegBackward0>)\n",
      "epoch 51 loss tensor(1643.6050, grad_fn=<NegBackward0>)\n",
      "epoch 52 loss tensor(1602.2578, grad_fn=<NegBackward0>)\n",
      "epoch 53 loss tensor(1561.6509, grad_fn=<NegBackward0>)\n",
      "epoch 54 loss tensor(1521.7808, grad_fn=<NegBackward0>)\n",
      "epoch 55 loss tensor(1482.6432, grad_fn=<NegBackward0>)\n",
      "epoch 56 loss tensor(1444.2333, grad_fn=<NegBackward0>)\n",
      "epoch 57 loss tensor(1406.5465, grad_fn=<NegBackward0>)\n",
      "epoch 58 loss tensor(1369.5786, grad_fn=<NegBackward0>)\n",
      "epoch 59 loss tensor(1333.3165, grad_fn=<NegBackward0>)\n",
      "epoch 60 loss tensor(1297.7504, grad_fn=<NegBackward0>)\n",
      "epoch 61 loss tensor(1262.8745, grad_fn=<NegBackward0>)\n",
      "epoch 62 loss tensor(1228.6842, grad_fn=<NegBackward0>)\n",
      "epoch 63 loss tensor(1195.1733, grad_fn=<NegBackward0>)\n",
      "epoch 64 loss tensor(1162.3368, grad_fn=<NegBackward0>)\n",
      "epoch 65 loss tensor(1130.1710, grad_fn=<NegBackward0>)\n",
      "epoch 66 loss tensor(1098.6714, grad_fn=<NegBackward0>)\n",
      "epoch 67 loss tensor(1067.8330, grad_fn=<NegBackward0>)\n",
      "epoch 68 loss tensor(1037.6497, grad_fn=<NegBackward0>)\n",
      "epoch 69 loss tensor(1008.1115, grad_fn=<NegBackward0>)\n",
      "epoch 70 loss tensor(979.2103, grad_fn=<NegBackward0>)\n",
      "epoch 71 loss tensor(950.9370, grad_fn=<NegBackward0>)\n",
      "epoch 72 loss tensor(923.2814, grad_fn=<NegBackward0>)\n",
      "epoch 73 loss tensor(896.2314, grad_fn=<NegBackward0>)\n",
      "epoch 74 loss tensor(869.7780, grad_fn=<NegBackward0>)\n",
      "epoch 75 loss tensor(843.9084, grad_fn=<NegBackward0>)\n",
      "epoch 76 loss tensor(818.6059, grad_fn=<NegBackward0>)\n",
      "epoch 77 loss tensor(793.8574, grad_fn=<NegBackward0>)\n",
      "epoch 78 loss tensor(769.6492, grad_fn=<NegBackward0>)\n",
      "epoch 79 loss tensor(745.9662, grad_fn=<NegBackward0>)\n",
      "epoch 80 loss tensor(722.7920, grad_fn=<NegBackward0>)\n",
      "epoch 81 loss tensor(700.1141, grad_fn=<NegBackward0>)\n",
      "epoch 82 loss tensor(677.9241, grad_fn=<NegBackward0>)\n",
      "epoch 83 loss tensor(656.2125, grad_fn=<NegBackward0>)\n",
      "epoch 84 loss tensor(634.9697, grad_fn=<NegBackward0>)\n",
      "epoch 85 loss tensor(614.1853, grad_fn=<NegBackward0>)\n",
      "epoch 86 loss tensor(593.8546, grad_fn=<NegBackward0>)\n",
      "epoch 87 loss tensor(573.9861, grad_fn=<NegBackward0>)\n",
      "epoch 88 loss tensor(554.5786, grad_fn=<NegBackward0>)\n",
      "epoch 89 loss tensor(535.6266, grad_fn=<NegBackward0>)\n",
      "epoch 90 loss tensor(517.1242, grad_fn=<NegBackward0>)\n",
      "epoch 91 loss tensor(499.0699, grad_fn=<NegBackward0>)\n",
      "epoch 92 loss tensor(481.4757, grad_fn=<NegBackward0>)\n",
      "epoch 93 loss tensor(464.3441, grad_fn=<NegBackward0>)\n",
      "epoch 94 loss tensor(447.6715, grad_fn=<NegBackward0>)\n",
      "epoch 95 loss tensor(431.4539, grad_fn=<NegBackward0>)\n",
      "epoch 96 loss tensor(415.6866, grad_fn=<NegBackward0>)\n",
      "epoch 97 loss tensor(400.3651, grad_fn=<NegBackward0>)\n",
      "epoch 98 loss tensor(385.4874, grad_fn=<NegBackward0>)\n",
      "epoch 99 loss tensor(371.0608, grad_fn=<NegBackward0>)\n",
      "epoch 100 loss tensor(357.0818, grad_fn=<NegBackward0>)\n",
      "epoch 101 loss tensor(343.5462, grad_fn=<NegBackward0>)\n",
      "epoch 102 loss tensor(330.4493, grad_fn=<NegBackward0>)\n",
      "epoch 103 loss tensor(317.7857, grad_fn=<NegBackward0>)\n",
      "epoch 104 loss tensor(305.5494, grad_fn=<NegBackward0>)\n",
      "epoch 105 loss tensor(293.7343, grad_fn=<NegBackward0>)\n",
      "epoch 106 loss tensor(282.3334, grad_fn=<NegBackward0>)\n",
      "epoch 107 loss tensor(271.3396, grad_fn=<NegBackward0>)\n",
      "epoch 108 loss tensor(260.7448, grad_fn=<NegBackward0>)\n",
      "epoch 109 loss tensor(250.5411, grad_fn=<NegBackward0>)\n",
      "epoch 110 loss tensor(240.7199, grad_fn=<NegBackward0>)\n",
      "epoch 111 loss tensor(231.2724, grad_fn=<NegBackward0>)\n",
      "epoch 112 loss tensor(222.1896, grad_fn=<NegBackward0>)\n",
      "epoch 113 loss tensor(213.4622, grad_fn=<NegBackward0>)\n",
      "epoch 114 loss tensor(205.0806, grad_fn=<NegBackward0>)\n",
      "epoch 115 loss tensor(197.0352, grad_fn=<NegBackward0>)\n",
      "epoch 116 loss tensor(189.3160, grad_fn=<NegBackward0>)\n",
      "epoch 117 loss tensor(181.9132, grad_fn=<NegBackward0>)\n",
      "epoch 118 loss tensor(174.8170, grad_fn=<NegBackward0>)\n",
      "epoch 119 loss tensor(168.0173, grad_fn=<NegBackward0>)\n",
      "epoch 120 loss tensor(161.5042, grad_fn=<NegBackward0>)\n",
      "epoch 121 loss tensor(155.2678, grad_fn=<NegBackward0>)\n",
      "epoch 122 loss tensor(149.2984, grad_fn=<NegBackward0>)\n",
      "epoch 123 loss tensor(143.5862, grad_fn=<NegBackward0>)\n",
      "epoch 124 loss tensor(138.1217, grad_fn=<NegBackward0>)\n",
      "epoch 125 loss tensor(132.8953, grad_fn=<NegBackward0>)\n",
      "epoch 126 loss tensor(127.8978, grad_fn=<NegBackward0>)\n",
      "epoch 127 loss tensor(123.1203, grad_fn=<NegBackward0>)\n",
      "epoch 128 loss tensor(118.5541, grad_fn=<NegBackward0>)\n",
      "epoch 129 loss tensor(114.1905, grad_fn=<NegBackward0>)\n",
      "epoch 130 loss tensor(110.0211, grad_fn=<NegBackward0>)\n",
      "epoch 131 loss tensor(106.0375, grad_fn=<NegBackward0>)\n",
      "epoch 132 loss tensor(102.2318, grad_fn=<NegBackward0>)\n",
      "epoch 133 loss tensor(98.5962, grad_fn=<NegBackward0>)\n",
      "epoch 134 loss tensor(95.1232, grad_fn=<NegBackward0>)\n",
      "epoch 135 loss tensor(91.8054, grad_fn=<NegBackward0>)\n",
      "epoch 136 loss tensor(88.6360, grad_fn=<NegBackward0>)\n",
      "epoch 137 loss tensor(85.6081, grad_fn=<NegBackward0>)\n",
      "epoch 138 loss tensor(82.7151, grad_fn=<NegBackward0>)\n",
      "epoch 139 loss tensor(79.9509, grad_fn=<NegBackward0>)\n",
      "epoch 140 loss tensor(77.3095, grad_fn=<NegBackward0>)\n",
      "epoch 141 loss tensor(74.7850, grad_fn=<NegBackward0>)\n",
      "epoch 142 loss tensor(72.3717, grad_fn=<NegBackward0>)\n",
      "epoch 143 loss tensor(70.0644, grad_fn=<NegBackward0>)\n",
      "epoch 144 loss tensor(67.8576, grad_fn=<NegBackward0>)\n",
      "epoch 145 loss tensor(65.7462, grad_fn=<NegBackward0>)\n",
      "epoch 146 loss tensor(63.7254, grad_fn=<NegBackward0>)\n",
      "epoch 147 loss tensor(61.7903, grad_fn=<NegBackward0>)\n",
      "epoch 148 loss tensor(59.9364, grad_fn=<NegBackward0>)\n",
      "epoch 149 loss tensor(58.1594, grad_fn=<NegBackward0>)\n",
      "epoch 150 loss tensor(56.4552, grad_fn=<NegBackward0>)\n",
      "epoch 151 loss tensor(54.8197, grad_fn=<NegBackward0>)\n",
      "epoch 152 loss tensor(53.2489, grad_fn=<NegBackward0>)\n",
      "epoch 153 loss tensor(51.7393, grad_fn=<NegBackward0>)\n",
      "epoch 154 loss tensor(50.2873, grad_fn=<NegBackward0>)\n",
      "epoch 155 loss tensor(48.8894, grad_fn=<NegBackward0>)\n",
      "epoch 156 loss tensor(47.5424, grad_fn=<NegBackward0>)\n",
      "epoch 157 loss tensor(46.2429, grad_fn=<NegBackward0>)\n",
      "epoch 158 loss tensor(44.9900, grad_fn=<NegBackward0>)\n",
      "epoch 159 loss tensor(43.7828, grad_fn=<NegBackward0>)\n",
      "epoch 160 loss tensor(42.6192, grad_fn=<NegBackward0>)\n",
      "epoch 161 loss tensor(41.4974, grad_fn=<NegBackward0>)\n",
      "epoch 162 loss tensor(40.4154, grad_fn=<NegBackward0>)\n",
      "epoch 163 loss tensor(39.3717, grad_fn=<NegBackward0>)\n",
      "epoch 164 loss tensor(38.3647, grad_fn=<NegBackward0>)\n",
      "epoch 165 loss tensor(37.3928, grad_fn=<NegBackward0>)\n",
      "epoch 166 loss tensor(36.4548, grad_fn=<NegBackward0>)\n",
      "epoch 167 loss tensor(35.5493, grad_fn=<NegBackward0>)\n",
      "epoch 168 loss tensor(34.6749, grad_fn=<NegBackward0>)\n",
      "epoch 169 loss tensor(33.8306, grad_fn=<NegBackward0>)\n",
      "epoch 170 loss tensor(33.0151, grad_fn=<NegBackward0>)\n",
      "epoch 171 loss tensor(32.2274, grad_fn=<NegBackward0>)\n",
      "epoch 172 loss tensor(31.4664, grad_fn=<NegBackward0>)\n",
      "epoch 173 loss tensor(30.7312, grad_fn=<NegBackward0>)\n",
      "epoch 174 loss tensor(30.0208, grad_fn=<NegBackward0>)\n",
      "epoch 175 loss tensor(29.3342, grad_fn=<NegBackward0>)\n",
      "epoch 176 loss tensor(28.6707, grad_fn=<NegBackward0>)\n",
      "epoch 177 loss tensor(28.0293, grad_fn=<NegBackward0>)\n",
      "epoch 178 loss tensor(27.4094, grad_fn=<NegBackward0>)\n",
      "epoch 179 loss tensor(26.8100, grad_fn=<NegBackward0>)\n",
      "epoch 180 loss tensor(26.2306, grad_fn=<NegBackward0>)\n",
      "epoch 181 loss tensor(25.6703, grad_fn=<NegBackward0>)\n",
      "epoch 182 loss tensor(25.1285, grad_fn=<NegBackward0>)\n",
      "epoch 183 loss tensor(24.6045, grad_fn=<NegBackward0>)\n",
      "epoch 184 loss tensor(24.0977, grad_fn=<NegBackward0>)\n",
      "epoch 185 loss tensor(23.6076, grad_fn=<NegBackward0>)\n",
      "epoch 186 loss tensor(23.1335, grad_fn=<NegBackward0>)\n",
      "epoch 187 loss tensor(22.6748, grad_fn=<NegBackward0>)\n",
      "epoch 188 loss tensor(22.2310, grad_fn=<NegBackward0>)\n",
      "epoch 189 loss tensor(21.8017, grad_fn=<NegBackward0>)\n",
      "epoch 190 loss tensor(21.3862, grad_fn=<NegBackward0>)\n",
      "epoch 191 loss tensor(20.9842, grad_fn=<NegBackward0>)\n",
      "epoch 192 loss tensor(20.5950, grad_fn=<NegBackward0>)\n",
      "epoch 193 loss tensor(20.2184, grad_fn=<NegBackward0>)\n",
      "epoch 194 loss tensor(19.8538, grad_fn=<NegBackward0>)\n",
      "epoch 195 loss tensor(19.5009, grad_fn=<NegBackward0>)\n",
      "epoch 196 loss tensor(19.1592, grad_fn=<NegBackward0>)\n",
      "epoch 197 loss tensor(18.8283, grad_fn=<NegBackward0>)\n",
      "epoch 198 loss tensor(18.5079, grad_fn=<NegBackward0>)\n",
      "epoch 199 loss tensor(18.1976, grad_fn=<NegBackward0>)\n",
      "epoch 200 loss tensor(17.8971, grad_fn=<NegBackward0>)\n",
      "epoch 201 loss tensor(17.6060, grad_fn=<NegBackward0>)\n",
      "epoch 202 loss tensor(17.3240, grad_fn=<NegBackward0>)\n",
      "epoch 203 loss tensor(17.0508, grad_fn=<NegBackward0>)\n",
      "epoch 204 loss tensor(16.7860, grad_fn=<NegBackward0>)\n",
      "epoch 205 loss tensor(16.5295, grad_fn=<NegBackward0>)\n",
      "epoch 206 loss tensor(16.2808, grad_fn=<NegBackward0>)\n",
      "epoch 207 loss tensor(16.0398, grad_fn=<NegBackward0>)\n",
      "epoch 208 loss tensor(15.8062, grad_fn=<NegBackward0>)\n",
      "epoch 209 loss tensor(15.5796, grad_fn=<NegBackward0>)\n",
      "epoch 210 loss tensor(15.3600, grad_fn=<NegBackward0>)\n",
      "epoch 211 loss tensor(15.1471, grad_fn=<NegBackward0>)\n",
      "epoch 212 loss tensor(14.9405, grad_fn=<NegBackward0>)\n",
      "epoch 213 loss tensor(14.7402, grad_fn=<NegBackward0>)\n",
      "epoch 214 loss tensor(14.5458, grad_fn=<NegBackward0>)\n",
      "epoch 215 loss tensor(14.3573, grad_fn=<NegBackward0>)\n",
      "epoch 216 loss tensor(14.1744, grad_fn=<NegBackward0>)\n",
      "epoch 217 loss tensor(13.9968, grad_fn=<NegBackward0>)\n",
      "epoch 218 loss tensor(13.8246, grad_fn=<NegBackward0>)\n",
      "epoch 219 loss tensor(13.6573, grad_fn=<NegBackward0>)\n",
      "epoch 220 loss tensor(13.4950, grad_fn=<NegBackward0>)\n",
      "epoch 221 loss tensor(13.3374, grad_fn=<NegBackward0>)\n",
      "epoch 222 loss tensor(13.1844, grad_fn=<NegBackward0>)\n",
      "epoch 223 loss tensor(13.0357, grad_fn=<NegBackward0>)\n",
      "epoch 224 loss tensor(12.8914, grad_fn=<NegBackward0>)\n",
      "epoch 225 loss tensor(12.7512, grad_fn=<NegBackward0>)\n",
      "epoch 226 loss tensor(12.6150, grad_fn=<NegBackward0>)\n",
      "epoch 227 loss tensor(12.4827, grad_fn=<NegBackward0>)\n",
      "epoch 228 loss tensor(12.3541, grad_fn=<NegBackward0>)\n",
      "epoch 229 loss tensor(12.2291, grad_fn=<NegBackward0>)\n",
      "epoch 230 loss tensor(12.1077, grad_fn=<NegBackward0>)\n",
      "epoch 231 loss tensor(11.9896, grad_fn=<NegBackward0>)\n",
      "epoch 232 loss tensor(11.8749, grad_fn=<NegBackward0>)\n",
      "epoch 233 loss tensor(11.7633, grad_fn=<NegBackward0>)\n",
      "epoch 234 loss tensor(11.6548, grad_fn=<NegBackward0>)\n",
      "epoch 235 loss tensor(11.5492, grad_fn=<NegBackward0>)\n",
      "epoch 236 loss tensor(11.4466, grad_fn=<NegBackward0>)\n",
      "epoch 237 loss tensor(11.3468, grad_fn=<NegBackward0>)\n",
      "epoch 238 loss tensor(11.2497, grad_fn=<NegBackward0>)\n",
      "epoch 239 loss tensor(11.1552, grad_fn=<NegBackward0>)\n",
      "epoch 240 loss tensor(11.0632, grad_fn=<NegBackward0>)\n",
      "epoch 241 loss tensor(10.9737, grad_fn=<NegBackward0>)\n",
      "epoch 242 loss tensor(10.8866, grad_fn=<NegBackward0>)\n",
      "epoch 243 loss tensor(10.8018, grad_fn=<NegBackward0>)\n",
      "epoch 244 loss tensor(10.7193, grad_fn=<NegBackward0>)\n",
      "epoch 245 loss tensor(10.6390, grad_fn=<NegBackward0>)\n",
      "epoch 246 loss tensor(10.5607, grad_fn=<NegBackward0>)\n",
      "epoch 247 loss tensor(10.4845, grad_fn=<NegBackward0>)\n",
      "epoch 248 loss tensor(10.4103, grad_fn=<NegBackward0>)\n",
      "epoch 249 loss tensor(10.3380, grad_fn=<NegBackward0>)\n",
      "epoch 250 loss tensor(10.2675, grad_fn=<NegBackward0>)\n",
      "epoch 251 loss tensor(10.1989, grad_fn=<NegBackward0>)\n",
      "epoch 252 loss tensor(10.1321, grad_fn=<NegBackward0>)\n",
      "epoch 253 loss tensor(10.0669, grad_fn=<NegBackward0>)\n",
      "epoch 254 loss tensor(10.0034, grad_fn=<NegBackward0>)\n",
      "epoch 255 loss tensor(9.9415, grad_fn=<NegBackward0>)\n",
      "epoch 256 loss tensor(9.8811, grad_fn=<NegBackward0>)\n",
      "epoch 257 loss tensor(9.8223, grad_fn=<NegBackward0>)\n",
      "epoch 258 loss tensor(9.7649, grad_fn=<NegBackward0>)\n",
      "epoch 259 loss tensor(9.7090, grad_fn=<NegBackward0>)\n",
      "epoch 260 loss tensor(9.6545, grad_fn=<NegBackward0>)\n",
      "epoch 261 loss tensor(9.6013, grad_fn=<NegBackward0>)\n",
      "epoch 262 loss tensor(9.5494, grad_fn=<NegBackward0>)\n",
      "epoch 263 loss tensor(9.4987, grad_fn=<NegBackward0>)\n",
      "epoch 264 loss tensor(9.4494, grad_fn=<NegBackward0>)\n",
      "epoch 265 loss tensor(9.4012, grad_fn=<NegBackward0>)\n",
      "epoch 266 loss tensor(9.3542, grad_fn=<NegBackward0>)\n",
      "epoch 267 loss tensor(9.3083, grad_fn=<NegBackward0>)\n",
      "epoch 268 loss tensor(9.2635, grad_fn=<NegBackward0>)\n",
      "epoch 269 loss tensor(9.2198, grad_fn=<NegBackward0>)\n",
      "epoch 270 loss tensor(9.1771, grad_fn=<NegBackward0>)\n",
      "epoch 271 loss tensor(9.1355, grad_fn=<NegBackward0>)\n",
      "epoch 272 loss tensor(9.0948, grad_fn=<NegBackward0>)\n",
      "epoch 273 loss tensor(9.0551, grad_fn=<NegBackward0>)\n",
      "epoch 274 loss tensor(9.0164, grad_fn=<NegBackward0>)\n",
      "epoch 275 loss tensor(8.9785, grad_fn=<NegBackward0>)\n",
      "epoch 276 loss tensor(8.9416, grad_fn=<NegBackward0>)\n",
      "epoch 277 loss tensor(8.9055, grad_fn=<NegBackward0>)\n",
      "epoch 278 loss tensor(8.8702, grad_fn=<NegBackward0>)\n",
      "epoch 279 loss tensor(8.8357, grad_fn=<NegBackward0>)\n",
      "epoch 280 loss tensor(8.8021, grad_fn=<NegBackward0>)\n",
      "epoch 281 loss tensor(8.7692, grad_fn=<NegBackward0>)\n",
      "epoch 282 loss tensor(8.7371, grad_fn=<NegBackward0>)\n",
      "epoch 283 loss tensor(8.7057, grad_fn=<NegBackward0>)\n",
      "epoch 284 loss tensor(8.6750, grad_fn=<NegBackward0>)\n",
      "epoch 285 loss tensor(8.6450, grad_fn=<NegBackward0>)\n",
      "epoch 286 loss tensor(8.6157, grad_fn=<NegBackward0>)\n",
      "epoch 287 loss tensor(8.5870, grad_fn=<NegBackward0>)\n",
      "epoch 288 loss tensor(8.5590, grad_fn=<NegBackward0>)\n",
      "epoch 289 loss tensor(8.5317, grad_fn=<NegBackward0>)\n",
      "epoch 290 loss tensor(8.5049, grad_fn=<NegBackward0>)\n",
      "epoch 291 loss tensor(8.4787, grad_fn=<NegBackward0>)\n",
      "epoch 292 loss tensor(8.4531, grad_fn=<NegBackward0>)\n",
      "epoch 293 loss tensor(8.4281, grad_fn=<NegBackward0>)\n",
      "epoch 294 loss tensor(8.4037, grad_fn=<NegBackward0>)\n",
      "epoch 295 loss tensor(8.3797, grad_fn=<NegBackward0>)\n",
      "epoch 296 loss tensor(8.3563, grad_fn=<NegBackward0>)\n",
      "epoch 297 loss tensor(8.3334, grad_fn=<NegBackward0>)\n",
      "epoch 298 loss tensor(8.3111, grad_fn=<NegBackward0>)\n",
      "epoch 299 loss tensor(8.2892, grad_fn=<NegBackward0>)\n",
      "epoch 300 loss tensor(8.2677, grad_fn=<NegBackward0>)\n",
      "epoch 301 loss tensor(8.2468, grad_fn=<NegBackward0>)\n",
      "epoch 302 loss tensor(8.2263, grad_fn=<NegBackward0>)\n",
      "epoch 303 loss tensor(8.2062, grad_fn=<NegBackward0>)\n",
      "epoch 304 loss tensor(8.1866, grad_fn=<NegBackward0>)\n",
      "epoch 305 loss tensor(8.1674, grad_fn=<NegBackward0>)\n",
      "epoch 306 loss tensor(8.1486, grad_fn=<NegBackward0>)\n",
      "epoch 307 loss tensor(8.1302, grad_fn=<NegBackward0>)\n",
      "epoch 308 loss tensor(8.1122, grad_fn=<NegBackward0>)\n",
      "epoch 309 loss tensor(8.0945, grad_fn=<NegBackward0>)\n",
      "epoch 310 loss tensor(8.0773, grad_fn=<NegBackward0>)\n",
      "epoch 311 loss tensor(8.0604, grad_fn=<NegBackward0>)\n",
      "epoch 312 loss tensor(8.0439, grad_fn=<NegBackward0>)\n",
      "epoch 313 loss tensor(8.0277, grad_fn=<NegBackward0>)\n",
      "epoch 314 loss tensor(8.0119, grad_fn=<NegBackward0>)\n",
      "epoch 315 loss tensor(7.9964, grad_fn=<NegBackward0>)\n",
      "epoch 316 loss tensor(7.9812, grad_fn=<NegBackward0>)\n",
      "epoch 317 loss tensor(7.9663, grad_fn=<NegBackward0>)\n",
      "epoch 318 loss tensor(7.9518, grad_fn=<NegBackward0>)\n",
      "epoch 319 loss tensor(7.9375, grad_fn=<NegBackward0>)\n",
      "epoch 320 loss tensor(7.9236, grad_fn=<NegBackward0>)\n",
      "epoch 321 loss tensor(7.9099, grad_fn=<NegBackward0>)\n",
      "epoch 322 loss tensor(7.8965, grad_fn=<NegBackward0>)\n",
      "epoch 323 loss tensor(7.8834, grad_fn=<NegBackward0>)\n",
      "epoch 324 loss tensor(7.8706, grad_fn=<NegBackward0>)\n",
      "epoch 325 loss tensor(7.8580, grad_fn=<NegBackward0>)\n",
      "epoch 326 loss tensor(7.8457, grad_fn=<NegBackward0>)\n",
      "epoch 327 loss tensor(7.8336, grad_fn=<NegBackward0>)\n",
      "epoch 328 loss tensor(7.8218, grad_fn=<NegBackward0>)\n",
      "epoch 329 loss tensor(7.8102, grad_fn=<NegBackward0>)\n",
      "epoch 330 loss tensor(7.7989, grad_fn=<NegBackward0>)\n",
      "epoch 331 loss tensor(7.7878, grad_fn=<NegBackward0>)\n",
      "epoch 332 loss tensor(7.7769, grad_fn=<NegBackward0>)\n",
      "epoch 333 loss tensor(7.7662, grad_fn=<NegBackward0>)\n",
      "epoch 334 loss tensor(7.7558, grad_fn=<NegBackward0>)\n",
      "epoch 335 loss tensor(7.7455, grad_fn=<NegBackward0>)\n",
      "epoch 336 loss tensor(7.7355, grad_fn=<NegBackward0>)\n",
      "epoch 337 loss tensor(7.7257, grad_fn=<NegBackward0>)\n",
      "epoch 338 loss tensor(7.7160, grad_fn=<NegBackward0>)\n",
      "epoch 339 loss tensor(7.7066, grad_fn=<NegBackward0>)\n",
      "epoch 340 loss tensor(7.6973, grad_fn=<NegBackward0>)\n",
      "epoch 341 loss tensor(7.6882, grad_fn=<NegBackward0>)\n",
      "epoch 342 loss tensor(7.6793, grad_fn=<NegBackward0>)\n",
      "epoch 343 loss tensor(7.6706, grad_fn=<NegBackward0>)\n",
      "epoch 344 loss tensor(7.6621, grad_fn=<NegBackward0>)\n",
      "epoch 345 loss tensor(7.6537, grad_fn=<NegBackward0>)\n",
      "epoch 346 loss tensor(7.6455, grad_fn=<NegBackward0>)\n",
      "epoch 347 loss tensor(7.6374, grad_fn=<NegBackward0>)\n",
      "epoch 348 loss tensor(7.6295, grad_fn=<NegBackward0>)\n",
      "epoch 349 loss tensor(7.6218, grad_fn=<NegBackward0>)\n",
      "epoch 350 loss tensor(7.6142, grad_fn=<NegBackward0>)\n",
      "epoch 351 loss tensor(7.6068, grad_fn=<NegBackward0>)\n",
      "epoch 352 loss tensor(7.5995, grad_fn=<NegBackward0>)\n",
      "epoch 353 loss tensor(7.5923, grad_fn=<NegBackward0>)\n",
      "epoch 354 loss tensor(7.5853, grad_fn=<NegBackward0>)\n",
      "epoch 355 loss tensor(7.5785, grad_fn=<NegBackward0>)\n",
      "epoch 356 loss tensor(7.5717, grad_fn=<NegBackward0>)\n",
      "epoch 357 loss tensor(7.5651, grad_fn=<NegBackward0>)\n",
      "epoch 358 loss tensor(7.5586, grad_fn=<NegBackward0>)\n",
      "epoch 359 loss tensor(7.5523, grad_fn=<NegBackward0>)\n",
      "epoch 360 loss tensor(7.5460, grad_fn=<NegBackward0>)\n",
      "epoch 361 loss tensor(7.5399, grad_fn=<NegBackward0>)\n",
      "epoch 362 loss tensor(7.5339, grad_fn=<NegBackward0>)\n",
      "epoch 363 loss tensor(7.5280, grad_fn=<NegBackward0>)\n",
      "epoch 364 loss tensor(7.5223, grad_fn=<NegBackward0>)\n",
      "epoch 365 loss tensor(7.5166, grad_fn=<NegBackward0>)\n",
      "epoch 366 loss tensor(7.5111, grad_fn=<NegBackward0>)\n",
      "epoch 367 loss tensor(7.5056, grad_fn=<NegBackward0>)\n",
      "epoch 368 loss tensor(7.5003, grad_fn=<NegBackward0>)\n",
      "epoch 369 loss tensor(7.4950, grad_fn=<NegBackward0>)\n",
      "epoch 370 loss tensor(7.4899, grad_fn=<NegBackward0>)\n",
      "epoch 371 loss tensor(7.4848, grad_fn=<NegBackward0>)\n",
      "epoch 372 loss tensor(7.4799, grad_fn=<NegBackward0>)\n",
      "epoch 373 loss tensor(7.4750, grad_fn=<NegBackward0>)\n",
      "epoch 374 loss tensor(7.4703, grad_fn=<NegBackward0>)\n",
      "epoch 375 loss tensor(7.4656, grad_fn=<NegBackward0>)\n",
      "epoch 376 loss tensor(7.4610, grad_fn=<NegBackward0>)\n",
      "epoch 377 loss tensor(7.4565, grad_fn=<NegBackward0>)\n",
      "epoch 378 loss tensor(7.4521, grad_fn=<NegBackward0>)\n",
      "epoch 379 loss tensor(7.4478, grad_fn=<NegBackward0>)\n",
      "epoch 380 loss tensor(7.4435, grad_fn=<NegBackward0>)\n",
      "epoch 381 loss tensor(7.4393, grad_fn=<NegBackward0>)\n",
      "epoch 382 loss tensor(7.4352, grad_fn=<NegBackward0>)\n",
      "epoch 383 loss tensor(7.4312, grad_fn=<NegBackward0>)\n",
      "epoch 384 loss tensor(7.4273, grad_fn=<NegBackward0>)\n",
      "epoch 385 loss tensor(7.4234, grad_fn=<NegBackward0>)\n",
      "epoch 386 loss tensor(7.4196, grad_fn=<NegBackward0>)\n",
      "epoch 387 loss tensor(7.4158, grad_fn=<NegBackward0>)\n",
      "epoch 388 loss tensor(7.4122, grad_fn=<NegBackward0>)\n",
      "epoch 389 loss tensor(7.4086, grad_fn=<NegBackward0>)\n",
      "epoch 390 loss tensor(7.4051, grad_fn=<NegBackward0>)\n",
      "epoch 391 loss tensor(7.4016, grad_fn=<NegBackward0>)\n",
      "epoch 392 loss tensor(7.3982, grad_fn=<NegBackward0>)\n",
      "epoch 393 loss tensor(7.3948, grad_fn=<NegBackward0>)\n",
      "epoch 394 loss tensor(7.3916, grad_fn=<NegBackward0>)\n",
      "epoch 395 loss tensor(7.3883, grad_fn=<NegBackward0>)\n",
      "epoch 396 loss tensor(7.3852, grad_fn=<NegBackward0>)\n",
      "epoch 397 loss tensor(7.3821, grad_fn=<NegBackward0>)\n",
      "epoch 398 loss tensor(7.3790, grad_fn=<NegBackward0>)\n",
      "epoch 399 loss tensor(7.3760, grad_fn=<NegBackward0>)\n",
      "epoch 400 loss tensor(7.3731, grad_fn=<NegBackward0>)\n",
      "epoch 401 loss tensor(7.3702, grad_fn=<NegBackward0>)\n",
      "epoch 402 loss tensor(7.3674, grad_fn=<NegBackward0>)\n",
      "epoch 403 loss tensor(7.3646, grad_fn=<NegBackward0>)\n",
      "epoch 404 loss tensor(7.3618, grad_fn=<NegBackward0>)\n",
      "epoch 405 loss tensor(7.3591, grad_fn=<NegBackward0>)\n",
      "epoch 406 loss tensor(7.3565, grad_fn=<NegBackward0>)\n",
      "epoch 407 loss tensor(7.3539, grad_fn=<NegBackward0>)\n",
      "epoch 408 loss tensor(7.3514, grad_fn=<NegBackward0>)\n",
      "epoch 409 loss tensor(7.3488, grad_fn=<NegBackward0>)\n",
      "epoch 410 loss tensor(7.3464, grad_fn=<NegBackward0>)\n",
      "epoch 411 loss tensor(7.3440, grad_fn=<NegBackward0>)\n",
      "epoch 412 loss tensor(7.3416, grad_fn=<NegBackward0>)\n",
      "epoch 413 loss tensor(7.3393, grad_fn=<NegBackward0>)\n",
      "epoch 414 loss tensor(7.3370, grad_fn=<NegBackward0>)\n",
      "epoch 415 loss tensor(7.3347, grad_fn=<NegBackward0>)\n",
      "epoch 416 loss tensor(7.3325, grad_fn=<NegBackward0>)\n",
      "epoch 417 loss tensor(7.3303, grad_fn=<NegBackward0>)\n",
      "epoch 418 loss tensor(7.3282, grad_fn=<NegBackward0>)\n",
      "epoch 419 loss tensor(7.3260, grad_fn=<NegBackward0>)\n",
      "epoch 420 loss tensor(7.3240, grad_fn=<NegBackward0>)\n",
      "epoch 421 loss tensor(7.3219, grad_fn=<NegBackward0>)\n",
      "epoch 422 loss tensor(7.3199, grad_fn=<NegBackward0>)\n",
      "epoch 423 loss tensor(7.3180, grad_fn=<NegBackward0>)\n",
      "epoch 424 loss tensor(7.3160, grad_fn=<NegBackward0>)\n",
      "epoch 425 loss tensor(7.3141, grad_fn=<NegBackward0>)\n",
      "epoch 426 loss tensor(7.3122, grad_fn=<NegBackward0>)\n",
      "epoch 427 loss tensor(7.3104, grad_fn=<NegBackward0>)\n",
      "epoch 428 loss tensor(7.3086, grad_fn=<NegBackward0>)\n",
      "epoch 429 loss tensor(7.3068, grad_fn=<NegBackward0>)\n",
      "epoch 430 loss tensor(7.3050, grad_fn=<NegBackward0>)\n",
      "epoch 431 loss tensor(7.3033, grad_fn=<NegBackward0>)\n",
      "epoch 432 loss tensor(7.3016, grad_fn=<NegBackward0>)\n",
      "epoch 433 loss tensor(7.3000, grad_fn=<NegBackward0>)\n",
      "epoch 434 loss tensor(7.2983, grad_fn=<NegBackward0>)\n",
      "epoch 435 loss tensor(7.2967, grad_fn=<NegBackward0>)\n",
      "epoch 436 loss tensor(7.2951, grad_fn=<NegBackward0>)\n",
      "epoch 437 loss tensor(7.2935, grad_fn=<NegBackward0>)\n",
      "epoch 438 loss tensor(7.2920, grad_fn=<NegBackward0>)\n",
      "epoch 439 loss tensor(7.2905, grad_fn=<NegBackward0>)\n",
      "epoch 440 loss tensor(7.2890, grad_fn=<NegBackward0>)\n",
      "epoch 441 loss tensor(7.2875, grad_fn=<NegBackward0>)\n",
      "epoch 442 loss tensor(7.2861, grad_fn=<NegBackward0>)\n",
      "epoch 443 loss tensor(7.2846, grad_fn=<NegBackward0>)\n",
      "epoch 444 loss tensor(7.2832, grad_fn=<NegBackward0>)\n",
      "epoch 445 loss tensor(7.2818, grad_fn=<NegBackward0>)\n",
      "epoch 446 loss tensor(7.2805, grad_fn=<NegBackward0>)\n",
      "epoch 447 loss tensor(7.2791, grad_fn=<NegBackward0>)\n",
      "epoch 448 loss tensor(7.2778, grad_fn=<NegBackward0>)\n",
      "epoch 449 loss tensor(7.2765, grad_fn=<NegBackward0>)\n",
      "epoch 450 loss tensor(7.2752, grad_fn=<NegBackward0>)\n",
      "epoch 451 loss tensor(7.2740, grad_fn=<NegBackward0>)\n",
      "epoch 452 loss tensor(7.2727, grad_fn=<NegBackward0>)\n",
      "epoch 453 loss tensor(7.2715, grad_fn=<NegBackward0>)\n",
      "epoch 454 loss tensor(7.2703, grad_fn=<NegBackward0>)\n",
      "epoch 455 loss tensor(7.2691, grad_fn=<NegBackward0>)\n",
      "epoch 456 loss tensor(7.2679, grad_fn=<NegBackward0>)\n",
      "epoch 457 loss tensor(7.2668, grad_fn=<NegBackward0>)\n",
      "epoch 458 loss tensor(7.2656, grad_fn=<NegBackward0>)\n",
      "epoch 459 loss tensor(7.2645, grad_fn=<NegBackward0>)\n",
      "epoch 460 loss tensor(7.2634, grad_fn=<NegBackward0>)\n",
      "epoch 461 loss tensor(7.2623, grad_fn=<NegBackward0>)\n",
      "epoch 462 loss tensor(7.2612, grad_fn=<NegBackward0>)\n",
      "epoch 463 loss tensor(7.2602, grad_fn=<NegBackward0>)\n",
      "epoch 464 loss tensor(7.2591, grad_fn=<NegBackward0>)\n",
      "epoch 465 loss tensor(7.2581, grad_fn=<NegBackward0>)\n",
      "epoch 466 loss tensor(7.2571, grad_fn=<NegBackward0>)\n",
      "epoch 467 loss tensor(7.2561, grad_fn=<NegBackward0>)\n",
      "epoch 468 loss tensor(7.2551, grad_fn=<NegBackward0>)\n",
      "epoch 469 loss tensor(7.2541, grad_fn=<NegBackward0>)\n",
      "epoch 470 loss tensor(7.2531, grad_fn=<NegBackward0>)\n",
      "epoch 471 loss tensor(7.2522, grad_fn=<NegBackward0>)\n",
      "epoch 472 loss tensor(7.2512, grad_fn=<NegBackward0>)\n",
      "epoch 473 loss tensor(7.2503, grad_fn=<NegBackward0>)\n",
      "epoch 474 loss tensor(7.2494, grad_fn=<NegBackward0>)\n",
      "epoch 475 loss tensor(7.2485, grad_fn=<NegBackward0>)\n",
      "epoch 476 loss tensor(7.2476, grad_fn=<NegBackward0>)\n",
      "epoch 477 loss tensor(7.2467, grad_fn=<NegBackward0>)\n",
      "epoch 478 loss tensor(7.2458, grad_fn=<NegBackward0>)\n",
      "epoch 479 loss tensor(7.2449, grad_fn=<NegBackward0>)\n",
      "epoch 480 loss tensor(7.2441, grad_fn=<NegBackward0>)\n",
      "epoch 481 loss tensor(7.2432, grad_fn=<NegBackward0>)\n",
      "epoch 482 loss tensor(7.2424, grad_fn=<NegBackward0>)\n",
      "epoch 483 loss tensor(7.2416, grad_fn=<NegBackward0>)\n",
      "epoch 484 loss tensor(7.2408, grad_fn=<NegBackward0>)\n",
      "epoch 485 loss tensor(7.2399, grad_fn=<NegBackward0>)\n",
      "epoch 486 loss tensor(7.2391, grad_fn=<NegBackward0>)\n",
      "epoch 487 loss tensor(7.2384, grad_fn=<NegBackward0>)\n",
      "epoch 488 loss tensor(7.2376, grad_fn=<NegBackward0>)\n",
      "epoch 489 loss tensor(7.2368, grad_fn=<NegBackward0>)\n",
      "epoch 490 loss tensor(7.2360, grad_fn=<NegBackward0>)\n",
      "epoch 491 loss tensor(7.2353, grad_fn=<NegBackward0>)\n",
      "epoch 492 loss tensor(7.2345, grad_fn=<NegBackward0>)\n",
      "epoch 493 loss tensor(7.2338, grad_fn=<NegBackward0>)\n",
      "epoch 494 loss tensor(7.2331, grad_fn=<NegBackward0>)\n",
      "epoch 495 loss tensor(7.2323, grad_fn=<NegBackward0>)\n",
      "epoch 496 loss tensor(7.2316, grad_fn=<NegBackward0>)\n",
      "epoch 497 loss tensor(7.2309, grad_fn=<NegBackward0>)\n",
      "epoch 498 loss tensor(7.2302, grad_fn=<NegBackward0>)\n",
      "epoch 499 loss tensor(7.2295, grad_fn=<NegBackward0>)\n",
      "epoch 500 loss tensor(7.2288, grad_fn=<NegBackward0>)\n",
      "epoch 501 loss tensor(7.2281, grad_fn=<NegBackward0>)\n",
      "epoch 502 loss tensor(7.2275, grad_fn=<NegBackward0>)\n",
      "epoch 503 loss tensor(7.2268, grad_fn=<NegBackward0>)\n",
      "epoch 504 loss tensor(7.2261, grad_fn=<NegBackward0>)\n",
      "epoch 505 loss tensor(7.2255, grad_fn=<NegBackward0>)\n",
      "epoch 506 loss tensor(7.2248, grad_fn=<NegBackward0>)\n",
      "epoch 507 loss tensor(7.2242, grad_fn=<NegBackward0>)\n",
      "epoch 508 loss tensor(7.2235, grad_fn=<NegBackward0>)\n",
      "epoch 509 loss tensor(7.2229, grad_fn=<NegBackward0>)\n",
      "epoch 510 loss tensor(7.2222, grad_fn=<NegBackward0>)\n",
      "epoch 511 loss tensor(7.2216, grad_fn=<NegBackward0>)\n",
      "epoch 512 loss tensor(7.2210, grad_fn=<NegBackward0>)\n",
      "epoch 513 loss tensor(7.2204, grad_fn=<NegBackward0>)\n",
      "epoch 514 loss tensor(7.2198, grad_fn=<NegBackward0>)\n",
      "epoch 515 loss tensor(7.2192, grad_fn=<NegBackward0>)\n",
      "epoch 516 loss tensor(7.2186, grad_fn=<NegBackward0>)\n",
      "epoch 517 loss tensor(7.2180, grad_fn=<NegBackward0>)\n",
      "epoch 518 loss tensor(7.2174, grad_fn=<NegBackward0>)\n",
      "epoch 519 loss tensor(7.2168, grad_fn=<NegBackward0>)\n",
      "epoch 520 loss tensor(7.2162, grad_fn=<NegBackward0>)\n",
      "epoch 521 loss tensor(7.2156, grad_fn=<NegBackward0>)\n",
      "epoch 522 loss tensor(7.2150, grad_fn=<NegBackward0>)\n",
      "epoch 523 loss tensor(7.2145, grad_fn=<NegBackward0>)\n",
      "epoch 524 loss tensor(7.2139, grad_fn=<NegBackward0>)\n",
      "epoch 525 loss tensor(7.2133, grad_fn=<NegBackward0>)\n",
      "epoch 526 loss tensor(7.2128, grad_fn=<NegBackward0>)\n",
      "epoch 527 loss tensor(7.2122, grad_fn=<NegBackward0>)\n",
      "epoch 528 loss tensor(7.2116, grad_fn=<NegBackward0>)\n",
      "epoch 529 loss tensor(7.2111, grad_fn=<NegBackward0>)\n",
      "epoch 530 loss tensor(7.2105, grad_fn=<NegBackward0>)\n",
      "epoch 531 loss tensor(7.2100, grad_fn=<NegBackward0>)\n",
      "epoch 532 loss tensor(7.2094, grad_fn=<NegBackward0>)\n",
      "epoch 533 loss tensor(7.2089, grad_fn=<NegBackward0>)\n",
      "epoch 534 loss tensor(7.2084, grad_fn=<NegBackward0>)\n",
      "epoch 535 loss tensor(7.2078, grad_fn=<NegBackward0>)\n",
      "epoch 536 loss tensor(7.2073, grad_fn=<NegBackward0>)\n",
      "epoch 537 loss tensor(7.2068, grad_fn=<NegBackward0>)\n",
      "epoch 538 loss tensor(7.2063, grad_fn=<NegBackward0>)\n",
      "epoch 539 loss tensor(7.2057, grad_fn=<NegBackward0>)\n",
      "epoch 540 loss tensor(7.2052, grad_fn=<NegBackward0>)\n",
      "epoch 541 loss tensor(7.2047, grad_fn=<NegBackward0>)\n",
      "epoch 542 loss tensor(7.2042, grad_fn=<NegBackward0>)\n",
      "epoch 543 loss tensor(7.2037, grad_fn=<NegBackward0>)\n",
      "epoch 544 loss tensor(7.2032, grad_fn=<NegBackward0>)\n",
      "epoch 545 loss tensor(7.2027, grad_fn=<NegBackward0>)\n",
      "epoch 546 loss tensor(7.2022, grad_fn=<NegBackward0>)\n",
      "epoch 547 loss tensor(7.2017, grad_fn=<NegBackward0>)\n",
      "epoch 548 loss tensor(7.2012, grad_fn=<NegBackward0>)\n",
      "epoch 549 loss tensor(7.2007, grad_fn=<NegBackward0>)\n",
      "epoch 550 loss tensor(7.2002, grad_fn=<NegBackward0>)\n",
      "epoch 551 loss tensor(7.1997, grad_fn=<NegBackward0>)\n",
      "epoch 552 loss tensor(7.1992, grad_fn=<NegBackward0>)\n",
      "epoch 553 loss tensor(7.1987, grad_fn=<NegBackward0>)\n",
      "epoch 554 loss tensor(7.1982, grad_fn=<NegBackward0>)\n",
      "epoch 555 loss tensor(7.1977, grad_fn=<NegBackward0>)\n",
      "epoch 556 loss tensor(7.1972, grad_fn=<NegBackward0>)\n",
      "epoch 557 loss tensor(7.1968, grad_fn=<NegBackward0>)\n",
      "epoch 558 loss tensor(7.1963, grad_fn=<NegBackward0>)\n",
      "epoch 559 loss tensor(7.1958, grad_fn=<NegBackward0>)\n",
      "epoch 560 loss tensor(7.1953, grad_fn=<NegBackward0>)\n",
      "epoch 561 loss tensor(7.1949, grad_fn=<NegBackward0>)\n",
      "epoch 562 loss tensor(7.1944, grad_fn=<NegBackward0>)\n",
      "epoch 563 loss tensor(7.1939, grad_fn=<NegBackward0>)\n",
      "epoch 564 loss tensor(7.1935, grad_fn=<NegBackward0>)\n",
      "epoch 565 loss tensor(7.1930, grad_fn=<NegBackward0>)\n",
      "epoch 566 loss tensor(7.1925, grad_fn=<NegBackward0>)\n",
      "epoch 567 loss tensor(7.1921, grad_fn=<NegBackward0>)\n",
      "epoch 568 loss tensor(7.1916, grad_fn=<NegBackward0>)\n",
      "epoch 569 loss tensor(7.1912, grad_fn=<NegBackward0>)\n",
      "epoch 570 loss tensor(7.1907, grad_fn=<NegBackward0>)\n",
      "epoch 571 loss tensor(7.1903, grad_fn=<NegBackward0>)\n",
      "epoch 572 loss tensor(7.1898, grad_fn=<NegBackward0>)\n",
      "epoch 573 loss tensor(7.1893, grad_fn=<NegBackward0>)\n",
      "epoch 574 loss tensor(7.1889, grad_fn=<NegBackward0>)\n",
      "epoch 575 loss tensor(7.1884, grad_fn=<NegBackward0>)\n",
      "epoch 576 loss tensor(7.1880, grad_fn=<NegBackward0>)\n",
      "epoch 577 loss tensor(7.1876, grad_fn=<NegBackward0>)\n",
      "epoch 578 loss tensor(7.1871, grad_fn=<NegBackward0>)\n",
      "epoch 579 loss tensor(7.1867, grad_fn=<NegBackward0>)\n",
      "epoch 580 loss tensor(7.1862, grad_fn=<NegBackward0>)\n",
      "epoch 581 loss tensor(7.1858, grad_fn=<NegBackward0>)\n",
      "epoch 582 loss tensor(7.1853, grad_fn=<NegBackward0>)\n",
      "epoch 583 loss tensor(7.1849, grad_fn=<NegBackward0>)\n",
      "epoch 584 loss tensor(7.1845, grad_fn=<NegBackward0>)\n",
      "epoch 585 loss tensor(7.1840, grad_fn=<NegBackward0>)\n",
      "epoch 586 loss tensor(7.1836, grad_fn=<NegBackward0>)\n",
      "epoch 587 loss tensor(7.1832, grad_fn=<NegBackward0>)\n",
      "epoch 588 loss tensor(7.1827, grad_fn=<NegBackward0>)\n",
      "epoch 589 loss tensor(7.1823, grad_fn=<NegBackward0>)\n",
      "epoch 590 loss tensor(7.1819, grad_fn=<NegBackward0>)\n",
      "epoch 591 loss tensor(7.1814, grad_fn=<NegBackward0>)\n",
      "epoch 592 loss tensor(7.1810, grad_fn=<NegBackward0>)\n",
      "epoch 593 loss tensor(7.1806, grad_fn=<NegBackward0>)\n",
      "epoch 594 loss tensor(7.1801, grad_fn=<NegBackward0>)\n",
      "epoch 595 loss tensor(7.1797, grad_fn=<NegBackward0>)\n",
      "epoch 596 loss tensor(7.1793, grad_fn=<NegBackward0>)\n",
      "epoch 597 loss tensor(7.1789, grad_fn=<NegBackward0>)\n",
      "epoch 598 loss tensor(7.1784, grad_fn=<NegBackward0>)\n",
      "epoch 599 loss tensor(7.1780, grad_fn=<NegBackward0>)\n",
      "epoch 600 loss tensor(7.1776, grad_fn=<NegBackward0>)\n",
      "epoch 601 loss tensor(7.1772, grad_fn=<NegBackward0>)\n",
      "epoch 602 loss tensor(7.1767, grad_fn=<NegBackward0>)\n",
      "epoch 603 loss tensor(7.1763, grad_fn=<NegBackward0>)\n",
      "epoch 604 loss tensor(7.1759, grad_fn=<NegBackward0>)\n",
      "epoch 605 loss tensor(7.1755, grad_fn=<NegBackward0>)\n",
      "epoch 606 loss tensor(7.1751, grad_fn=<NegBackward0>)\n",
      "epoch 607 loss tensor(7.1747, grad_fn=<NegBackward0>)\n",
      "epoch 608 loss tensor(7.1742, grad_fn=<NegBackward0>)\n",
      "epoch 609 loss tensor(7.1738, grad_fn=<NegBackward0>)\n",
      "epoch 610 loss tensor(7.1734, grad_fn=<NegBackward0>)\n",
      "epoch 611 loss tensor(7.1730, grad_fn=<NegBackward0>)\n",
      "epoch 612 loss tensor(7.1726, grad_fn=<NegBackward0>)\n",
      "epoch 613 loss tensor(7.1722, grad_fn=<NegBackward0>)\n",
      "epoch 614 loss tensor(7.1718, grad_fn=<NegBackward0>)\n",
      "epoch 615 loss tensor(7.1713, grad_fn=<NegBackward0>)\n",
      "epoch 616 loss tensor(7.1709, grad_fn=<NegBackward0>)\n",
      "epoch 617 loss tensor(7.1705, grad_fn=<NegBackward0>)\n",
      "epoch 618 loss tensor(7.1701, grad_fn=<NegBackward0>)\n",
      "epoch 619 loss tensor(7.1697, grad_fn=<NegBackward0>)\n",
      "epoch 620 loss tensor(7.1693, grad_fn=<NegBackward0>)\n",
      "epoch 621 loss tensor(7.1689, grad_fn=<NegBackward0>)\n",
      "epoch 622 loss tensor(7.1685, grad_fn=<NegBackward0>)\n",
      "epoch 623 loss tensor(7.1681, grad_fn=<NegBackward0>)\n",
      "epoch 624 loss tensor(7.1677, grad_fn=<NegBackward0>)\n",
      "epoch 625 loss tensor(7.1673, grad_fn=<NegBackward0>)\n",
      "epoch 626 loss tensor(7.1669, grad_fn=<NegBackward0>)\n",
      "epoch 627 loss tensor(7.1665, grad_fn=<NegBackward0>)\n",
      "epoch 628 loss tensor(7.1661, grad_fn=<NegBackward0>)\n",
      "epoch 629 loss tensor(7.1657, grad_fn=<NegBackward0>)\n",
      "epoch 630 loss tensor(7.1653, grad_fn=<NegBackward0>)\n",
      "epoch 631 loss tensor(7.1649, grad_fn=<NegBackward0>)\n",
      "epoch 632 loss tensor(7.1645, grad_fn=<NegBackward0>)\n",
      "epoch 633 loss tensor(7.1641, grad_fn=<NegBackward0>)\n",
      "epoch 634 loss tensor(7.1637, grad_fn=<NegBackward0>)\n",
      "epoch 635 loss tensor(7.1633, grad_fn=<NegBackward0>)\n",
      "epoch 636 loss tensor(7.1629, grad_fn=<NegBackward0>)\n",
      "epoch 637 loss tensor(7.1625, grad_fn=<NegBackward0>)\n",
      "epoch 638 loss tensor(7.1621, grad_fn=<NegBackward0>)\n",
      "epoch 639 loss tensor(7.1617, grad_fn=<NegBackward0>)\n",
      "epoch 640 loss tensor(7.1613, grad_fn=<NegBackward0>)\n",
      "epoch 641 loss tensor(7.1609, grad_fn=<NegBackward0>)\n",
      "epoch 642 loss tensor(7.1605, grad_fn=<NegBackward0>)\n",
      "epoch 643 loss tensor(7.1601, grad_fn=<NegBackward0>)\n",
      "epoch 644 loss tensor(7.1597, grad_fn=<NegBackward0>)\n",
      "epoch 645 loss tensor(7.1593, grad_fn=<NegBackward0>)\n",
      "epoch 646 loss tensor(7.1589, grad_fn=<NegBackward0>)\n",
      "epoch 647 loss tensor(7.1585, grad_fn=<NegBackward0>)\n",
      "epoch 648 loss tensor(7.1582, grad_fn=<NegBackward0>)\n",
      "epoch 649 loss tensor(7.1578, grad_fn=<NegBackward0>)\n",
      "epoch 650 loss tensor(7.1574, grad_fn=<NegBackward0>)\n",
      "epoch 651 loss tensor(7.1570, grad_fn=<NegBackward0>)\n",
      "epoch 652 loss tensor(7.1566, grad_fn=<NegBackward0>)\n",
      "epoch 653 loss tensor(7.1562, grad_fn=<NegBackward0>)\n",
      "epoch 654 loss tensor(7.1558, grad_fn=<NegBackward0>)\n",
      "epoch 655 loss tensor(7.1554, grad_fn=<NegBackward0>)\n",
      "epoch 656 loss tensor(7.1550, grad_fn=<NegBackward0>)\n",
      "epoch 657 loss tensor(7.1547, grad_fn=<NegBackward0>)\n",
      "epoch 658 loss tensor(7.1543, grad_fn=<NegBackward0>)\n",
      "epoch 659 loss tensor(7.1539, grad_fn=<NegBackward0>)\n",
      "epoch 660 loss tensor(7.1535, grad_fn=<NegBackward0>)\n",
      "epoch 661 loss tensor(7.1531, grad_fn=<NegBackward0>)\n",
      "epoch 662 loss tensor(7.1527, grad_fn=<NegBackward0>)\n",
      "epoch 663 loss tensor(7.1524, grad_fn=<NegBackward0>)\n",
      "epoch 664 loss tensor(7.1520, grad_fn=<NegBackward0>)\n",
      "epoch 665 loss tensor(7.1516, grad_fn=<NegBackward0>)\n",
      "epoch 666 loss tensor(7.1512, grad_fn=<NegBackward0>)\n",
      "epoch 667 loss tensor(7.1508, grad_fn=<NegBackward0>)\n",
      "epoch 668 loss tensor(7.1504, grad_fn=<NegBackward0>)\n",
      "epoch 669 loss tensor(7.1501, grad_fn=<NegBackward0>)\n",
      "epoch 670 loss tensor(7.1497, grad_fn=<NegBackward0>)\n",
      "epoch 671 loss tensor(7.1493, grad_fn=<NegBackward0>)\n",
      "epoch 672 loss tensor(7.1489, grad_fn=<NegBackward0>)\n",
      "epoch 673 loss tensor(7.1486, grad_fn=<NegBackward0>)\n",
      "epoch 674 loss tensor(7.1482, grad_fn=<NegBackward0>)\n",
      "epoch 675 loss tensor(7.1478, grad_fn=<NegBackward0>)\n",
      "epoch 676 loss tensor(7.1474, grad_fn=<NegBackward0>)\n",
      "epoch 677 loss tensor(7.1470, grad_fn=<NegBackward0>)\n",
      "epoch 678 loss tensor(7.1467, grad_fn=<NegBackward0>)\n",
      "epoch 679 loss tensor(7.1463, grad_fn=<NegBackward0>)\n",
      "epoch 680 loss tensor(7.1459, grad_fn=<NegBackward0>)\n",
      "epoch 681 loss tensor(7.1455, grad_fn=<NegBackward0>)\n",
      "epoch 682 loss tensor(7.1452, grad_fn=<NegBackward0>)\n",
      "epoch 683 loss tensor(7.1448, grad_fn=<NegBackward0>)\n",
      "epoch 684 loss tensor(7.1444, grad_fn=<NegBackward0>)\n",
      "epoch 685 loss tensor(7.1440, grad_fn=<NegBackward0>)\n",
      "epoch 686 loss tensor(7.1437, grad_fn=<NegBackward0>)\n",
      "epoch 687 loss tensor(7.1433, grad_fn=<NegBackward0>)\n",
      "epoch 688 loss tensor(7.1429, grad_fn=<NegBackward0>)\n",
      "epoch 689 loss tensor(7.1425, grad_fn=<NegBackward0>)\n",
      "epoch 690 loss tensor(7.1422, grad_fn=<NegBackward0>)\n",
      "epoch 691 loss tensor(7.1418, grad_fn=<NegBackward0>)\n",
      "epoch 692 loss tensor(7.1414, grad_fn=<NegBackward0>)\n",
      "epoch 693 loss tensor(7.1411, grad_fn=<NegBackward0>)\n",
      "epoch 694 loss tensor(7.1407, grad_fn=<NegBackward0>)\n",
      "epoch 695 loss tensor(7.1403, grad_fn=<NegBackward0>)\n",
      "epoch 696 loss tensor(7.1399, grad_fn=<NegBackward0>)\n",
      "epoch 697 loss tensor(7.1396, grad_fn=<NegBackward0>)\n",
      "epoch 698 loss tensor(7.1392, grad_fn=<NegBackward0>)\n",
      "epoch 699 loss tensor(7.1388, grad_fn=<NegBackward0>)\n",
      "epoch 700 loss tensor(7.1385, grad_fn=<NegBackward0>)\n",
      "epoch 701 loss tensor(7.1381, grad_fn=<NegBackward0>)\n",
      "epoch 702 loss tensor(7.1377, grad_fn=<NegBackward0>)\n",
      "epoch 703 loss tensor(7.1374, grad_fn=<NegBackward0>)\n",
      "epoch 704 loss tensor(7.1370, grad_fn=<NegBackward0>)\n",
      "epoch 705 loss tensor(7.1366, grad_fn=<NegBackward0>)\n",
      "epoch 706 loss tensor(7.1363, grad_fn=<NegBackward0>)\n",
      "epoch 707 loss tensor(7.1359, grad_fn=<NegBackward0>)\n",
      "epoch 708 loss tensor(7.1355, grad_fn=<NegBackward0>)\n",
      "epoch 709 loss tensor(7.1352, grad_fn=<NegBackward0>)\n",
      "epoch 710 loss tensor(7.1348, grad_fn=<NegBackward0>)\n",
      "epoch 711 loss tensor(7.1344, grad_fn=<NegBackward0>)\n",
      "epoch 712 loss tensor(7.1341, grad_fn=<NegBackward0>)\n",
      "epoch 713 loss tensor(7.1337, grad_fn=<NegBackward0>)\n",
      "epoch 714 loss tensor(7.1333, grad_fn=<NegBackward0>)\n",
      "epoch 715 loss tensor(7.1330, grad_fn=<NegBackward0>)\n",
      "epoch 716 loss tensor(7.1326, grad_fn=<NegBackward0>)\n",
      "epoch 717 loss tensor(7.1322, grad_fn=<NegBackward0>)\n",
      "epoch 718 loss tensor(7.1319, grad_fn=<NegBackward0>)\n",
      "epoch 719 loss tensor(7.1315, grad_fn=<NegBackward0>)\n",
      "epoch 720 loss tensor(7.1312, grad_fn=<NegBackward0>)\n",
      "epoch 721 loss tensor(7.1308, grad_fn=<NegBackward0>)\n",
      "epoch 722 loss tensor(7.1304, grad_fn=<NegBackward0>)\n",
      "epoch 723 loss tensor(7.1301, grad_fn=<NegBackward0>)\n",
      "epoch 724 loss tensor(7.1297, grad_fn=<NegBackward0>)\n",
      "epoch 725 loss tensor(7.1293, grad_fn=<NegBackward0>)\n",
      "epoch 726 loss tensor(7.1290, grad_fn=<NegBackward0>)\n",
      "epoch 727 loss tensor(7.1286, grad_fn=<NegBackward0>)\n",
      "epoch 728 loss tensor(7.1283, grad_fn=<NegBackward0>)\n",
      "epoch 729 loss tensor(7.1279, grad_fn=<NegBackward0>)\n",
      "epoch 730 loss tensor(7.1275, grad_fn=<NegBackward0>)\n",
      "epoch 731 loss tensor(7.1272, grad_fn=<NegBackward0>)\n",
      "epoch 732 loss tensor(7.1268, grad_fn=<NegBackward0>)\n",
      "epoch 733 loss tensor(7.1265, grad_fn=<NegBackward0>)\n",
      "epoch 734 loss tensor(7.1261, grad_fn=<NegBackward0>)\n",
      "epoch 735 loss tensor(7.1257, grad_fn=<NegBackward0>)\n",
      "epoch 736 loss tensor(7.1254, grad_fn=<NegBackward0>)\n",
      "epoch 737 loss tensor(7.1250, grad_fn=<NegBackward0>)\n",
      "epoch 738 loss tensor(7.1247, grad_fn=<NegBackward0>)\n",
      "epoch 739 loss tensor(7.1243, grad_fn=<NegBackward0>)\n",
      "epoch 740 loss tensor(7.1240, grad_fn=<NegBackward0>)\n",
      "epoch 741 loss tensor(7.1236, grad_fn=<NegBackward0>)\n",
      "epoch 742 loss tensor(7.1232, grad_fn=<NegBackward0>)\n",
      "epoch 743 loss tensor(7.1229, grad_fn=<NegBackward0>)\n",
      "epoch 744 loss tensor(7.1225, grad_fn=<NegBackward0>)\n",
      "epoch 745 loss tensor(7.1222, grad_fn=<NegBackward0>)\n",
      "epoch 746 loss tensor(7.1218, grad_fn=<NegBackward0>)\n",
      "epoch 747 loss tensor(7.1215, grad_fn=<NegBackward0>)\n",
      "epoch 748 loss tensor(7.1211, grad_fn=<NegBackward0>)\n",
      "epoch 749 loss tensor(7.1208, grad_fn=<NegBackward0>)\n",
      "epoch 750 loss tensor(7.1204, grad_fn=<NegBackward0>)\n",
      "epoch 751 loss tensor(7.1201, grad_fn=<NegBackward0>)\n",
      "epoch 752 loss tensor(7.1197, grad_fn=<NegBackward0>)\n",
      "epoch 753 loss tensor(7.1194, grad_fn=<NegBackward0>)\n",
      "epoch 754 loss tensor(7.1190, grad_fn=<NegBackward0>)\n",
      "epoch 755 loss tensor(7.1186, grad_fn=<NegBackward0>)\n",
      "epoch 756 loss tensor(7.1183, grad_fn=<NegBackward0>)\n",
      "epoch 757 loss tensor(7.1179, grad_fn=<NegBackward0>)\n",
      "epoch 758 loss tensor(7.1176, grad_fn=<NegBackward0>)\n",
      "epoch 759 loss tensor(7.1172, grad_fn=<NegBackward0>)\n",
      "epoch 760 loss tensor(7.1169, grad_fn=<NegBackward0>)\n",
      "epoch 761 loss tensor(7.1165, grad_fn=<NegBackward0>)\n",
      "epoch 762 loss tensor(7.1162, grad_fn=<NegBackward0>)\n",
      "epoch 763 loss tensor(7.1158, grad_fn=<NegBackward0>)\n",
      "epoch 764 loss tensor(7.1155, grad_fn=<NegBackward0>)\n",
      "epoch 765 loss tensor(7.1151, grad_fn=<NegBackward0>)\n",
      "epoch 766 loss tensor(7.1148, grad_fn=<NegBackward0>)\n",
      "epoch 767 loss tensor(7.1144, grad_fn=<NegBackward0>)\n",
      "epoch 768 loss tensor(7.1141, grad_fn=<NegBackward0>)\n",
      "epoch 769 loss tensor(7.1138, grad_fn=<NegBackward0>)\n",
      "epoch 770 loss tensor(7.1134, grad_fn=<NegBackward0>)\n",
      "epoch 771 loss tensor(7.1131, grad_fn=<NegBackward0>)\n",
      "epoch 772 loss tensor(7.1127, grad_fn=<NegBackward0>)\n",
      "epoch 773 loss tensor(7.1124, grad_fn=<NegBackward0>)\n",
      "epoch 774 loss tensor(7.1120, grad_fn=<NegBackward0>)\n",
      "epoch 775 loss tensor(7.1117, grad_fn=<NegBackward0>)\n",
      "epoch 776 loss tensor(7.1113, grad_fn=<NegBackward0>)\n",
      "epoch 777 loss tensor(7.1110, grad_fn=<NegBackward0>)\n",
      "epoch 778 loss tensor(7.1106, grad_fn=<NegBackward0>)\n",
      "epoch 779 loss tensor(7.1103, grad_fn=<NegBackward0>)\n",
      "epoch 780 loss tensor(7.1099, grad_fn=<NegBackward0>)\n",
      "epoch 781 loss tensor(7.1096, grad_fn=<NegBackward0>)\n",
      "epoch 782 loss tensor(7.1093, grad_fn=<NegBackward0>)\n",
      "epoch 783 loss tensor(7.1089, grad_fn=<NegBackward0>)\n",
      "epoch 784 loss tensor(7.1086, grad_fn=<NegBackward0>)\n",
      "epoch 785 loss tensor(7.1082, grad_fn=<NegBackward0>)\n",
      "epoch 786 loss tensor(7.1079, grad_fn=<NegBackward0>)\n",
      "epoch 787 loss tensor(7.1075, grad_fn=<NegBackward0>)\n",
      "epoch 788 loss tensor(7.1072, grad_fn=<NegBackward0>)\n",
      "epoch 789 loss tensor(7.1069, grad_fn=<NegBackward0>)\n",
      "epoch 790 loss tensor(7.1065, grad_fn=<NegBackward0>)\n",
      "epoch 791 loss tensor(7.1062, grad_fn=<NegBackward0>)\n",
      "epoch 792 loss tensor(7.1058, grad_fn=<NegBackward0>)\n",
      "epoch 793 loss tensor(7.1055, grad_fn=<NegBackward0>)\n",
      "epoch 794 loss tensor(7.1051, grad_fn=<NegBackward0>)\n",
      "epoch 795 loss tensor(7.1048, grad_fn=<NegBackward0>)\n",
      "epoch 796 loss tensor(7.1045, grad_fn=<NegBackward0>)\n",
      "epoch 797 loss tensor(7.1041, grad_fn=<NegBackward0>)\n",
      "epoch 798 loss tensor(7.1038, grad_fn=<NegBackward0>)\n",
      "epoch 799 loss tensor(7.1034, grad_fn=<NegBackward0>)\n",
      "epoch 800 loss tensor(7.1031, grad_fn=<NegBackward0>)\n",
      "epoch 801 loss tensor(7.1028, grad_fn=<NegBackward0>)\n",
      "epoch 802 loss tensor(7.1024, grad_fn=<NegBackward0>)\n",
      "epoch 803 loss tensor(7.1021, grad_fn=<NegBackward0>)\n",
      "epoch 804 loss tensor(7.1017, grad_fn=<NegBackward0>)\n",
      "epoch 805 loss tensor(7.1014, grad_fn=<NegBackward0>)\n",
      "epoch 806 loss tensor(7.1011, grad_fn=<NegBackward0>)\n",
      "epoch 807 loss tensor(7.1007, grad_fn=<NegBackward0>)\n",
      "epoch 808 loss tensor(7.1004, grad_fn=<NegBackward0>)\n",
      "epoch 809 loss tensor(7.1001, grad_fn=<NegBackward0>)\n",
      "epoch 810 loss tensor(7.0997, grad_fn=<NegBackward0>)\n",
      "epoch 811 loss tensor(7.0994, grad_fn=<NegBackward0>)\n",
      "epoch 812 loss tensor(7.0990, grad_fn=<NegBackward0>)\n",
      "epoch 813 loss tensor(7.0987, grad_fn=<NegBackward0>)\n",
      "epoch 814 loss tensor(7.0984, grad_fn=<NegBackward0>)\n",
      "epoch 815 loss tensor(7.0980, grad_fn=<NegBackward0>)\n",
      "epoch 816 loss tensor(7.0977, grad_fn=<NegBackward0>)\n",
      "epoch 817 loss tensor(7.0974, grad_fn=<NegBackward0>)\n",
      "epoch 818 loss tensor(7.0970, grad_fn=<NegBackward0>)\n",
      "epoch 819 loss tensor(7.0967, grad_fn=<NegBackward0>)\n",
      "epoch 820 loss tensor(7.0964, grad_fn=<NegBackward0>)\n",
      "epoch 821 loss tensor(7.0960, grad_fn=<NegBackward0>)\n",
      "epoch 822 loss tensor(7.0957, grad_fn=<NegBackward0>)\n",
      "epoch 823 loss tensor(7.0954, grad_fn=<NegBackward0>)\n",
      "epoch 824 loss tensor(7.0950, grad_fn=<NegBackward0>)\n",
      "epoch 825 loss tensor(7.0947, grad_fn=<NegBackward0>)\n",
      "epoch 826 loss tensor(7.0944, grad_fn=<NegBackward0>)\n",
      "epoch 827 loss tensor(7.0940, grad_fn=<NegBackward0>)\n",
      "epoch 828 loss tensor(7.0937, grad_fn=<NegBackward0>)\n",
      "epoch 829 loss tensor(7.0934, grad_fn=<NegBackward0>)\n",
      "epoch 830 loss tensor(7.0930, grad_fn=<NegBackward0>)\n",
      "epoch 831 loss tensor(7.0927, grad_fn=<NegBackward0>)\n",
      "epoch 832 loss tensor(7.0924, grad_fn=<NegBackward0>)\n",
      "epoch 833 loss tensor(7.0920, grad_fn=<NegBackward0>)\n",
      "epoch 834 loss tensor(7.0917, grad_fn=<NegBackward0>)\n",
      "epoch 835 loss tensor(7.0914, grad_fn=<NegBackward0>)\n",
      "epoch 836 loss tensor(7.0911, grad_fn=<NegBackward0>)\n",
      "epoch 837 loss tensor(7.0907, grad_fn=<NegBackward0>)\n",
      "epoch 838 loss tensor(7.0904, grad_fn=<NegBackward0>)\n",
      "epoch 839 loss tensor(7.0901, grad_fn=<NegBackward0>)\n",
      "epoch 840 loss tensor(7.0897, grad_fn=<NegBackward0>)\n",
      "epoch 841 loss tensor(7.0894, grad_fn=<NegBackward0>)\n",
      "epoch 842 loss tensor(7.0891, grad_fn=<NegBackward0>)\n",
      "epoch 843 loss tensor(7.0887, grad_fn=<NegBackward0>)\n",
      "epoch 844 loss tensor(7.0884, grad_fn=<NegBackward0>)\n",
      "epoch 845 loss tensor(7.0881, grad_fn=<NegBackward0>)\n",
      "epoch 846 loss tensor(7.0878, grad_fn=<NegBackward0>)\n",
      "epoch 847 loss tensor(7.0874, grad_fn=<NegBackward0>)\n",
      "epoch 848 loss tensor(7.0871, grad_fn=<NegBackward0>)\n",
      "epoch 849 loss tensor(7.0868, grad_fn=<NegBackward0>)\n",
      "epoch 850 loss tensor(7.0864, grad_fn=<NegBackward0>)\n",
      "epoch 851 loss tensor(7.0861, grad_fn=<NegBackward0>)\n",
      "epoch 852 loss tensor(7.0858, grad_fn=<NegBackward0>)\n",
      "epoch 853 loss tensor(7.0855, grad_fn=<NegBackward0>)\n",
      "epoch 854 loss tensor(7.0851, grad_fn=<NegBackward0>)\n",
      "epoch 855 loss tensor(7.0848, grad_fn=<NegBackward0>)\n",
      "epoch 856 loss tensor(7.0845, grad_fn=<NegBackward0>)\n",
      "epoch 857 loss tensor(7.0842, grad_fn=<NegBackward0>)\n",
      "epoch 858 loss tensor(7.0838, grad_fn=<NegBackward0>)\n",
      "epoch 859 loss tensor(7.0835, grad_fn=<NegBackward0>)\n",
      "epoch 860 loss tensor(7.0832, grad_fn=<NegBackward0>)\n",
      "epoch 861 loss tensor(7.0829, grad_fn=<NegBackward0>)\n",
      "epoch 862 loss tensor(7.0825, grad_fn=<NegBackward0>)\n",
      "epoch 863 loss tensor(7.0822, grad_fn=<NegBackward0>)\n",
      "epoch 864 loss tensor(7.0819, grad_fn=<NegBackward0>)\n",
      "epoch 865 loss tensor(7.0816, grad_fn=<NegBackward0>)\n",
      "epoch 866 loss tensor(7.0812, grad_fn=<NegBackward0>)\n",
      "epoch 867 loss tensor(7.0809, grad_fn=<NegBackward0>)\n",
      "epoch 868 loss tensor(7.0806, grad_fn=<NegBackward0>)\n",
      "epoch 869 loss tensor(7.0803, grad_fn=<NegBackward0>)\n",
      "epoch 870 loss tensor(7.0799, grad_fn=<NegBackward0>)\n",
      "epoch 871 loss tensor(7.0796, grad_fn=<NegBackward0>)\n",
      "epoch 872 loss tensor(7.0793, grad_fn=<NegBackward0>)\n",
      "epoch 873 loss tensor(7.0790, grad_fn=<NegBackward0>)\n",
      "epoch 874 loss tensor(7.0786, grad_fn=<NegBackward0>)\n",
      "epoch 875 loss tensor(7.0783, grad_fn=<NegBackward0>)\n",
      "epoch 876 loss tensor(7.0780, grad_fn=<NegBackward0>)\n",
      "epoch 877 loss tensor(7.0777, grad_fn=<NegBackward0>)\n",
      "epoch 878 loss tensor(7.0774, grad_fn=<NegBackward0>)\n",
      "epoch 879 loss tensor(7.0770, grad_fn=<NegBackward0>)\n",
      "epoch 880 loss tensor(7.0767, grad_fn=<NegBackward0>)\n",
      "epoch 881 loss tensor(7.0764, grad_fn=<NegBackward0>)\n",
      "epoch 882 loss tensor(7.0761, grad_fn=<NegBackward0>)\n",
      "epoch 883 loss tensor(7.0757, grad_fn=<NegBackward0>)\n",
      "epoch 884 loss tensor(7.0754, grad_fn=<NegBackward0>)\n",
      "epoch 885 loss tensor(7.0751, grad_fn=<NegBackward0>)\n",
      "epoch 886 loss tensor(7.0748, grad_fn=<NegBackward0>)\n",
      "epoch 887 loss tensor(7.0745, grad_fn=<NegBackward0>)\n",
      "epoch 888 loss tensor(7.0741, grad_fn=<NegBackward0>)\n",
      "epoch 889 loss tensor(7.0738, grad_fn=<NegBackward0>)\n",
      "epoch 890 loss tensor(7.0735, grad_fn=<NegBackward0>)\n",
      "epoch 891 loss tensor(7.0732, grad_fn=<NegBackward0>)\n",
      "epoch 892 loss tensor(7.0729, grad_fn=<NegBackward0>)\n",
      "epoch 893 loss tensor(7.0725, grad_fn=<NegBackward0>)\n",
      "epoch 894 loss tensor(7.0722, grad_fn=<NegBackward0>)\n",
      "epoch 895 loss tensor(7.0719, grad_fn=<NegBackward0>)\n",
      "epoch 896 loss tensor(7.0716, grad_fn=<NegBackward0>)\n",
      "epoch 897 loss tensor(7.0713, grad_fn=<NegBackward0>)\n",
      "epoch 898 loss tensor(7.0710, grad_fn=<NegBackward0>)\n",
      "epoch 899 loss tensor(7.0706, grad_fn=<NegBackward0>)\n",
      "epoch 900 loss tensor(7.0703, grad_fn=<NegBackward0>)\n",
      "epoch 901 loss tensor(7.0700, grad_fn=<NegBackward0>)\n",
      "epoch 902 loss tensor(7.0697, grad_fn=<NegBackward0>)\n",
      "epoch 903 loss tensor(7.0694, grad_fn=<NegBackward0>)\n",
      "epoch 904 loss tensor(7.0690, grad_fn=<NegBackward0>)\n",
      "epoch 905 loss tensor(7.0687, grad_fn=<NegBackward0>)\n",
      "epoch 906 loss tensor(7.0684, grad_fn=<NegBackward0>)\n",
      "epoch 907 loss tensor(7.0681, grad_fn=<NegBackward0>)\n",
      "epoch 908 loss tensor(7.0678, grad_fn=<NegBackward0>)\n",
      "epoch 909 loss tensor(7.0675, grad_fn=<NegBackward0>)\n",
      "epoch 910 loss tensor(7.0671, grad_fn=<NegBackward0>)\n",
      "epoch 911 loss tensor(7.0668, grad_fn=<NegBackward0>)\n",
      "epoch 912 loss tensor(7.0665, grad_fn=<NegBackward0>)\n",
      "epoch 913 loss tensor(7.0662, grad_fn=<NegBackward0>)\n",
      "epoch 914 loss tensor(7.0659, grad_fn=<NegBackward0>)\n",
      "epoch 915 loss tensor(7.0656, grad_fn=<NegBackward0>)\n",
      "epoch 916 loss tensor(7.0653, grad_fn=<NegBackward0>)\n",
      "epoch 917 loss tensor(7.0649, grad_fn=<NegBackward0>)\n",
      "epoch 918 loss tensor(7.0646, grad_fn=<NegBackward0>)\n",
      "epoch 919 loss tensor(7.0643, grad_fn=<NegBackward0>)\n",
      "epoch 920 loss tensor(7.0640, grad_fn=<NegBackward0>)\n",
      "epoch 921 loss tensor(7.0637, grad_fn=<NegBackward0>)\n",
      "epoch 922 loss tensor(7.0634, grad_fn=<NegBackward0>)\n",
      "epoch 923 loss tensor(7.0631, grad_fn=<NegBackward0>)\n",
      "epoch 924 loss tensor(7.0627, grad_fn=<NegBackward0>)\n",
      "epoch 925 loss tensor(7.0624, grad_fn=<NegBackward0>)\n",
      "epoch 926 loss tensor(7.0621, grad_fn=<NegBackward0>)\n",
      "epoch 927 loss tensor(7.0618, grad_fn=<NegBackward0>)\n",
      "epoch 928 loss tensor(7.0615, grad_fn=<NegBackward0>)\n",
      "epoch 929 loss tensor(7.0612, grad_fn=<NegBackward0>)\n",
      "epoch 930 loss tensor(7.0609, grad_fn=<NegBackward0>)\n",
      "epoch 931 loss tensor(7.0605, grad_fn=<NegBackward0>)\n",
      "epoch 932 loss tensor(7.0602, grad_fn=<NegBackward0>)\n",
      "epoch 933 loss tensor(7.0599, grad_fn=<NegBackward0>)\n",
      "epoch 934 loss tensor(7.0596, grad_fn=<NegBackward0>)\n",
      "epoch 935 loss tensor(7.0593, grad_fn=<NegBackward0>)\n",
      "epoch 936 loss tensor(7.0590, grad_fn=<NegBackward0>)\n",
      "epoch 937 loss tensor(7.0587, grad_fn=<NegBackward0>)\n",
      "epoch 938 loss tensor(7.0584, grad_fn=<NegBackward0>)\n",
      "epoch 939 loss tensor(7.0580, grad_fn=<NegBackward0>)\n",
      "epoch 940 loss tensor(7.0577, grad_fn=<NegBackward0>)\n",
      "epoch 941 loss tensor(7.0574, grad_fn=<NegBackward0>)\n",
      "epoch 942 loss tensor(7.0571, grad_fn=<NegBackward0>)\n",
      "epoch 943 loss tensor(7.0568, grad_fn=<NegBackward0>)\n",
      "epoch 944 loss tensor(7.0565, grad_fn=<NegBackward0>)\n",
      "epoch 945 loss tensor(7.0562, grad_fn=<NegBackward0>)\n",
      "epoch 946 loss tensor(7.0559, grad_fn=<NegBackward0>)\n",
      "epoch 947 loss tensor(7.0556, grad_fn=<NegBackward0>)\n",
      "epoch 948 loss tensor(7.0553, grad_fn=<NegBackward0>)\n",
      "epoch 949 loss tensor(7.0549, grad_fn=<NegBackward0>)\n",
      "epoch 950 loss tensor(7.0546, grad_fn=<NegBackward0>)\n",
      "epoch 951 loss tensor(7.0543, grad_fn=<NegBackward0>)\n",
      "epoch 952 loss tensor(7.0540, grad_fn=<NegBackward0>)\n",
      "epoch 953 loss tensor(7.0537, grad_fn=<NegBackward0>)\n",
      "epoch 954 loss tensor(7.0534, grad_fn=<NegBackward0>)\n",
      "epoch 955 loss tensor(7.0531, grad_fn=<NegBackward0>)\n",
      "epoch 956 loss tensor(7.0528, grad_fn=<NegBackward0>)\n",
      "epoch 957 loss tensor(7.0525, grad_fn=<NegBackward0>)\n",
      "epoch 958 loss tensor(7.0522, grad_fn=<NegBackward0>)\n",
      "epoch 959 loss tensor(7.0519, grad_fn=<NegBackward0>)\n",
      "epoch 960 loss tensor(7.0515, grad_fn=<NegBackward0>)\n",
      "epoch 961 loss tensor(7.0512, grad_fn=<NegBackward0>)\n",
      "epoch 962 loss tensor(7.0509, grad_fn=<NegBackward0>)\n",
      "epoch 963 loss tensor(7.0506, grad_fn=<NegBackward0>)\n",
      "epoch 964 loss tensor(7.0503, grad_fn=<NegBackward0>)\n",
      "epoch 965 loss tensor(7.0500, grad_fn=<NegBackward0>)\n",
      "epoch 966 loss tensor(7.0497, grad_fn=<NegBackward0>)\n",
      "epoch 967 loss tensor(7.0494, grad_fn=<NegBackward0>)\n",
      "epoch 968 loss tensor(7.0491, grad_fn=<NegBackward0>)\n",
      "epoch 969 loss tensor(7.0488, grad_fn=<NegBackward0>)\n",
      "epoch 970 loss tensor(7.0485, grad_fn=<NegBackward0>)\n",
      "epoch 971 loss tensor(7.0482, grad_fn=<NegBackward0>)\n",
      "epoch 972 loss tensor(7.0479, grad_fn=<NegBackward0>)\n",
      "epoch 973 loss tensor(7.0476, grad_fn=<NegBackward0>)\n",
      "epoch 974 loss tensor(7.0472, grad_fn=<NegBackward0>)\n",
      "epoch 975 loss tensor(7.0469, grad_fn=<NegBackward0>)\n",
      "epoch 976 loss tensor(7.0466, grad_fn=<NegBackward0>)\n",
      "epoch 977 loss tensor(7.0463, grad_fn=<NegBackward0>)\n",
      "epoch 978 loss tensor(7.0460, grad_fn=<NegBackward0>)\n",
      "epoch 979 loss tensor(7.0457, grad_fn=<NegBackward0>)\n",
      "epoch 980 loss tensor(7.0454, grad_fn=<NegBackward0>)\n",
      "epoch 981 loss tensor(7.0451, grad_fn=<NegBackward0>)\n",
      "epoch 982 loss tensor(7.0448, grad_fn=<NegBackward0>)\n",
      "epoch 983 loss tensor(7.0445, grad_fn=<NegBackward0>)\n",
      "epoch 984 loss tensor(7.0442, grad_fn=<NegBackward0>)\n",
      "epoch 985 loss tensor(7.0439, grad_fn=<NegBackward0>)\n",
      "epoch 986 loss tensor(7.0436, grad_fn=<NegBackward0>)\n",
      "epoch 987 loss tensor(7.0433, grad_fn=<NegBackward0>)\n",
      "epoch 988 loss tensor(7.0430, grad_fn=<NegBackward0>)\n",
      "epoch 989 loss tensor(7.0427, grad_fn=<NegBackward0>)\n",
      "epoch 990 loss tensor(7.0424, grad_fn=<NegBackward0>)\n",
      "epoch 991 loss tensor(7.0421, grad_fn=<NegBackward0>)\n",
      "epoch 992 loss tensor(7.0418, grad_fn=<NegBackward0>)\n",
      "epoch 993 loss tensor(7.0415, grad_fn=<NegBackward0>)\n",
      "epoch 994 loss tensor(7.0412, grad_fn=<NegBackward0>)\n",
      "epoch 995 loss tensor(7.0409, grad_fn=<NegBackward0>)\n",
      "epoch 996 loss tensor(7.0405, grad_fn=<NegBackward0>)\n",
      "epoch 997 loss tensor(7.0402, grad_fn=<NegBackward0>)\n",
      "epoch 998 loss tensor(7.0399, grad_fn=<NegBackward0>)\n",
      "epoch 999 loss tensor(7.0396, grad_fn=<NegBackward0>)\n",
      "epoch 1000 loss tensor(7.0393, grad_fn=<NegBackward0>)\n",
      "epoch 1001 loss tensor(7.0390, grad_fn=<NegBackward0>)\n",
      "epoch 1002 loss tensor(7.0387, grad_fn=<NegBackward0>)\n",
      "epoch 1003 loss tensor(7.0384, grad_fn=<NegBackward0>)\n",
      "epoch 1004 loss tensor(7.0381, grad_fn=<NegBackward0>)\n",
      "epoch 1005 loss tensor(7.0378, grad_fn=<NegBackward0>)\n",
      "epoch 1006 loss tensor(7.0375, grad_fn=<NegBackward0>)\n",
      "epoch 1007 loss tensor(7.0372, grad_fn=<NegBackward0>)\n",
      "epoch 1008 loss tensor(7.0369, grad_fn=<NegBackward0>)\n",
      "epoch 1009 loss tensor(7.0366, grad_fn=<NegBackward0>)\n",
      "epoch 1010 loss tensor(7.0363, grad_fn=<NegBackward0>)\n",
      "epoch 1011 loss tensor(7.0360, grad_fn=<NegBackward0>)\n",
      "epoch 1012 loss tensor(7.0357, grad_fn=<NegBackward0>)\n",
      "epoch 1013 loss tensor(7.0354, grad_fn=<NegBackward0>)\n",
      "epoch 1014 loss tensor(7.0351, grad_fn=<NegBackward0>)\n",
      "epoch 1015 loss tensor(7.0348, grad_fn=<NegBackward0>)\n",
      "epoch 1016 loss tensor(7.0345, grad_fn=<NegBackward0>)\n",
      "epoch 1017 loss tensor(7.0342, grad_fn=<NegBackward0>)\n",
      "epoch 1018 loss tensor(7.0339, grad_fn=<NegBackward0>)\n",
      "epoch 1019 loss tensor(7.0336, grad_fn=<NegBackward0>)\n",
      "epoch 1020 loss tensor(7.0333, grad_fn=<NegBackward0>)\n",
      "epoch 1021 loss tensor(7.0330, grad_fn=<NegBackward0>)\n",
      "epoch 1022 loss tensor(7.0327, grad_fn=<NegBackward0>)\n",
      "epoch 1023 loss tensor(7.0324, grad_fn=<NegBackward0>)\n",
      "epoch 1024 loss tensor(7.0321, grad_fn=<NegBackward0>)\n",
      "epoch 1025 loss tensor(7.0318, grad_fn=<NegBackward0>)\n",
      "epoch 1026 loss tensor(7.0315, grad_fn=<NegBackward0>)\n",
      "epoch 1027 loss tensor(7.0312, grad_fn=<NegBackward0>)\n",
      "epoch 1028 loss tensor(7.0309, grad_fn=<NegBackward0>)\n",
      "epoch 1029 loss tensor(7.0306, grad_fn=<NegBackward0>)\n",
      "epoch 1030 loss tensor(7.0303, grad_fn=<NegBackward0>)\n",
      "epoch 1031 loss tensor(7.0300, grad_fn=<NegBackward0>)\n",
      "epoch 1032 loss tensor(7.0297, grad_fn=<NegBackward0>)\n",
      "epoch 1033 loss tensor(7.0294, grad_fn=<NegBackward0>)\n",
      "epoch 1034 loss tensor(7.0291, grad_fn=<NegBackward0>)\n",
      "epoch 1035 loss tensor(7.0288, grad_fn=<NegBackward0>)\n",
      "epoch 1036 loss tensor(7.0285, grad_fn=<NegBackward0>)\n",
      "epoch 1037 loss tensor(7.0282, grad_fn=<NegBackward0>)\n",
      "epoch 1038 loss tensor(7.0279, grad_fn=<NegBackward0>)\n",
      "epoch 1039 loss tensor(7.0276, grad_fn=<NegBackward0>)\n",
      "epoch 1040 loss tensor(7.0274, grad_fn=<NegBackward0>)\n",
      "epoch 1041 loss tensor(7.0271, grad_fn=<NegBackward0>)\n",
      "epoch 1042 loss tensor(7.0268, grad_fn=<NegBackward0>)\n",
      "epoch 1043 loss tensor(7.0265, grad_fn=<NegBackward0>)\n",
      "epoch 1044 loss tensor(7.0262, grad_fn=<NegBackward0>)\n",
      "epoch 1045 loss tensor(7.0259, grad_fn=<NegBackward0>)\n",
      "epoch 1046 loss tensor(7.0256, grad_fn=<NegBackward0>)\n",
      "epoch 1047 loss tensor(7.0253, grad_fn=<NegBackward0>)\n",
      "epoch 1048 loss tensor(7.0250, grad_fn=<NegBackward0>)\n",
      "epoch 1049 loss tensor(7.0247, grad_fn=<NegBackward0>)\n",
      "epoch 1050 loss tensor(7.0244, grad_fn=<NegBackward0>)\n",
      "epoch 1051 loss tensor(7.0241, grad_fn=<NegBackward0>)\n",
      "epoch 1052 loss tensor(7.0238, grad_fn=<NegBackward0>)\n",
      "epoch 1053 loss tensor(7.0235, grad_fn=<NegBackward0>)\n",
      "epoch 1054 loss tensor(7.0232, grad_fn=<NegBackward0>)\n",
      "epoch 1055 loss tensor(7.0229, grad_fn=<NegBackward0>)\n",
      "epoch 1056 loss tensor(7.0226, grad_fn=<NegBackward0>)\n",
      "epoch 1057 loss tensor(7.0223, grad_fn=<NegBackward0>)\n",
      "epoch 1058 loss tensor(7.0220, grad_fn=<NegBackward0>)\n",
      "epoch 1059 loss tensor(7.0217, grad_fn=<NegBackward0>)\n",
      "epoch 1060 loss tensor(7.0214, grad_fn=<NegBackward0>)\n",
      "epoch 1061 loss tensor(7.0211, grad_fn=<NegBackward0>)\n",
      "epoch 1062 loss tensor(7.0208, grad_fn=<NegBackward0>)\n",
      "epoch 1063 loss tensor(7.0205, grad_fn=<NegBackward0>)\n",
      "epoch 1064 loss tensor(7.0202, grad_fn=<NegBackward0>)\n",
      "epoch 1065 loss tensor(7.0200, grad_fn=<NegBackward0>)\n",
      "epoch 1066 loss tensor(7.0197, grad_fn=<NegBackward0>)\n",
      "epoch 1067 loss tensor(7.0194, grad_fn=<NegBackward0>)\n",
      "epoch 1068 loss tensor(7.0191, grad_fn=<NegBackward0>)\n",
      "epoch 1069 loss tensor(7.0188, grad_fn=<NegBackward0>)\n",
      "epoch 1070 loss tensor(7.0185, grad_fn=<NegBackward0>)\n",
      "epoch 1071 loss tensor(7.0182, grad_fn=<NegBackward0>)\n",
      "epoch 1072 loss tensor(7.0179, grad_fn=<NegBackward0>)\n",
      "epoch 1073 loss tensor(7.0176, grad_fn=<NegBackward0>)\n",
      "epoch 1074 loss tensor(7.0173, grad_fn=<NegBackward0>)\n",
      "epoch 1075 loss tensor(7.0170, grad_fn=<NegBackward0>)\n",
      "epoch 1076 loss tensor(7.0167, grad_fn=<NegBackward0>)\n",
      "epoch 1077 loss tensor(7.0164, grad_fn=<NegBackward0>)\n",
      "epoch 1078 loss tensor(7.0161, grad_fn=<NegBackward0>)\n",
      "epoch 1079 loss tensor(7.0158, grad_fn=<NegBackward0>)\n",
      "epoch 1080 loss tensor(7.0155, grad_fn=<NegBackward0>)\n",
      "epoch 1081 loss tensor(7.0153, grad_fn=<NegBackward0>)\n",
      "epoch 1082 loss tensor(7.0150, grad_fn=<NegBackward0>)\n",
      "epoch 1083 loss tensor(7.0147, grad_fn=<NegBackward0>)\n",
      "epoch 1084 loss tensor(7.0144, grad_fn=<NegBackward0>)\n",
      "epoch 1085 loss tensor(7.0141, grad_fn=<NegBackward0>)\n",
      "epoch 1086 loss tensor(7.0138, grad_fn=<NegBackward0>)\n",
      "epoch 1087 loss tensor(7.0135, grad_fn=<NegBackward0>)\n",
      "epoch 1088 loss tensor(7.0132, grad_fn=<NegBackward0>)\n",
      "epoch 1089 loss tensor(7.0129, grad_fn=<NegBackward0>)\n",
      "epoch 1090 loss tensor(7.0126, grad_fn=<NegBackward0>)\n",
      "epoch 1091 loss tensor(7.0123, grad_fn=<NegBackward0>)\n",
      "epoch 1092 loss tensor(7.0120, grad_fn=<NegBackward0>)\n",
      "epoch 1093 loss tensor(7.0117, grad_fn=<NegBackward0>)\n",
      "epoch 1094 loss tensor(7.0115, grad_fn=<NegBackward0>)\n",
      "epoch 1095 loss tensor(7.0112, grad_fn=<NegBackward0>)\n",
      "epoch 1096 loss tensor(7.0109, grad_fn=<NegBackward0>)\n",
      "epoch 1097 loss tensor(7.0106, grad_fn=<NegBackward0>)\n",
      "epoch 1098 loss tensor(7.0103, grad_fn=<NegBackward0>)\n",
      "epoch 1099 loss tensor(7.0100, grad_fn=<NegBackward0>)\n",
      "epoch 1100 loss tensor(7.0097, grad_fn=<NegBackward0>)\n",
      "epoch 1101 loss tensor(7.0094, grad_fn=<NegBackward0>)\n",
      "epoch 1102 loss tensor(7.0091, grad_fn=<NegBackward0>)\n",
      "epoch 1103 loss tensor(7.0088, grad_fn=<NegBackward0>)\n",
      "epoch 1104 loss tensor(7.0085, grad_fn=<NegBackward0>)\n",
      "epoch 1105 loss tensor(7.0083, grad_fn=<NegBackward0>)\n",
      "epoch 1106 loss tensor(7.0080, grad_fn=<NegBackward0>)\n",
      "epoch 1107 loss tensor(7.0077, grad_fn=<NegBackward0>)\n",
      "epoch 1108 loss tensor(7.0074, grad_fn=<NegBackward0>)\n",
      "epoch 1109 loss tensor(7.0071, grad_fn=<NegBackward0>)\n",
      "epoch 1110 loss tensor(7.0068, grad_fn=<NegBackward0>)\n",
      "epoch 1111 loss tensor(7.0065, grad_fn=<NegBackward0>)\n",
      "epoch 1112 loss tensor(7.0062, grad_fn=<NegBackward0>)\n",
      "epoch 1113 loss tensor(7.0059, grad_fn=<NegBackward0>)\n",
      "epoch 1114 loss tensor(7.0056, grad_fn=<NegBackward0>)\n",
      "epoch 1115 loss tensor(7.0054, grad_fn=<NegBackward0>)\n",
      "epoch 1116 loss tensor(7.0051, grad_fn=<NegBackward0>)\n",
      "epoch 1117 loss tensor(7.0048, grad_fn=<NegBackward0>)\n",
      "epoch 1118 loss tensor(7.0045, grad_fn=<NegBackward0>)\n",
      "epoch 1119 loss tensor(7.0042, grad_fn=<NegBackward0>)\n",
      "epoch 1120 loss tensor(7.0039, grad_fn=<NegBackward0>)\n",
      "epoch 1121 loss tensor(7.0036, grad_fn=<NegBackward0>)\n",
      "epoch 1122 loss tensor(7.0033, grad_fn=<NegBackward0>)\n",
      "epoch 1123 loss tensor(7.0031, grad_fn=<NegBackward0>)\n",
      "epoch 1124 loss tensor(7.0028, grad_fn=<NegBackward0>)\n",
      "epoch 1125 loss tensor(7.0025, grad_fn=<NegBackward0>)\n",
      "epoch 1126 loss tensor(7.0022, grad_fn=<NegBackward0>)\n",
      "epoch 1127 loss tensor(7.0019, grad_fn=<NegBackward0>)\n",
      "epoch 1128 loss tensor(7.0016, grad_fn=<NegBackward0>)\n",
      "epoch 1129 loss tensor(7.0013, grad_fn=<NegBackward0>)\n",
      "epoch 1130 loss tensor(7.0010, grad_fn=<NegBackward0>)\n",
      "epoch 1131 loss tensor(7.0008, grad_fn=<NegBackward0>)\n",
      "epoch 1132 loss tensor(7.0005, grad_fn=<NegBackward0>)\n",
      "epoch 1133 loss tensor(7.0002, grad_fn=<NegBackward0>)\n",
      "epoch 1134 loss tensor(6.9999, grad_fn=<NegBackward0>)\n",
      "epoch 1135 loss tensor(6.9996, grad_fn=<NegBackward0>)\n",
      "epoch 1136 loss tensor(6.9993, grad_fn=<NegBackward0>)\n",
      "epoch 1137 loss tensor(6.9990, grad_fn=<NegBackward0>)\n",
      "epoch 1138 loss tensor(6.9987, grad_fn=<NegBackward0>)\n",
      "epoch 1139 loss tensor(6.9985, grad_fn=<NegBackward0>)\n",
      "epoch 1140 loss tensor(6.9982, grad_fn=<NegBackward0>)\n",
      "epoch 1141 loss tensor(6.9979, grad_fn=<NegBackward0>)\n",
      "epoch 1142 loss tensor(6.9976, grad_fn=<NegBackward0>)\n",
      "epoch 1143 loss tensor(6.9973, grad_fn=<NegBackward0>)\n",
      "epoch 1144 loss tensor(6.9970, grad_fn=<NegBackward0>)\n",
      "epoch 1145 loss tensor(6.9967, grad_fn=<NegBackward0>)\n",
      "epoch 1146 loss tensor(6.9965, grad_fn=<NegBackward0>)\n",
      "epoch 1147 loss tensor(6.9962, grad_fn=<NegBackward0>)\n",
      "epoch 1148 loss tensor(6.9959, grad_fn=<NegBackward0>)\n",
      "epoch 1149 loss tensor(6.9956, grad_fn=<NegBackward0>)\n",
      "epoch 1150 loss tensor(6.9953, grad_fn=<NegBackward0>)\n",
      "epoch 1151 loss tensor(6.9950, grad_fn=<NegBackward0>)\n",
      "epoch 1152 loss tensor(6.9947, grad_fn=<NegBackward0>)\n",
      "epoch 1153 loss tensor(6.9945, grad_fn=<NegBackward0>)\n",
      "epoch 1154 loss tensor(6.9942, grad_fn=<NegBackward0>)\n",
      "epoch 1155 loss tensor(6.9939, grad_fn=<NegBackward0>)\n",
      "epoch 1156 loss tensor(6.9936, grad_fn=<NegBackward0>)\n",
      "epoch 1157 loss tensor(6.9933, grad_fn=<NegBackward0>)\n",
      "epoch 1158 loss tensor(6.9930, grad_fn=<NegBackward0>)\n",
      "epoch 1159 loss tensor(6.9927, grad_fn=<NegBackward0>)\n",
      "epoch 1160 loss tensor(6.9925, grad_fn=<NegBackward0>)\n",
      "epoch 1161 loss tensor(6.9922, grad_fn=<NegBackward0>)\n",
      "epoch 1162 loss tensor(6.9919, grad_fn=<NegBackward0>)\n",
      "epoch 1163 loss tensor(6.9916, grad_fn=<NegBackward0>)\n",
      "epoch 1164 loss tensor(6.9913, grad_fn=<NegBackward0>)\n",
      "epoch 1165 loss tensor(6.9910, grad_fn=<NegBackward0>)\n",
      "epoch 1166 loss tensor(6.9908, grad_fn=<NegBackward0>)\n",
      "epoch 1167 loss tensor(6.9905, grad_fn=<NegBackward0>)\n",
      "epoch 1168 loss tensor(6.9902, grad_fn=<NegBackward0>)\n",
      "epoch 1169 loss tensor(6.9899, grad_fn=<NegBackward0>)\n",
      "epoch 1170 loss tensor(6.9896, grad_fn=<NegBackward0>)\n",
      "epoch 1171 loss tensor(6.9893, grad_fn=<NegBackward0>)\n",
      "epoch 1172 loss tensor(6.9891, grad_fn=<NegBackward0>)\n",
      "epoch 1173 loss tensor(6.9888, grad_fn=<NegBackward0>)\n",
      "epoch 1174 loss tensor(6.9885, grad_fn=<NegBackward0>)\n",
      "epoch 1175 loss tensor(6.9882, grad_fn=<NegBackward0>)\n",
      "epoch 1176 loss tensor(6.9879, grad_fn=<NegBackward0>)\n",
      "epoch 1177 loss tensor(6.9876, grad_fn=<NegBackward0>)\n",
      "epoch 1178 loss tensor(6.9874, grad_fn=<NegBackward0>)\n",
      "epoch 1179 loss tensor(6.9871, grad_fn=<NegBackward0>)\n",
      "epoch 1180 loss tensor(6.9868, grad_fn=<NegBackward0>)\n",
      "epoch 1181 loss tensor(6.9865, grad_fn=<NegBackward0>)\n",
      "epoch 1182 loss tensor(6.9862, grad_fn=<NegBackward0>)\n",
      "epoch 1183 loss tensor(6.9860, grad_fn=<NegBackward0>)\n",
      "epoch 1184 loss tensor(6.9857, grad_fn=<NegBackward0>)\n",
      "epoch 1185 loss tensor(6.9854, grad_fn=<NegBackward0>)\n",
      "epoch 1186 loss tensor(6.9851, grad_fn=<NegBackward0>)\n",
      "epoch 1187 loss tensor(6.9848, grad_fn=<NegBackward0>)\n",
      "epoch 1188 loss tensor(6.9845, grad_fn=<NegBackward0>)\n",
      "epoch 1189 loss tensor(6.9843, grad_fn=<NegBackward0>)\n",
      "epoch 1190 loss tensor(6.9840, grad_fn=<NegBackward0>)\n",
      "epoch 1191 loss tensor(6.9837, grad_fn=<NegBackward0>)\n",
      "epoch 1192 loss tensor(6.9834, grad_fn=<NegBackward0>)\n",
      "epoch 1193 loss tensor(6.9831, grad_fn=<NegBackward0>)\n",
      "epoch 1194 loss tensor(6.9829, grad_fn=<NegBackward0>)\n",
      "epoch 1195 loss tensor(6.9826, grad_fn=<NegBackward0>)\n",
      "epoch 1196 loss tensor(6.9823, grad_fn=<NegBackward0>)\n",
      "epoch 1197 loss tensor(6.9820, grad_fn=<NegBackward0>)\n",
      "epoch 1198 loss tensor(6.9817, grad_fn=<NegBackward0>)\n",
      "epoch 1199 loss tensor(6.9815, grad_fn=<NegBackward0>)\n",
      "epoch 1200 loss tensor(6.9812, grad_fn=<NegBackward0>)\n",
      "epoch 1201 loss tensor(6.9809, grad_fn=<NegBackward0>)\n",
      "epoch 1202 loss tensor(6.9806, grad_fn=<NegBackward0>)\n",
      "epoch 1203 loss tensor(6.9803, grad_fn=<NegBackward0>)\n",
      "epoch 1204 loss tensor(6.9801, grad_fn=<NegBackward0>)\n",
      "epoch 1205 loss tensor(6.9798, grad_fn=<NegBackward0>)\n",
      "epoch 1206 loss tensor(6.9795, grad_fn=<NegBackward0>)\n",
      "epoch 1207 loss tensor(6.9792, grad_fn=<NegBackward0>)\n",
      "epoch 1208 loss tensor(6.9789, grad_fn=<NegBackward0>)\n",
      "epoch 1209 loss tensor(6.9787, grad_fn=<NegBackward0>)\n",
      "epoch 1210 loss tensor(6.9784, grad_fn=<NegBackward0>)\n",
      "epoch 1211 loss tensor(6.9781, grad_fn=<NegBackward0>)\n",
      "epoch 1212 loss tensor(6.9778, grad_fn=<NegBackward0>)\n",
      "epoch 1213 loss tensor(6.9775, grad_fn=<NegBackward0>)\n",
      "epoch 1214 loss tensor(6.9773, grad_fn=<NegBackward0>)\n",
      "epoch 1215 loss tensor(6.9770, grad_fn=<NegBackward0>)\n",
      "epoch 1216 loss tensor(6.9767, grad_fn=<NegBackward0>)\n",
      "epoch 1217 loss tensor(6.9764, grad_fn=<NegBackward0>)\n",
      "epoch 1218 loss tensor(6.9761, grad_fn=<NegBackward0>)\n",
      "epoch 1219 loss tensor(6.9759, grad_fn=<NegBackward0>)\n",
      "epoch 1220 loss tensor(6.9756, grad_fn=<NegBackward0>)\n",
      "epoch 1221 loss tensor(6.9753, grad_fn=<NegBackward0>)\n",
      "epoch 1222 loss tensor(6.9750, grad_fn=<NegBackward0>)\n",
      "epoch 1223 loss tensor(6.9748, grad_fn=<NegBackward0>)\n",
      "epoch 1224 loss tensor(6.9745, grad_fn=<NegBackward0>)\n",
      "epoch 1225 loss tensor(6.9742, grad_fn=<NegBackward0>)\n",
      "epoch 1226 loss tensor(6.9739, grad_fn=<NegBackward0>)\n",
      "epoch 1227 loss tensor(6.9736, grad_fn=<NegBackward0>)\n",
      "epoch 1228 loss tensor(6.9734, grad_fn=<NegBackward0>)\n",
      "epoch 1229 loss tensor(6.9731, grad_fn=<NegBackward0>)\n",
      "epoch 1230 loss tensor(6.9728, grad_fn=<NegBackward0>)\n",
      "epoch 1231 loss tensor(6.9725, grad_fn=<NegBackward0>)\n",
      "epoch 1232 loss tensor(6.9723, grad_fn=<NegBackward0>)\n",
      "epoch 1233 loss tensor(6.9720, grad_fn=<NegBackward0>)\n",
      "epoch 1234 loss tensor(6.9717, grad_fn=<NegBackward0>)\n",
      "epoch 1235 loss tensor(6.9714, grad_fn=<NegBackward0>)\n",
      "epoch 1236 loss tensor(6.9711, grad_fn=<NegBackward0>)\n",
      "epoch 1237 loss tensor(6.9709, grad_fn=<NegBackward0>)\n",
      "epoch 1238 loss tensor(6.9706, grad_fn=<NegBackward0>)\n",
      "epoch 1239 loss tensor(6.9703, grad_fn=<NegBackward0>)\n",
      "epoch 1240 loss tensor(6.9700, grad_fn=<NegBackward0>)\n",
      "epoch 1241 loss tensor(6.9698, grad_fn=<NegBackward0>)\n",
      "epoch 1242 loss tensor(6.9695, grad_fn=<NegBackward0>)\n",
      "epoch 1243 loss tensor(6.9692, grad_fn=<NegBackward0>)\n",
      "epoch 1244 loss tensor(6.9689, grad_fn=<NegBackward0>)\n",
      "epoch 1245 loss tensor(6.9687, grad_fn=<NegBackward0>)\n",
      "epoch 1246 loss tensor(6.9684, grad_fn=<NegBackward0>)\n",
      "epoch 1247 loss tensor(6.9681, grad_fn=<NegBackward0>)\n",
      "epoch 1248 loss tensor(6.9678, grad_fn=<NegBackward0>)\n",
      "epoch 1249 loss tensor(6.9675, grad_fn=<NegBackward0>)\n",
      "epoch 1250 loss tensor(6.9673, grad_fn=<NegBackward0>)\n",
      "epoch 1251 loss tensor(6.9670, grad_fn=<NegBackward0>)\n",
      "epoch 1252 loss tensor(6.9667, grad_fn=<NegBackward0>)\n",
      "epoch 1253 loss tensor(6.9664, grad_fn=<NegBackward0>)\n",
      "epoch 1254 loss tensor(6.9662, grad_fn=<NegBackward0>)\n",
      "epoch 1255 loss tensor(6.9659, grad_fn=<NegBackward0>)\n",
      "epoch 1256 loss tensor(6.9656, grad_fn=<NegBackward0>)\n",
      "epoch 1257 loss tensor(6.9653, grad_fn=<NegBackward0>)\n",
      "epoch 1258 loss tensor(6.9651, grad_fn=<NegBackward0>)\n",
      "epoch 1259 loss tensor(6.9648, grad_fn=<NegBackward0>)\n",
      "epoch 1260 loss tensor(6.9645, grad_fn=<NegBackward0>)\n",
      "epoch 1261 loss tensor(6.9642, grad_fn=<NegBackward0>)\n",
      "epoch 1262 loss tensor(6.9640, grad_fn=<NegBackward0>)\n",
      "epoch 1263 loss tensor(6.9637, grad_fn=<NegBackward0>)\n",
      "epoch 1264 loss tensor(6.9634, grad_fn=<NegBackward0>)\n",
      "epoch 1265 loss tensor(6.9631, grad_fn=<NegBackward0>)\n",
      "epoch 1266 loss tensor(6.9629, grad_fn=<NegBackward0>)\n",
      "epoch 1267 loss tensor(6.9626, grad_fn=<NegBackward0>)\n",
      "epoch 1268 loss tensor(6.9623, grad_fn=<NegBackward0>)\n",
      "epoch 1269 loss tensor(6.9620, grad_fn=<NegBackward0>)\n",
      "epoch 1270 loss tensor(6.9618, grad_fn=<NegBackward0>)\n",
      "epoch 1271 loss tensor(6.9615, grad_fn=<NegBackward0>)\n",
      "epoch 1272 loss tensor(6.9612, grad_fn=<NegBackward0>)\n",
      "epoch 1273 loss tensor(6.9610, grad_fn=<NegBackward0>)\n",
      "epoch 1274 loss tensor(6.9607, grad_fn=<NegBackward0>)\n",
      "epoch 1275 loss tensor(6.9604, grad_fn=<NegBackward0>)\n",
      "epoch 1276 loss tensor(6.9601, grad_fn=<NegBackward0>)\n",
      "epoch 1277 loss tensor(6.9599, grad_fn=<NegBackward0>)\n",
      "epoch 1278 loss tensor(6.9596, grad_fn=<NegBackward0>)\n",
      "epoch 1279 loss tensor(6.9593, grad_fn=<NegBackward0>)\n",
      "epoch 1280 loss tensor(6.9590, grad_fn=<NegBackward0>)\n",
      "epoch 1281 loss tensor(6.9588, grad_fn=<NegBackward0>)\n",
      "epoch 1282 loss tensor(6.9585, grad_fn=<NegBackward0>)\n",
      "epoch 1283 loss tensor(6.9582, grad_fn=<NegBackward0>)\n",
      "epoch 1284 loss tensor(6.9579, grad_fn=<NegBackward0>)\n",
      "epoch 1285 loss tensor(6.9577, grad_fn=<NegBackward0>)\n",
      "epoch 1286 loss tensor(6.9574, grad_fn=<NegBackward0>)\n",
      "epoch 1287 loss tensor(6.9571, grad_fn=<NegBackward0>)\n",
      "epoch 1288 loss tensor(6.9569, grad_fn=<NegBackward0>)\n",
      "epoch 1289 loss tensor(6.9566, grad_fn=<NegBackward0>)\n",
      "epoch 1290 loss tensor(6.9563, grad_fn=<NegBackward0>)\n",
      "epoch 1291 loss tensor(6.9560, grad_fn=<NegBackward0>)\n",
      "epoch 1292 loss tensor(6.9558, grad_fn=<NegBackward0>)\n",
      "epoch 1293 loss tensor(6.9555, grad_fn=<NegBackward0>)\n",
      "epoch 1294 loss tensor(6.9552, grad_fn=<NegBackward0>)\n",
      "epoch 1295 loss tensor(6.9549, grad_fn=<NegBackward0>)\n",
      "epoch 1296 loss tensor(6.9547, grad_fn=<NegBackward0>)\n",
      "epoch 1297 loss tensor(6.9544, grad_fn=<NegBackward0>)\n",
      "epoch 1298 loss tensor(6.9541, grad_fn=<NegBackward0>)\n",
      "epoch 1299 loss tensor(6.9539, grad_fn=<NegBackward0>)\n",
      "epoch 1300 loss tensor(6.9536, grad_fn=<NegBackward0>)\n",
      "epoch 1301 loss tensor(6.9533, grad_fn=<NegBackward0>)\n",
      "epoch 1302 loss tensor(6.9530, grad_fn=<NegBackward0>)\n",
      "epoch 1303 loss tensor(6.9528, grad_fn=<NegBackward0>)\n",
      "epoch 1304 loss tensor(6.9525, grad_fn=<NegBackward0>)\n",
      "epoch 1305 loss tensor(6.9522, grad_fn=<NegBackward0>)\n",
      "epoch 1306 loss tensor(6.9520, grad_fn=<NegBackward0>)\n",
      "epoch 1307 loss tensor(6.9517, grad_fn=<NegBackward0>)\n",
      "epoch 1308 loss tensor(6.9514, grad_fn=<NegBackward0>)\n",
      "epoch 1309 loss tensor(6.9512, grad_fn=<NegBackward0>)\n",
      "epoch 1310 loss tensor(6.9509, grad_fn=<NegBackward0>)\n",
      "epoch 1311 loss tensor(6.9506, grad_fn=<NegBackward0>)\n",
      "epoch 1312 loss tensor(6.9503, grad_fn=<NegBackward0>)\n",
      "epoch 1313 loss tensor(6.9501, grad_fn=<NegBackward0>)\n",
      "epoch 1314 loss tensor(6.9498, grad_fn=<NegBackward0>)\n",
      "epoch 1315 loss tensor(6.9495, grad_fn=<NegBackward0>)\n",
      "epoch 1316 loss tensor(6.9493, grad_fn=<NegBackward0>)\n",
      "epoch 1317 loss tensor(6.9490, grad_fn=<NegBackward0>)\n",
      "epoch 1318 loss tensor(6.9487, grad_fn=<NegBackward0>)\n",
      "epoch 1319 loss tensor(6.9484, grad_fn=<NegBackward0>)\n",
      "epoch 1320 loss tensor(6.9482, grad_fn=<NegBackward0>)\n",
      "epoch 1321 loss tensor(6.9479, grad_fn=<NegBackward0>)\n",
      "epoch 1322 loss tensor(6.9476, grad_fn=<NegBackward0>)\n",
      "epoch 1323 loss tensor(6.9474, grad_fn=<NegBackward0>)\n",
      "epoch 1324 loss tensor(6.9471, grad_fn=<NegBackward0>)\n",
      "epoch 1325 loss tensor(6.9468, grad_fn=<NegBackward0>)\n",
      "epoch 1326 loss tensor(6.9466, grad_fn=<NegBackward0>)\n",
      "epoch 1327 loss tensor(6.9463, grad_fn=<NegBackward0>)\n",
      "epoch 1328 loss tensor(6.9460, grad_fn=<NegBackward0>)\n",
      "epoch 1329 loss tensor(6.9458, grad_fn=<NegBackward0>)\n",
      "epoch 1330 loss tensor(6.9455, grad_fn=<NegBackward0>)\n",
      "epoch 1331 loss tensor(6.9452, grad_fn=<NegBackward0>)\n",
      "epoch 1332 loss tensor(6.9449, grad_fn=<NegBackward0>)\n",
      "epoch 1333 loss tensor(6.9447, grad_fn=<NegBackward0>)\n",
      "epoch 1334 loss tensor(6.9444, grad_fn=<NegBackward0>)\n",
      "epoch 1335 loss tensor(6.9441, grad_fn=<NegBackward0>)\n",
      "epoch 1336 loss tensor(6.9439, grad_fn=<NegBackward0>)\n",
      "epoch 1337 loss tensor(6.9436, grad_fn=<NegBackward0>)\n",
      "epoch 1338 loss tensor(6.9433, grad_fn=<NegBackward0>)\n",
      "epoch 1339 loss tensor(6.9431, grad_fn=<NegBackward0>)\n",
      "epoch 1340 loss tensor(6.9428, grad_fn=<NegBackward0>)\n",
      "epoch 1341 loss tensor(6.9425, grad_fn=<NegBackward0>)\n",
      "epoch 1342 loss tensor(6.9423, grad_fn=<NegBackward0>)\n",
      "epoch 1343 loss tensor(6.9420, grad_fn=<NegBackward0>)\n",
      "epoch 1344 loss tensor(6.9417, grad_fn=<NegBackward0>)\n",
      "epoch 1345 loss tensor(6.9415, grad_fn=<NegBackward0>)\n",
      "epoch 1346 loss tensor(6.9412, grad_fn=<NegBackward0>)\n",
      "epoch 1347 loss tensor(6.9409, grad_fn=<NegBackward0>)\n",
      "epoch 1348 loss tensor(6.9407, grad_fn=<NegBackward0>)\n",
      "epoch 1349 loss tensor(6.9404, grad_fn=<NegBackward0>)\n",
      "epoch 1350 loss tensor(6.9401, grad_fn=<NegBackward0>)\n",
      "epoch 1351 loss tensor(6.9399, grad_fn=<NegBackward0>)\n",
      "epoch 1352 loss tensor(6.9396, grad_fn=<NegBackward0>)\n",
      "epoch 1353 loss tensor(6.9393, grad_fn=<NegBackward0>)\n",
      "epoch 1354 loss tensor(6.9391, grad_fn=<NegBackward0>)\n",
      "epoch 1355 loss tensor(6.9388, grad_fn=<NegBackward0>)\n",
      "epoch 1356 loss tensor(6.9385, grad_fn=<NegBackward0>)\n",
      "epoch 1357 loss tensor(6.9383, grad_fn=<NegBackward0>)\n",
      "epoch 1358 loss tensor(6.9380, grad_fn=<NegBackward0>)\n",
      "epoch 1359 loss tensor(6.9377, grad_fn=<NegBackward0>)\n",
      "epoch 1360 loss tensor(6.9375, grad_fn=<NegBackward0>)\n",
      "epoch 1361 loss tensor(6.9372, grad_fn=<NegBackward0>)\n",
      "epoch 1362 loss tensor(6.9369, grad_fn=<NegBackward0>)\n",
      "epoch 1363 loss tensor(6.9367, grad_fn=<NegBackward0>)\n",
      "epoch 1364 loss tensor(6.9364, grad_fn=<NegBackward0>)\n",
      "epoch 1365 loss tensor(6.9361, grad_fn=<NegBackward0>)\n",
      "epoch 1366 loss tensor(6.9359, grad_fn=<NegBackward0>)\n",
      "epoch 1367 loss tensor(6.9356, grad_fn=<NegBackward0>)\n",
      "epoch 1368 loss tensor(6.9353, grad_fn=<NegBackward0>)\n",
      "epoch 1369 loss tensor(6.9351, grad_fn=<NegBackward0>)\n",
      "epoch 1370 loss tensor(6.9348, grad_fn=<NegBackward0>)\n",
      "epoch 1371 loss tensor(6.9345, grad_fn=<NegBackward0>)\n",
      "epoch 1372 loss tensor(6.9343, grad_fn=<NegBackward0>)\n",
      "epoch 1373 loss tensor(6.9340, grad_fn=<NegBackward0>)\n",
      "epoch 1374 loss tensor(6.9337, grad_fn=<NegBackward0>)\n",
      "epoch 1375 loss tensor(6.9335, grad_fn=<NegBackward0>)\n",
      "epoch 1376 loss tensor(6.9332, grad_fn=<NegBackward0>)\n",
      "epoch 1377 loss tensor(6.9329, grad_fn=<NegBackward0>)\n",
      "epoch 1378 loss tensor(6.9327, grad_fn=<NegBackward0>)\n",
      "epoch 1379 loss tensor(6.9324, grad_fn=<NegBackward0>)\n",
      "epoch 1380 loss tensor(6.9321, grad_fn=<NegBackward0>)\n",
      "epoch 1381 loss tensor(6.9319, grad_fn=<NegBackward0>)\n",
      "epoch 1382 loss tensor(6.9316, grad_fn=<NegBackward0>)\n",
      "epoch 1383 loss tensor(6.9314, grad_fn=<NegBackward0>)\n",
      "epoch 1384 loss tensor(6.9311, grad_fn=<NegBackward0>)\n",
      "epoch 1385 loss tensor(6.9308, grad_fn=<NegBackward0>)\n",
      "epoch 1386 loss tensor(6.9306, grad_fn=<NegBackward0>)\n",
      "epoch 1387 loss tensor(6.9303, grad_fn=<NegBackward0>)\n",
      "epoch 1388 loss tensor(6.9300, grad_fn=<NegBackward0>)\n",
      "epoch 1389 loss tensor(6.9298, grad_fn=<NegBackward0>)\n",
      "epoch 1390 loss tensor(6.9295, grad_fn=<NegBackward0>)\n",
      "epoch 1391 loss tensor(6.9292, grad_fn=<NegBackward0>)\n",
      "epoch 1392 loss tensor(6.9290, grad_fn=<NegBackward0>)\n",
      "epoch 1393 loss tensor(6.9287, grad_fn=<NegBackward0>)\n",
      "epoch 1394 loss tensor(6.9284, grad_fn=<NegBackward0>)\n",
      "epoch 1395 loss tensor(6.9282, grad_fn=<NegBackward0>)\n",
      "epoch 1396 loss tensor(6.9279, grad_fn=<NegBackward0>)\n",
      "epoch 1397 loss tensor(6.9277, grad_fn=<NegBackward0>)\n",
      "epoch 1398 loss tensor(6.9274, grad_fn=<NegBackward0>)\n",
      "epoch 1399 loss tensor(6.9271, grad_fn=<NegBackward0>)\n",
      "epoch 1400 loss tensor(6.9269, grad_fn=<NegBackward0>)\n",
      "epoch 1401 loss tensor(6.9266, grad_fn=<NegBackward0>)\n",
      "epoch 1402 loss tensor(6.9263, grad_fn=<NegBackward0>)\n",
      "epoch 1403 loss tensor(6.9261, grad_fn=<NegBackward0>)\n",
      "epoch 1404 loss tensor(6.9258, grad_fn=<NegBackward0>)\n",
      "epoch 1405 loss tensor(6.9256, grad_fn=<NegBackward0>)\n",
      "epoch 1406 loss tensor(6.9253, grad_fn=<NegBackward0>)\n",
      "epoch 1407 loss tensor(6.9250, grad_fn=<NegBackward0>)\n",
      "epoch 1408 loss tensor(6.9248, grad_fn=<NegBackward0>)\n",
      "epoch 1409 loss tensor(6.9245, grad_fn=<NegBackward0>)\n",
      "epoch 1410 loss tensor(6.9242, grad_fn=<NegBackward0>)\n",
      "epoch 1411 loss tensor(6.9240, grad_fn=<NegBackward0>)\n",
      "epoch 1412 loss tensor(6.9237, grad_fn=<NegBackward0>)\n",
      "epoch 1413 loss tensor(6.9235, grad_fn=<NegBackward0>)\n",
      "epoch 1414 loss tensor(6.9232, grad_fn=<NegBackward0>)\n",
      "epoch 1415 loss tensor(6.9229, grad_fn=<NegBackward0>)\n",
      "epoch 1416 loss tensor(6.9227, grad_fn=<NegBackward0>)\n",
      "epoch 1417 loss tensor(6.9224, grad_fn=<NegBackward0>)\n",
      "epoch 1418 loss tensor(6.9221, grad_fn=<NegBackward0>)\n",
      "epoch 1419 loss tensor(6.9219, grad_fn=<NegBackward0>)\n",
      "epoch 1420 loss tensor(6.9216, grad_fn=<NegBackward0>)\n",
      "epoch 1421 loss tensor(6.9214, grad_fn=<NegBackward0>)\n",
      "epoch 1422 loss tensor(6.9211, grad_fn=<NegBackward0>)\n",
      "epoch 1423 loss tensor(6.9208, grad_fn=<NegBackward0>)\n",
      "epoch 1424 loss tensor(6.9206, grad_fn=<NegBackward0>)\n",
      "epoch 1425 loss tensor(6.9203, grad_fn=<NegBackward0>)\n",
      "epoch 1426 loss tensor(6.9201, grad_fn=<NegBackward0>)\n",
      "epoch 1427 loss tensor(6.9198, grad_fn=<NegBackward0>)\n",
      "epoch 1428 loss tensor(6.9195, grad_fn=<NegBackward0>)\n",
      "epoch 1429 loss tensor(6.9193, grad_fn=<NegBackward0>)\n",
      "epoch 1430 loss tensor(6.9190, grad_fn=<NegBackward0>)\n",
      "epoch 1431 loss tensor(6.9188, grad_fn=<NegBackward0>)\n",
      "epoch 1432 loss tensor(6.9185, grad_fn=<NegBackward0>)\n",
      "epoch 1433 loss tensor(6.9182, grad_fn=<NegBackward0>)\n",
      "epoch 1434 loss tensor(6.9180, grad_fn=<NegBackward0>)\n",
      "epoch 1435 loss tensor(6.9177, grad_fn=<NegBackward0>)\n",
      "epoch 1436 loss tensor(6.9175, grad_fn=<NegBackward0>)\n",
      "epoch 1437 loss tensor(6.9172, grad_fn=<NegBackward0>)\n",
      "epoch 1438 loss tensor(6.9169, grad_fn=<NegBackward0>)\n",
      "epoch 1439 loss tensor(6.9167, grad_fn=<NegBackward0>)\n",
      "epoch 1440 loss tensor(6.9164, grad_fn=<NegBackward0>)\n",
      "epoch 1441 loss tensor(6.9162, grad_fn=<NegBackward0>)\n",
      "epoch 1442 loss tensor(6.9159, grad_fn=<NegBackward0>)\n",
      "epoch 1443 loss tensor(6.9156, grad_fn=<NegBackward0>)\n",
      "epoch 1444 loss tensor(6.9154, grad_fn=<NegBackward0>)\n",
      "epoch 1445 loss tensor(6.9151, grad_fn=<NegBackward0>)\n",
      "epoch 1446 loss tensor(6.9149, grad_fn=<NegBackward0>)\n",
      "epoch 1447 loss tensor(6.9146, grad_fn=<NegBackward0>)\n",
      "epoch 1448 loss tensor(6.9143, grad_fn=<NegBackward0>)\n",
      "epoch 1449 loss tensor(6.9141, grad_fn=<NegBackward0>)\n",
      "epoch 1450 loss tensor(6.9138, grad_fn=<NegBackward0>)\n",
      "epoch 1451 loss tensor(6.9136, grad_fn=<NegBackward0>)\n",
      "epoch 1452 loss tensor(6.9133, grad_fn=<NegBackward0>)\n",
      "epoch 1453 loss tensor(6.9131, grad_fn=<NegBackward0>)\n",
      "epoch 1454 loss tensor(6.9128, grad_fn=<NegBackward0>)\n",
      "epoch 1455 loss tensor(6.9125, grad_fn=<NegBackward0>)\n",
      "epoch 1456 loss tensor(6.9123, grad_fn=<NegBackward0>)\n",
      "epoch 1457 loss tensor(6.9120, grad_fn=<NegBackward0>)\n",
      "epoch 1458 loss tensor(6.9118, grad_fn=<NegBackward0>)\n",
      "epoch 1459 loss tensor(6.9115, grad_fn=<NegBackward0>)\n",
      "epoch 1460 loss tensor(6.9112, grad_fn=<NegBackward0>)\n",
      "epoch 1461 loss tensor(6.9110, grad_fn=<NegBackward0>)\n",
      "epoch 1462 loss tensor(6.9107, grad_fn=<NegBackward0>)\n",
      "epoch 1463 loss tensor(6.9105, grad_fn=<NegBackward0>)\n",
      "epoch 1464 loss tensor(6.9102, grad_fn=<NegBackward0>)\n",
      "epoch 1465 loss tensor(6.9100, grad_fn=<NegBackward0>)\n",
      "epoch 1466 loss tensor(6.9097, grad_fn=<NegBackward0>)\n",
      "epoch 1467 loss tensor(6.9094, grad_fn=<NegBackward0>)\n",
      "epoch 1468 loss tensor(6.9092, grad_fn=<NegBackward0>)\n",
      "epoch 1469 loss tensor(6.9089, grad_fn=<NegBackward0>)\n",
      "epoch 1470 loss tensor(6.9087, grad_fn=<NegBackward0>)\n",
      "epoch 1471 loss tensor(6.9084, grad_fn=<NegBackward0>)\n",
      "epoch 1472 loss tensor(6.9082, grad_fn=<NegBackward0>)\n",
      "epoch 1473 loss tensor(6.9079, grad_fn=<NegBackward0>)\n",
      "epoch 1474 loss tensor(6.9076, grad_fn=<NegBackward0>)\n",
      "epoch 1475 loss tensor(6.9074, grad_fn=<NegBackward0>)\n",
      "epoch 1476 loss tensor(6.9071, grad_fn=<NegBackward0>)\n",
      "epoch 1477 loss tensor(6.9069, grad_fn=<NegBackward0>)\n",
      "epoch 1478 loss tensor(6.9066, grad_fn=<NegBackward0>)\n",
      "epoch 1479 loss tensor(6.9064, grad_fn=<NegBackward0>)\n",
      "epoch 1480 loss tensor(6.9061, grad_fn=<NegBackward0>)\n",
      "epoch 1481 loss tensor(6.9059, grad_fn=<NegBackward0>)\n",
      "epoch 1482 loss tensor(6.9056, grad_fn=<NegBackward0>)\n",
      "epoch 1483 loss tensor(6.9053, grad_fn=<NegBackward0>)\n",
      "epoch 1484 loss tensor(6.9051, grad_fn=<NegBackward0>)\n",
      "epoch 1485 loss tensor(6.9048, grad_fn=<NegBackward0>)\n",
      "epoch 1486 loss tensor(6.9046, grad_fn=<NegBackward0>)\n",
      "epoch 1487 loss tensor(6.9043, grad_fn=<NegBackward0>)\n",
      "epoch 1488 loss tensor(6.9041, grad_fn=<NegBackward0>)\n",
      "epoch 1489 loss tensor(6.9038, grad_fn=<NegBackward0>)\n",
      "epoch 1490 loss tensor(6.9036, grad_fn=<NegBackward0>)\n",
      "epoch 1491 loss tensor(6.9033, grad_fn=<NegBackward0>)\n",
      "epoch 1492 loss tensor(6.9030, grad_fn=<NegBackward0>)\n",
      "epoch 1493 loss tensor(6.9028, grad_fn=<NegBackward0>)\n",
      "epoch 1494 loss tensor(6.9025, grad_fn=<NegBackward0>)\n",
      "epoch 1495 loss tensor(6.9023, grad_fn=<NegBackward0>)\n",
      "epoch 1496 loss tensor(6.9020, grad_fn=<NegBackward0>)\n",
      "epoch 1497 loss tensor(6.9018, grad_fn=<NegBackward0>)\n",
      "epoch 1498 loss tensor(6.9015, grad_fn=<NegBackward0>)\n",
      "epoch 1499 loss tensor(6.9013, grad_fn=<NegBackward0>)\n",
      "epoch 1500 loss tensor(6.9010, grad_fn=<NegBackward0>)\n",
      "epoch 1501 loss tensor(6.9007, grad_fn=<NegBackward0>)\n",
      "epoch 1502 loss tensor(6.9005, grad_fn=<NegBackward0>)\n",
      "epoch 1503 loss tensor(6.9002, grad_fn=<NegBackward0>)\n",
      "epoch 1504 loss tensor(6.9000, grad_fn=<NegBackward0>)\n",
      "epoch 1505 loss tensor(6.8997, grad_fn=<NegBackward0>)\n",
      "epoch 1506 loss tensor(6.8995, grad_fn=<NegBackward0>)\n",
      "epoch 1507 loss tensor(6.8992, grad_fn=<NegBackward0>)\n",
      "epoch 1508 loss tensor(6.8990, grad_fn=<NegBackward0>)\n",
      "epoch 1509 loss tensor(6.8987, grad_fn=<NegBackward0>)\n",
      "epoch 1510 loss tensor(6.8985, grad_fn=<NegBackward0>)\n",
      "epoch 1511 loss tensor(6.8982, grad_fn=<NegBackward0>)\n",
      "epoch 1512 loss tensor(6.8980, grad_fn=<NegBackward0>)\n",
      "epoch 1513 loss tensor(6.8977, grad_fn=<NegBackward0>)\n",
      "epoch 1514 loss tensor(6.8974, grad_fn=<NegBackward0>)\n",
      "epoch 1515 loss tensor(6.8972, grad_fn=<NegBackward0>)\n",
      "epoch 1516 loss tensor(6.8969, grad_fn=<NegBackward0>)\n",
      "epoch 1517 loss tensor(6.8967, grad_fn=<NegBackward0>)\n",
      "epoch 1518 loss tensor(6.8964, grad_fn=<NegBackward0>)\n",
      "epoch 1519 loss tensor(6.8962, grad_fn=<NegBackward0>)\n",
      "epoch 1520 loss tensor(6.8959, grad_fn=<NegBackward0>)\n",
      "epoch 1521 loss tensor(6.8957, grad_fn=<NegBackward0>)\n",
      "epoch 1522 loss tensor(6.8954, grad_fn=<NegBackward0>)\n",
      "epoch 1523 loss tensor(6.8952, grad_fn=<NegBackward0>)\n",
      "epoch 1524 loss tensor(6.8949, grad_fn=<NegBackward0>)\n",
      "epoch 1525 loss tensor(6.8947, grad_fn=<NegBackward0>)\n",
      "epoch 1526 loss tensor(6.8944, grad_fn=<NegBackward0>)\n",
      "epoch 1527 loss tensor(6.8942, grad_fn=<NegBackward0>)\n",
      "epoch 1528 loss tensor(6.8939, grad_fn=<NegBackward0>)\n",
      "epoch 1529 loss tensor(6.8936, grad_fn=<NegBackward0>)\n",
      "epoch 1530 loss tensor(6.8934, grad_fn=<NegBackward0>)\n",
      "epoch 1531 loss tensor(6.8931, grad_fn=<NegBackward0>)\n",
      "epoch 1532 loss tensor(6.8929, grad_fn=<NegBackward0>)\n",
      "epoch 1533 loss tensor(6.8926, grad_fn=<NegBackward0>)\n",
      "epoch 1534 loss tensor(6.8924, grad_fn=<NegBackward0>)\n",
      "epoch 1535 loss tensor(6.8921, grad_fn=<NegBackward0>)\n",
      "epoch 1536 loss tensor(6.8919, grad_fn=<NegBackward0>)\n",
      "epoch 1537 loss tensor(6.8916, grad_fn=<NegBackward0>)\n",
      "epoch 1538 loss tensor(6.8914, grad_fn=<NegBackward0>)\n",
      "epoch 1539 loss tensor(6.8911, grad_fn=<NegBackward0>)\n",
      "epoch 1540 loss tensor(6.8909, grad_fn=<NegBackward0>)\n",
      "epoch 1541 loss tensor(6.8906, grad_fn=<NegBackward0>)\n",
      "epoch 1542 loss tensor(6.8904, grad_fn=<NegBackward0>)\n",
      "epoch 1543 loss tensor(6.8901, grad_fn=<NegBackward0>)\n",
      "epoch 1544 loss tensor(6.8899, grad_fn=<NegBackward0>)\n",
      "epoch 1545 loss tensor(6.8896, grad_fn=<NegBackward0>)\n",
      "epoch 1546 loss tensor(6.8894, grad_fn=<NegBackward0>)\n",
      "epoch 1547 loss tensor(6.8891, grad_fn=<NegBackward0>)\n",
      "epoch 1548 loss tensor(6.8889, grad_fn=<NegBackward0>)\n",
      "epoch 1549 loss tensor(6.8886, grad_fn=<NegBackward0>)\n",
      "epoch 1550 loss tensor(6.8884, grad_fn=<NegBackward0>)\n",
      "epoch 1551 loss tensor(6.8881, grad_fn=<NegBackward0>)\n",
      "epoch 1552 loss tensor(6.8879, grad_fn=<NegBackward0>)\n",
      "epoch 1553 loss tensor(6.8876, grad_fn=<NegBackward0>)\n",
      "epoch 1554 loss tensor(6.8874, grad_fn=<NegBackward0>)\n",
      "epoch 1555 loss tensor(6.8871, grad_fn=<NegBackward0>)\n",
      "epoch 1556 loss tensor(6.8869, grad_fn=<NegBackward0>)\n",
      "epoch 1557 loss tensor(6.8866, grad_fn=<NegBackward0>)\n",
      "epoch 1558 loss tensor(6.8864, grad_fn=<NegBackward0>)\n",
      "epoch 1559 loss tensor(6.8861, grad_fn=<NegBackward0>)\n",
      "epoch 1560 loss tensor(6.8859, grad_fn=<NegBackward0>)\n",
      "epoch 1561 loss tensor(6.8856, grad_fn=<NegBackward0>)\n",
      "epoch 1562 loss tensor(6.8854, grad_fn=<NegBackward0>)\n",
      "epoch 1563 loss tensor(6.8851, grad_fn=<NegBackward0>)\n",
      "epoch 1564 loss tensor(6.8849, grad_fn=<NegBackward0>)\n",
      "epoch 1565 loss tensor(6.8846, grad_fn=<NegBackward0>)\n",
      "epoch 1566 loss tensor(6.8844, grad_fn=<NegBackward0>)\n",
      "epoch 1567 loss tensor(6.8841, grad_fn=<NegBackward0>)\n",
      "epoch 1568 loss tensor(6.8839, grad_fn=<NegBackward0>)\n",
      "epoch 1569 loss tensor(6.8836, grad_fn=<NegBackward0>)\n",
      "epoch 1570 loss tensor(6.8834, grad_fn=<NegBackward0>)\n",
      "epoch 1571 loss tensor(6.8831, grad_fn=<NegBackward0>)\n",
      "epoch 1572 loss tensor(6.8829, grad_fn=<NegBackward0>)\n",
      "epoch 1573 loss tensor(6.8826, grad_fn=<NegBackward0>)\n",
      "epoch 1574 loss tensor(6.8824, grad_fn=<NegBackward0>)\n",
      "epoch 1575 loss tensor(6.8821, grad_fn=<NegBackward0>)\n",
      "epoch 1576 loss tensor(6.8819, grad_fn=<NegBackward0>)\n",
      "epoch 1577 loss tensor(6.8816, grad_fn=<NegBackward0>)\n",
      "epoch 1578 loss tensor(6.8814, grad_fn=<NegBackward0>)\n",
      "epoch 1579 loss tensor(6.8811, grad_fn=<NegBackward0>)\n",
      "epoch 1580 loss tensor(6.8809, grad_fn=<NegBackward0>)\n",
      "epoch 1581 loss tensor(6.8806, grad_fn=<NegBackward0>)\n",
      "epoch 1582 loss tensor(6.8804, grad_fn=<NegBackward0>)\n",
      "epoch 1583 loss tensor(6.8801, grad_fn=<NegBackward0>)\n",
      "epoch 1584 loss tensor(6.8799, grad_fn=<NegBackward0>)\n",
      "epoch 1585 loss tensor(6.8796, grad_fn=<NegBackward0>)\n",
      "epoch 1586 loss tensor(6.8794, grad_fn=<NegBackward0>)\n",
      "epoch 1587 loss tensor(6.8791, grad_fn=<NegBackward0>)\n",
      "epoch 1588 loss tensor(6.8789, grad_fn=<NegBackward0>)\n",
      "epoch 1589 loss tensor(6.8786, grad_fn=<NegBackward0>)\n",
      "epoch 1590 loss tensor(6.8784, grad_fn=<NegBackward0>)\n",
      "epoch 1591 loss tensor(6.8781, grad_fn=<NegBackward0>)\n",
      "epoch 1592 loss tensor(6.8779, grad_fn=<NegBackward0>)\n",
      "epoch 1593 loss tensor(6.8776, grad_fn=<NegBackward0>)\n",
      "epoch 1594 loss tensor(6.8774, grad_fn=<NegBackward0>)\n",
      "epoch 1595 loss tensor(6.8771, grad_fn=<NegBackward0>)\n",
      "epoch 1596 loss tensor(6.8769, grad_fn=<NegBackward0>)\n",
      "epoch 1597 loss tensor(6.8767, grad_fn=<NegBackward0>)\n",
      "epoch 1598 loss tensor(6.8764, grad_fn=<NegBackward0>)\n",
      "epoch 1599 loss tensor(6.8762, grad_fn=<NegBackward0>)\n",
      "epoch 1600 loss tensor(6.8759, grad_fn=<NegBackward0>)\n",
      "epoch 1601 loss tensor(6.8757, grad_fn=<NegBackward0>)\n",
      "epoch 1602 loss tensor(6.8754, grad_fn=<NegBackward0>)\n",
      "epoch 1603 loss tensor(6.8752, grad_fn=<NegBackward0>)\n",
      "epoch 1604 loss tensor(6.8749, grad_fn=<NegBackward0>)\n",
      "epoch 1605 loss tensor(6.8747, grad_fn=<NegBackward0>)\n",
      "epoch 1606 loss tensor(6.8744, grad_fn=<NegBackward0>)\n",
      "epoch 1607 loss tensor(6.8742, grad_fn=<NegBackward0>)\n",
      "epoch 1608 loss tensor(6.8739, grad_fn=<NegBackward0>)\n",
      "epoch 1609 loss tensor(6.8737, grad_fn=<NegBackward0>)\n",
      "epoch 1610 loss tensor(6.8734, grad_fn=<NegBackward0>)\n",
      "epoch 1611 loss tensor(6.8732, grad_fn=<NegBackward0>)\n",
      "epoch 1612 loss tensor(6.8730, grad_fn=<NegBackward0>)\n",
      "epoch 1613 loss tensor(6.8727, grad_fn=<NegBackward0>)\n",
      "epoch 1614 loss tensor(6.8725, grad_fn=<NegBackward0>)\n",
      "epoch 1615 loss tensor(6.8722, grad_fn=<NegBackward0>)\n",
      "epoch 1616 loss tensor(6.8720, grad_fn=<NegBackward0>)\n",
      "epoch 1617 loss tensor(6.8717, grad_fn=<NegBackward0>)\n",
      "epoch 1618 loss tensor(6.8715, grad_fn=<NegBackward0>)\n",
      "epoch 1619 loss tensor(6.8712, grad_fn=<NegBackward0>)\n",
      "epoch 1620 loss tensor(6.8710, grad_fn=<NegBackward0>)\n",
      "epoch 1621 loss tensor(6.8707, grad_fn=<NegBackward0>)\n",
      "epoch 1622 loss tensor(6.8705, grad_fn=<NegBackward0>)\n",
      "epoch 1623 loss tensor(6.8702, grad_fn=<NegBackward0>)\n",
      "epoch 1624 loss tensor(6.8700, grad_fn=<NegBackward0>)\n",
      "epoch 1625 loss tensor(6.8698, grad_fn=<NegBackward0>)\n",
      "epoch 1626 loss tensor(6.8695, grad_fn=<NegBackward0>)\n",
      "epoch 1627 loss tensor(6.8693, grad_fn=<NegBackward0>)\n",
      "epoch 1628 loss tensor(6.8690, grad_fn=<NegBackward0>)\n",
      "epoch 1629 loss tensor(6.8688, grad_fn=<NegBackward0>)\n",
      "epoch 1630 loss tensor(6.8685, grad_fn=<NegBackward0>)\n",
      "epoch 1631 loss tensor(6.8683, grad_fn=<NegBackward0>)\n",
      "epoch 1632 loss tensor(6.8680, grad_fn=<NegBackward0>)\n",
      "epoch 1633 loss tensor(6.8678, grad_fn=<NegBackward0>)\n",
      "epoch 1634 loss tensor(6.8676, grad_fn=<NegBackward0>)\n",
      "epoch 1635 loss tensor(6.8673, grad_fn=<NegBackward0>)\n",
      "epoch 1636 loss tensor(6.8671, grad_fn=<NegBackward0>)\n",
      "epoch 1637 loss tensor(6.8668, grad_fn=<NegBackward0>)\n",
      "epoch 1638 loss tensor(6.8666, grad_fn=<NegBackward0>)\n",
      "epoch 1639 loss tensor(6.8663, grad_fn=<NegBackward0>)\n",
      "epoch 1640 loss tensor(6.8661, grad_fn=<NegBackward0>)\n",
      "epoch 1641 loss tensor(6.8658, grad_fn=<NegBackward0>)\n",
      "epoch 1642 loss tensor(6.8656, grad_fn=<NegBackward0>)\n",
      "epoch 1643 loss tensor(6.8654, grad_fn=<NegBackward0>)\n",
      "epoch 1644 loss tensor(6.8651, grad_fn=<NegBackward0>)\n",
      "epoch 1645 loss tensor(6.8649, grad_fn=<NegBackward0>)\n",
      "epoch 1646 loss tensor(6.8646, grad_fn=<NegBackward0>)\n",
      "epoch 1647 loss tensor(6.8644, grad_fn=<NegBackward0>)\n",
      "epoch 1648 loss tensor(6.8641, grad_fn=<NegBackward0>)\n",
      "epoch 1649 loss tensor(6.8639, grad_fn=<NegBackward0>)\n",
      "epoch 1650 loss tensor(6.8637, grad_fn=<NegBackward0>)\n",
      "epoch 1651 loss tensor(6.8634, grad_fn=<NegBackward0>)\n",
      "epoch 1652 loss tensor(6.8632, grad_fn=<NegBackward0>)\n",
      "epoch 1653 loss tensor(6.8629, grad_fn=<NegBackward0>)\n",
      "epoch 1654 loss tensor(6.8627, grad_fn=<NegBackward0>)\n",
      "epoch 1655 loss tensor(6.8624, grad_fn=<NegBackward0>)\n",
      "epoch 1656 loss tensor(6.8622, grad_fn=<NegBackward0>)\n",
      "epoch 1657 loss tensor(6.8620, grad_fn=<NegBackward0>)\n",
      "epoch 1658 loss tensor(6.8617, grad_fn=<NegBackward0>)\n",
      "epoch 1659 loss tensor(6.8615, grad_fn=<NegBackward0>)\n",
      "epoch 1660 loss tensor(6.8612, grad_fn=<NegBackward0>)\n",
      "epoch 1661 loss tensor(6.8610, grad_fn=<NegBackward0>)\n",
      "epoch 1662 loss tensor(6.8607, grad_fn=<NegBackward0>)\n",
      "epoch 1663 loss tensor(6.8605, grad_fn=<NegBackward0>)\n",
      "epoch 1664 loss tensor(6.8603, grad_fn=<NegBackward0>)\n",
      "epoch 1665 loss tensor(6.8600, grad_fn=<NegBackward0>)\n",
      "epoch 1666 loss tensor(6.8598, grad_fn=<NegBackward0>)\n",
      "epoch 1667 loss tensor(6.8595, grad_fn=<NegBackward0>)\n",
      "epoch 1668 loss tensor(6.8593, grad_fn=<NegBackward0>)\n",
      "epoch 1669 loss tensor(6.8590, grad_fn=<NegBackward0>)\n",
      "epoch 1670 loss tensor(6.8588, grad_fn=<NegBackward0>)\n",
      "epoch 1671 loss tensor(6.8586, grad_fn=<NegBackward0>)\n",
      "epoch 1672 loss tensor(6.8583, grad_fn=<NegBackward0>)\n",
      "epoch 1673 loss tensor(6.8581, grad_fn=<NegBackward0>)\n",
      "epoch 1674 loss tensor(6.8578, grad_fn=<NegBackward0>)\n",
      "epoch 1675 loss tensor(6.8576, grad_fn=<NegBackward0>)\n",
      "epoch 1676 loss tensor(6.8574, grad_fn=<NegBackward0>)\n",
      "epoch 1677 loss tensor(6.8571, grad_fn=<NegBackward0>)\n",
      "epoch 1678 loss tensor(6.8569, grad_fn=<NegBackward0>)\n",
      "epoch 1679 loss tensor(6.8566, grad_fn=<NegBackward0>)\n",
      "epoch 1680 loss tensor(6.8564, grad_fn=<NegBackward0>)\n",
      "epoch 1681 loss tensor(6.8561, grad_fn=<NegBackward0>)\n",
      "epoch 1682 loss tensor(6.8559, grad_fn=<NegBackward0>)\n",
      "epoch 1683 loss tensor(6.8557, grad_fn=<NegBackward0>)\n",
      "epoch 1684 loss tensor(6.8554, grad_fn=<NegBackward0>)\n",
      "epoch 1685 loss tensor(6.8552, grad_fn=<NegBackward0>)\n",
      "epoch 1686 loss tensor(6.8549, grad_fn=<NegBackward0>)\n",
      "epoch 1687 loss tensor(6.8547, grad_fn=<NegBackward0>)\n",
      "epoch 1688 loss tensor(6.8545, grad_fn=<NegBackward0>)\n",
      "epoch 1689 loss tensor(6.8542, grad_fn=<NegBackward0>)\n",
      "epoch 1690 loss tensor(6.8540, grad_fn=<NegBackward0>)\n",
      "epoch 1691 loss tensor(6.8537, grad_fn=<NegBackward0>)\n",
      "epoch 1692 loss tensor(6.8535, grad_fn=<NegBackward0>)\n",
      "epoch 1693 loss tensor(6.8533, grad_fn=<NegBackward0>)\n",
      "epoch 1694 loss tensor(6.8530, grad_fn=<NegBackward0>)\n",
      "epoch 1695 loss tensor(6.8528, grad_fn=<NegBackward0>)\n",
      "epoch 1696 loss tensor(6.8525, grad_fn=<NegBackward0>)\n",
      "epoch 1697 loss tensor(6.8523, grad_fn=<NegBackward0>)\n",
      "epoch 1698 loss tensor(6.8521, grad_fn=<NegBackward0>)\n",
      "epoch 1699 loss tensor(6.8518, grad_fn=<NegBackward0>)\n",
      "epoch 1700 loss tensor(6.8516, grad_fn=<NegBackward0>)\n",
      "epoch 1701 loss tensor(6.8513, grad_fn=<NegBackward0>)\n",
      "epoch 1702 loss tensor(6.8511, grad_fn=<NegBackward0>)\n",
      "epoch 1703 loss tensor(6.8509, grad_fn=<NegBackward0>)\n",
      "epoch 1704 loss tensor(6.8506, grad_fn=<NegBackward0>)\n",
      "epoch 1705 loss tensor(6.8504, grad_fn=<NegBackward0>)\n",
      "epoch 1706 loss tensor(6.8501, grad_fn=<NegBackward0>)\n",
      "epoch 1707 loss tensor(6.8499, grad_fn=<NegBackward0>)\n",
      "epoch 1708 loss tensor(6.8497, grad_fn=<NegBackward0>)\n",
      "epoch 1709 loss tensor(6.8494, grad_fn=<NegBackward0>)\n",
      "epoch 1710 loss tensor(6.8492, grad_fn=<NegBackward0>)\n",
      "epoch 1711 loss tensor(6.8490, grad_fn=<NegBackward0>)\n",
      "epoch 1712 loss tensor(6.8487, grad_fn=<NegBackward0>)\n",
      "epoch 1713 loss tensor(6.8485, grad_fn=<NegBackward0>)\n",
      "epoch 1714 loss tensor(6.8482, grad_fn=<NegBackward0>)\n",
      "epoch 1715 loss tensor(6.8480, grad_fn=<NegBackward0>)\n",
      "epoch 1716 loss tensor(6.8478, grad_fn=<NegBackward0>)\n",
      "epoch 1717 loss tensor(6.8475, grad_fn=<NegBackward0>)\n",
      "epoch 1718 loss tensor(6.8473, grad_fn=<NegBackward0>)\n",
      "epoch 1719 loss tensor(6.8470, grad_fn=<NegBackward0>)\n",
      "epoch 1720 loss tensor(6.8468, grad_fn=<NegBackward0>)\n",
      "epoch 1721 loss tensor(6.8466, grad_fn=<NegBackward0>)\n",
      "epoch 1722 loss tensor(6.8463, grad_fn=<NegBackward0>)\n",
      "epoch 1723 loss tensor(6.8461, grad_fn=<NegBackward0>)\n",
      "epoch 1724 loss tensor(6.8459, grad_fn=<NegBackward0>)\n",
      "epoch 1725 loss tensor(6.8456, grad_fn=<NegBackward0>)\n",
      "epoch 1726 loss tensor(6.8454, grad_fn=<NegBackward0>)\n",
      "epoch 1727 loss tensor(6.8451, grad_fn=<NegBackward0>)\n",
      "epoch 1728 loss tensor(6.8449, grad_fn=<NegBackward0>)\n",
      "epoch 1729 loss tensor(6.8447, grad_fn=<NegBackward0>)\n",
      "epoch 1730 loss tensor(6.8444, grad_fn=<NegBackward0>)\n",
      "epoch 1731 loss tensor(6.8442, grad_fn=<NegBackward0>)\n",
      "epoch 1732 loss tensor(6.8440, grad_fn=<NegBackward0>)\n",
      "epoch 1733 loss tensor(6.8437, grad_fn=<NegBackward0>)\n",
      "epoch 1734 loss tensor(6.8435, grad_fn=<NegBackward0>)\n",
      "epoch 1735 loss tensor(6.8432, grad_fn=<NegBackward0>)\n",
      "epoch 1736 loss tensor(6.8430, grad_fn=<NegBackward0>)\n",
      "epoch 1737 loss tensor(6.8428, grad_fn=<NegBackward0>)\n",
      "epoch 1738 loss tensor(6.8425, grad_fn=<NegBackward0>)\n",
      "epoch 1739 loss tensor(6.8423, grad_fn=<NegBackward0>)\n",
      "epoch 1740 loss tensor(6.8421, grad_fn=<NegBackward0>)\n",
      "epoch 1741 loss tensor(6.8418, grad_fn=<NegBackward0>)\n",
      "epoch 1742 loss tensor(6.8416, grad_fn=<NegBackward0>)\n",
      "epoch 1743 loss tensor(6.8413, grad_fn=<NegBackward0>)\n",
      "epoch 1744 loss tensor(6.8411, grad_fn=<NegBackward0>)\n",
      "epoch 1745 loss tensor(6.8409, grad_fn=<NegBackward0>)\n",
      "epoch 1746 loss tensor(6.8406, grad_fn=<NegBackward0>)\n",
      "epoch 1747 loss tensor(6.8404, grad_fn=<NegBackward0>)\n",
      "epoch 1748 loss tensor(6.8402, grad_fn=<NegBackward0>)\n",
      "epoch 1749 loss tensor(6.8399, grad_fn=<NegBackward0>)\n",
      "epoch 1750 loss tensor(6.8397, grad_fn=<NegBackward0>)\n",
      "epoch 1751 loss tensor(6.8395, grad_fn=<NegBackward0>)\n",
      "epoch 1752 loss tensor(6.8392, grad_fn=<NegBackward0>)\n",
      "epoch 1753 loss tensor(6.8390, grad_fn=<NegBackward0>)\n",
      "epoch 1754 loss tensor(6.8387, grad_fn=<NegBackward0>)\n",
      "epoch 1755 loss tensor(6.8385, grad_fn=<NegBackward0>)\n",
      "epoch 1756 loss tensor(6.8383, grad_fn=<NegBackward0>)\n",
      "epoch 1757 loss tensor(6.8380, grad_fn=<NegBackward0>)\n",
      "epoch 1758 loss tensor(6.8378, grad_fn=<NegBackward0>)\n",
      "epoch 1759 loss tensor(6.8376, grad_fn=<NegBackward0>)\n",
      "epoch 1760 loss tensor(6.8373, grad_fn=<NegBackward0>)\n",
      "epoch 1761 loss tensor(6.8371, grad_fn=<NegBackward0>)\n",
      "epoch 1762 loss tensor(6.8369, grad_fn=<NegBackward0>)\n",
      "epoch 1763 loss tensor(6.8366, grad_fn=<NegBackward0>)\n",
      "epoch 1764 loss tensor(6.8364, grad_fn=<NegBackward0>)\n",
      "epoch 1765 loss tensor(6.8362, grad_fn=<NegBackward0>)\n",
      "epoch 1766 loss tensor(6.8359, grad_fn=<NegBackward0>)\n",
      "epoch 1767 loss tensor(6.8357, grad_fn=<NegBackward0>)\n",
      "epoch 1768 loss tensor(6.8355, grad_fn=<NegBackward0>)\n",
      "epoch 1769 loss tensor(6.8352, grad_fn=<NegBackward0>)\n",
      "epoch 1770 loss tensor(6.8350, grad_fn=<NegBackward0>)\n",
      "epoch 1771 loss tensor(6.8347, grad_fn=<NegBackward0>)\n",
      "epoch 1772 loss tensor(6.8345, grad_fn=<NegBackward0>)\n",
      "epoch 1773 loss tensor(6.8343, grad_fn=<NegBackward0>)\n",
      "epoch 1774 loss tensor(6.8340, grad_fn=<NegBackward0>)\n",
      "epoch 1775 loss tensor(6.8338, grad_fn=<NegBackward0>)\n",
      "epoch 1776 loss tensor(6.8336, grad_fn=<NegBackward0>)\n",
      "epoch 1777 loss tensor(6.8333, grad_fn=<NegBackward0>)\n",
      "epoch 1778 loss tensor(6.8331, grad_fn=<NegBackward0>)\n",
      "epoch 1779 loss tensor(6.8329, grad_fn=<NegBackward0>)\n",
      "epoch 1780 loss tensor(6.8326, grad_fn=<NegBackward0>)\n",
      "epoch 1781 loss tensor(6.8324, grad_fn=<NegBackward0>)\n",
      "epoch 1782 loss tensor(6.8322, grad_fn=<NegBackward0>)\n",
      "epoch 1783 loss tensor(6.8319, grad_fn=<NegBackward0>)\n",
      "epoch 1784 loss tensor(6.8317, grad_fn=<NegBackward0>)\n",
      "epoch 1785 loss tensor(6.8315, grad_fn=<NegBackward0>)\n",
      "epoch 1786 loss tensor(6.8312, grad_fn=<NegBackward0>)\n",
      "epoch 1787 loss tensor(6.8310, grad_fn=<NegBackward0>)\n",
      "epoch 1788 loss tensor(6.8308, grad_fn=<NegBackward0>)\n",
      "epoch 1789 loss tensor(6.8305, grad_fn=<NegBackward0>)\n",
      "epoch 1790 loss tensor(6.8303, grad_fn=<NegBackward0>)\n",
      "epoch 1791 loss tensor(6.8301, grad_fn=<NegBackward0>)\n",
      "epoch 1792 loss tensor(6.8298, grad_fn=<NegBackward0>)\n",
      "epoch 1793 loss tensor(6.8296, grad_fn=<NegBackward0>)\n",
      "epoch 1794 loss tensor(6.8294, grad_fn=<NegBackward0>)\n",
      "epoch 1795 loss tensor(6.8291, grad_fn=<NegBackward0>)\n",
      "epoch 1796 loss tensor(6.8289, grad_fn=<NegBackward0>)\n",
      "epoch 1797 loss tensor(6.8287, grad_fn=<NegBackward0>)\n",
      "epoch 1798 loss tensor(6.8284, grad_fn=<NegBackward0>)\n",
      "epoch 1799 loss tensor(6.8282, grad_fn=<NegBackward0>)\n",
      "epoch 1800 loss tensor(6.8280, grad_fn=<NegBackward0>)\n",
      "epoch 1801 loss tensor(6.8277, grad_fn=<NegBackward0>)\n",
      "epoch 1802 loss tensor(6.8275, grad_fn=<NegBackward0>)\n",
      "epoch 1803 loss tensor(6.8273, grad_fn=<NegBackward0>)\n",
      "epoch 1804 loss tensor(6.8270, grad_fn=<NegBackward0>)\n",
      "epoch 1805 loss tensor(6.8268, grad_fn=<NegBackward0>)\n",
      "epoch 1806 loss tensor(6.8266, grad_fn=<NegBackward0>)\n",
      "epoch 1807 loss tensor(6.8263, grad_fn=<NegBackward0>)\n",
      "epoch 1808 loss tensor(6.8261, grad_fn=<NegBackward0>)\n",
      "epoch 1809 loss tensor(6.8259, grad_fn=<NegBackward0>)\n",
      "epoch 1810 loss tensor(6.8256, grad_fn=<NegBackward0>)\n",
      "epoch 1811 loss tensor(6.8254, grad_fn=<NegBackward0>)\n",
      "epoch 1812 loss tensor(6.8252, grad_fn=<NegBackward0>)\n",
      "epoch 1813 loss tensor(6.8250, grad_fn=<NegBackward0>)\n",
      "epoch 1814 loss tensor(6.8247, grad_fn=<NegBackward0>)\n",
      "epoch 1815 loss tensor(6.8245, grad_fn=<NegBackward0>)\n",
      "epoch 1816 loss tensor(6.8243, grad_fn=<NegBackward0>)\n",
      "epoch 1817 loss tensor(6.8240, grad_fn=<NegBackward0>)\n",
      "epoch 1818 loss tensor(6.8238, grad_fn=<NegBackward0>)\n",
      "epoch 1819 loss tensor(6.8236, grad_fn=<NegBackward0>)\n",
      "epoch 1820 loss tensor(6.8233, grad_fn=<NegBackward0>)\n",
      "epoch 1821 loss tensor(6.8231, grad_fn=<NegBackward0>)\n",
      "epoch 1822 loss tensor(6.8229, grad_fn=<NegBackward0>)\n",
      "epoch 1823 loss tensor(6.8226, grad_fn=<NegBackward0>)\n",
      "epoch 1824 loss tensor(6.8224, grad_fn=<NegBackward0>)\n",
      "epoch 1825 loss tensor(6.8222, grad_fn=<NegBackward0>)\n",
      "epoch 1826 loss tensor(6.8219, grad_fn=<NegBackward0>)\n",
      "epoch 1827 loss tensor(6.8217, grad_fn=<NegBackward0>)\n",
      "epoch 1828 loss tensor(6.8215, grad_fn=<NegBackward0>)\n",
      "epoch 1829 loss tensor(6.8213, grad_fn=<NegBackward0>)\n",
      "epoch 1830 loss tensor(6.8210, grad_fn=<NegBackward0>)\n",
      "epoch 1831 loss tensor(6.8208, grad_fn=<NegBackward0>)\n",
      "epoch 1832 loss tensor(6.8206, grad_fn=<NegBackward0>)\n",
      "epoch 1833 loss tensor(6.8203, grad_fn=<NegBackward0>)\n",
      "epoch 1834 loss tensor(6.8201, grad_fn=<NegBackward0>)\n",
      "epoch 1835 loss tensor(6.8199, grad_fn=<NegBackward0>)\n",
      "epoch 1836 loss tensor(6.8196, grad_fn=<NegBackward0>)\n",
      "epoch 1837 loss tensor(6.8194, grad_fn=<NegBackward0>)\n",
      "epoch 1838 loss tensor(6.8192, grad_fn=<NegBackward0>)\n",
      "epoch 1839 loss tensor(6.8190, grad_fn=<NegBackward0>)\n",
      "epoch 1840 loss tensor(6.8187, grad_fn=<NegBackward0>)\n",
      "epoch 1841 loss tensor(6.8185, grad_fn=<NegBackward0>)\n",
      "epoch 1842 loss tensor(6.8183, grad_fn=<NegBackward0>)\n",
      "epoch 1843 loss tensor(6.8180, grad_fn=<NegBackward0>)\n",
      "epoch 1844 loss tensor(6.8178, grad_fn=<NegBackward0>)\n",
      "epoch 1845 loss tensor(6.8176, grad_fn=<NegBackward0>)\n",
      "epoch 1846 loss tensor(6.8173, grad_fn=<NegBackward0>)\n",
      "epoch 1847 loss tensor(6.8171, grad_fn=<NegBackward0>)\n",
      "epoch 1848 loss tensor(6.8169, grad_fn=<NegBackward0>)\n",
      "epoch 1849 loss tensor(6.8167, grad_fn=<NegBackward0>)\n",
      "epoch 1850 loss tensor(6.8164, grad_fn=<NegBackward0>)\n",
      "epoch 1851 loss tensor(6.8162, grad_fn=<NegBackward0>)\n",
      "epoch 1852 loss tensor(6.8160, grad_fn=<NegBackward0>)\n",
      "epoch 1853 loss tensor(6.8157, grad_fn=<NegBackward0>)\n",
      "epoch 1854 loss tensor(6.8155, grad_fn=<NegBackward0>)\n",
      "epoch 1855 loss tensor(6.8153, grad_fn=<NegBackward0>)\n",
      "epoch 1856 loss tensor(6.8150, grad_fn=<NegBackward0>)\n",
      "epoch 1857 loss tensor(6.8148, grad_fn=<NegBackward0>)\n",
      "epoch 1858 loss tensor(6.8146, grad_fn=<NegBackward0>)\n",
      "epoch 1859 loss tensor(6.8144, grad_fn=<NegBackward0>)\n",
      "epoch 1860 loss tensor(6.8141, grad_fn=<NegBackward0>)\n",
      "epoch 1861 loss tensor(6.8139, grad_fn=<NegBackward0>)\n",
      "epoch 1862 loss tensor(6.8137, grad_fn=<NegBackward0>)\n",
      "epoch 1863 loss tensor(6.8134, grad_fn=<NegBackward0>)\n",
      "epoch 1864 loss tensor(6.8132, grad_fn=<NegBackward0>)\n",
      "epoch 1865 loss tensor(6.8130, grad_fn=<NegBackward0>)\n",
      "epoch 1866 loss tensor(6.8128, grad_fn=<NegBackward0>)\n",
      "epoch 1867 loss tensor(6.8125, grad_fn=<NegBackward0>)\n",
      "epoch 1868 loss tensor(6.8123, grad_fn=<NegBackward0>)\n",
      "epoch 1869 loss tensor(6.8121, grad_fn=<NegBackward0>)\n",
      "epoch 1870 loss tensor(6.8119, grad_fn=<NegBackward0>)\n",
      "epoch 1871 loss tensor(6.8116, grad_fn=<NegBackward0>)\n",
      "epoch 1872 loss tensor(6.8114, grad_fn=<NegBackward0>)\n",
      "epoch 1873 loss tensor(6.8112, grad_fn=<NegBackward0>)\n",
      "epoch 1874 loss tensor(6.8109, grad_fn=<NegBackward0>)\n",
      "epoch 1875 loss tensor(6.8107, grad_fn=<NegBackward0>)\n",
      "epoch 1876 loss tensor(6.8105, grad_fn=<NegBackward0>)\n",
      "epoch 1877 loss tensor(6.8103, grad_fn=<NegBackward0>)\n",
      "epoch 1878 loss tensor(6.8100, grad_fn=<NegBackward0>)\n",
      "epoch 1879 loss tensor(6.8098, grad_fn=<NegBackward0>)\n",
      "epoch 1880 loss tensor(6.8096, grad_fn=<NegBackward0>)\n",
      "epoch 1881 loss tensor(6.8093, grad_fn=<NegBackward0>)\n",
      "epoch 1882 loss tensor(6.8091, grad_fn=<NegBackward0>)\n",
      "epoch 1883 loss tensor(6.8089, grad_fn=<NegBackward0>)\n",
      "epoch 1884 loss tensor(6.8087, grad_fn=<NegBackward0>)\n",
      "epoch 1885 loss tensor(6.8084, grad_fn=<NegBackward0>)\n",
      "epoch 1886 loss tensor(6.8082, grad_fn=<NegBackward0>)\n",
      "epoch 1887 loss tensor(6.8080, grad_fn=<NegBackward0>)\n",
      "epoch 1888 loss tensor(6.8078, grad_fn=<NegBackward0>)\n",
      "epoch 1889 loss tensor(6.8075, grad_fn=<NegBackward0>)\n",
      "epoch 1890 loss tensor(6.8073, grad_fn=<NegBackward0>)\n",
      "epoch 1891 loss tensor(6.8071, grad_fn=<NegBackward0>)\n",
      "epoch 1892 loss tensor(6.8069, grad_fn=<NegBackward0>)\n",
      "epoch 1893 loss tensor(6.8066, grad_fn=<NegBackward0>)\n",
      "epoch 1894 loss tensor(6.8064, grad_fn=<NegBackward0>)\n",
      "epoch 1895 loss tensor(6.8062, grad_fn=<NegBackward0>)\n",
      "epoch 1896 loss tensor(6.8059, grad_fn=<NegBackward0>)\n",
      "epoch 1897 loss tensor(6.8057, grad_fn=<NegBackward0>)\n",
      "epoch 1898 loss tensor(6.8055, grad_fn=<NegBackward0>)\n",
      "epoch 1899 loss tensor(6.8053, grad_fn=<NegBackward0>)\n",
      "epoch 1900 loss tensor(6.8050, grad_fn=<NegBackward0>)\n",
      "epoch 1901 loss tensor(6.8048, grad_fn=<NegBackward0>)\n",
      "epoch 1902 loss tensor(6.8046, grad_fn=<NegBackward0>)\n",
      "epoch 1903 loss tensor(6.8044, grad_fn=<NegBackward0>)\n",
      "epoch 1904 loss tensor(6.8041, grad_fn=<NegBackward0>)\n",
      "epoch 1905 loss tensor(6.8039, grad_fn=<NegBackward0>)\n",
      "epoch 1906 loss tensor(6.8037, grad_fn=<NegBackward0>)\n",
      "epoch 1907 loss tensor(6.8035, grad_fn=<NegBackward0>)\n",
      "epoch 1908 loss tensor(6.8032, grad_fn=<NegBackward0>)\n",
      "epoch 1909 loss tensor(6.8030, grad_fn=<NegBackward0>)\n",
      "epoch 1910 loss tensor(6.8028, grad_fn=<NegBackward0>)\n",
      "epoch 1911 loss tensor(6.8026, grad_fn=<NegBackward0>)\n",
      "epoch 1912 loss tensor(6.8023, grad_fn=<NegBackward0>)\n",
      "epoch 1913 loss tensor(6.8021, grad_fn=<NegBackward0>)\n",
      "epoch 1914 loss tensor(6.8019, grad_fn=<NegBackward0>)\n",
      "epoch 1915 loss tensor(6.8017, grad_fn=<NegBackward0>)\n",
      "epoch 1916 loss tensor(6.8014, grad_fn=<NegBackward0>)\n",
      "epoch 1917 loss tensor(6.8012, grad_fn=<NegBackward0>)\n",
      "epoch 1918 loss tensor(6.8010, grad_fn=<NegBackward0>)\n",
      "epoch 1919 loss tensor(6.8008, grad_fn=<NegBackward0>)\n",
      "epoch 1920 loss tensor(6.8005, grad_fn=<NegBackward0>)\n",
      "epoch 1921 loss tensor(6.8003, grad_fn=<NegBackward0>)\n",
      "epoch 1922 loss tensor(6.8001, grad_fn=<NegBackward0>)\n",
      "epoch 1923 loss tensor(6.7999, grad_fn=<NegBackward0>)\n",
      "epoch 1924 loss tensor(6.7996, grad_fn=<NegBackward0>)\n",
      "epoch 1925 loss tensor(6.7994, grad_fn=<NegBackward0>)\n",
      "epoch 1926 loss tensor(6.7992, grad_fn=<NegBackward0>)\n",
      "epoch 1927 loss tensor(6.7990, grad_fn=<NegBackward0>)\n",
      "epoch 1928 loss tensor(6.7987, grad_fn=<NegBackward0>)\n",
      "epoch 1929 loss tensor(6.7985, grad_fn=<NegBackward0>)\n",
      "epoch 1930 loss tensor(6.7983, grad_fn=<NegBackward0>)\n",
      "epoch 1931 loss tensor(6.7981, grad_fn=<NegBackward0>)\n",
      "epoch 1932 loss tensor(6.7978, grad_fn=<NegBackward0>)\n",
      "epoch 1933 loss tensor(6.7976, grad_fn=<NegBackward0>)\n",
      "epoch 1934 loss tensor(6.7974, grad_fn=<NegBackward0>)\n",
      "epoch 1935 loss tensor(6.7972, grad_fn=<NegBackward0>)\n",
      "epoch 1936 loss tensor(6.7970, grad_fn=<NegBackward0>)\n",
      "epoch 1937 loss tensor(6.7967, grad_fn=<NegBackward0>)\n",
      "epoch 1938 loss tensor(6.7965, grad_fn=<NegBackward0>)\n",
      "epoch 1939 loss tensor(6.7963, grad_fn=<NegBackward0>)\n",
      "epoch 1940 loss tensor(6.7961, grad_fn=<NegBackward0>)\n",
      "epoch 1941 loss tensor(6.7958, grad_fn=<NegBackward0>)\n",
      "epoch 1942 loss tensor(6.7956, grad_fn=<NegBackward0>)\n",
      "epoch 1943 loss tensor(6.7954, grad_fn=<NegBackward0>)\n",
      "epoch 1944 loss tensor(6.7952, grad_fn=<NegBackward0>)\n",
      "epoch 1945 loss tensor(6.7949, grad_fn=<NegBackward0>)\n",
      "epoch 1946 loss tensor(6.7947, grad_fn=<NegBackward0>)\n",
      "epoch 1947 loss tensor(6.7945, grad_fn=<NegBackward0>)\n",
      "epoch 1948 loss tensor(6.7943, grad_fn=<NegBackward0>)\n",
      "epoch 1949 loss tensor(6.7941, grad_fn=<NegBackward0>)\n",
      "epoch 1950 loss tensor(6.7938, grad_fn=<NegBackward0>)\n",
      "epoch 1951 loss tensor(6.7936, grad_fn=<NegBackward0>)\n",
      "epoch 1952 loss tensor(6.7934, grad_fn=<NegBackward0>)\n",
      "epoch 1953 loss tensor(6.7932, grad_fn=<NegBackward0>)\n",
      "epoch 1954 loss tensor(6.7929, grad_fn=<NegBackward0>)\n",
      "epoch 1955 loss tensor(6.7927, grad_fn=<NegBackward0>)\n",
      "epoch 1956 loss tensor(6.7925, grad_fn=<NegBackward0>)\n",
      "epoch 1957 loss tensor(6.7923, grad_fn=<NegBackward0>)\n",
      "epoch 1958 loss tensor(6.7920, grad_fn=<NegBackward0>)\n",
      "epoch 1959 loss tensor(6.7918, grad_fn=<NegBackward0>)\n",
      "epoch 1960 loss tensor(6.7916, grad_fn=<NegBackward0>)\n",
      "epoch 1961 loss tensor(6.7914, grad_fn=<NegBackward0>)\n",
      "epoch 1962 loss tensor(6.7912, grad_fn=<NegBackward0>)\n",
      "epoch 1963 loss tensor(6.7909, grad_fn=<NegBackward0>)\n",
      "epoch 1964 loss tensor(6.7907, grad_fn=<NegBackward0>)\n",
      "epoch 1965 loss tensor(6.7905, grad_fn=<NegBackward0>)\n",
      "epoch 1966 loss tensor(6.7903, grad_fn=<NegBackward0>)\n",
      "epoch 1967 loss tensor(6.7901, grad_fn=<NegBackward0>)\n",
      "epoch 1968 loss tensor(6.7898, grad_fn=<NegBackward0>)\n",
      "epoch 1969 loss tensor(6.7896, grad_fn=<NegBackward0>)\n",
      "epoch 1970 loss tensor(6.7894, grad_fn=<NegBackward0>)\n",
      "epoch 1971 loss tensor(6.7892, grad_fn=<NegBackward0>)\n",
      "epoch 1972 loss tensor(6.7889, grad_fn=<NegBackward0>)\n",
      "epoch 1973 loss tensor(6.7887, grad_fn=<NegBackward0>)\n",
      "epoch 1974 loss tensor(6.7885, grad_fn=<NegBackward0>)\n",
      "epoch 1975 loss tensor(6.7883, grad_fn=<NegBackward0>)\n",
      "epoch 1976 loss tensor(6.7881, grad_fn=<NegBackward0>)\n",
      "epoch 1977 loss tensor(6.7878, grad_fn=<NegBackward0>)\n",
      "epoch 1978 loss tensor(6.7876, grad_fn=<NegBackward0>)\n",
      "epoch 1979 loss tensor(6.7874, grad_fn=<NegBackward0>)\n",
      "epoch 1980 loss tensor(6.7872, grad_fn=<NegBackward0>)\n",
      "epoch 1981 loss tensor(6.7870, grad_fn=<NegBackward0>)\n",
      "epoch 1982 loss tensor(6.7867, grad_fn=<NegBackward0>)\n",
      "epoch 1983 loss tensor(6.7865, grad_fn=<NegBackward0>)\n",
      "epoch 1984 loss tensor(6.7863, grad_fn=<NegBackward0>)\n",
      "epoch 1985 loss tensor(6.7861, grad_fn=<NegBackward0>)\n",
      "epoch 1986 loss tensor(6.7859, grad_fn=<NegBackward0>)\n",
      "epoch 1987 loss tensor(6.7856, grad_fn=<NegBackward0>)\n",
      "epoch 1988 loss tensor(6.7854, grad_fn=<NegBackward0>)\n",
      "epoch 1989 loss tensor(6.7852, grad_fn=<NegBackward0>)\n",
      "epoch 1990 loss tensor(6.7850, grad_fn=<NegBackward0>)\n",
      "epoch 1991 loss tensor(6.7848, grad_fn=<NegBackward0>)\n",
      "epoch 1992 loss tensor(6.7845, grad_fn=<NegBackward0>)\n",
      "epoch 1993 loss tensor(6.7843, grad_fn=<NegBackward0>)\n",
      "epoch 1994 loss tensor(6.7841, grad_fn=<NegBackward0>)\n",
      "epoch 1995 loss tensor(6.7839, grad_fn=<NegBackward0>)\n",
      "epoch 1996 loss tensor(6.7836, grad_fn=<NegBackward0>)\n",
      "epoch 1997 loss tensor(6.7834, grad_fn=<NegBackward0>)\n",
      "epoch 1998 loss tensor(6.7832, grad_fn=<NegBackward0>)\n",
      "epoch 1999 loss tensor(6.7830, grad_fn=<NegBackward0>)\n",
      "epoch 2000 loss tensor(6.7828, grad_fn=<NegBackward0>)\n",
      "epoch 2001 loss tensor(6.7826, grad_fn=<NegBackward0>)\n",
      "epoch 2002 loss tensor(6.7823, grad_fn=<NegBackward0>)\n",
      "epoch 2003 loss tensor(6.7821, grad_fn=<NegBackward0>)\n",
      "epoch 2004 loss tensor(6.7819, grad_fn=<NegBackward0>)\n",
      "epoch 2005 loss tensor(6.7817, grad_fn=<NegBackward0>)\n",
      "epoch 2006 loss tensor(6.7815, grad_fn=<NegBackward0>)\n",
      "epoch 2007 loss tensor(6.7812, grad_fn=<NegBackward0>)\n",
      "epoch 2008 loss tensor(6.7810, grad_fn=<NegBackward0>)\n",
      "epoch 2009 loss tensor(6.7808, grad_fn=<NegBackward0>)\n",
      "epoch 2010 loss tensor(6.7806, grad_fn=<NegBackward0>)\n",
      "epoch 2011 loss tensor(6.7804, grad_fn=<NegBackward0>)\n",
      "epoch 2012 loss tensor(6.7801, grad_fn=<NegBackward0>)\n",
      "epoch 2013 loss tensor(6.7799, grad_fn=<NegBackward0>)\n",
      "epoch 2014 loss tensor(6.7797, grad_fn=<NegBackward0>)\n",
      "epoch 2015 loss tensor(6.7795, grad_fn=<NegBackward0>)\n",
      "epoch 2016 loss tensor(6.7793, grad_fn=<NegBackward0>)\n",
      "epoch 2017 loss tensor(6.7790, grad_fn=<NegBackward0>)\n",
      "epoch 2018 loss tensor(6.7788, grad_fn=<NegBackward0>)\n",
      "epoch 2019 loss tensor(6.7786, grad_fn=<NegBackward0>)\n",
      "epoch 2020 loss tensor(6.7784, grad_fn=<NegBackward0>)\n",
      "epoch 2021 loss tensor(6.7782, grad_fn=<NegBackward0>)\n",
      "epoch 2022 loss tensor(6.7780, grad_fn=<NegBackward0>)\n",
      "epoch 2023 loss tensor(6.7777, grad_fn=<NegBackward0>)\n",
      "epoch 2024 loss tensor(6.7775, grad_fn=<NegBackward0>)\n",
      "epoch 2025 loss tensor(6.7773, grad_fn=<NegBackward0>)\n",
      "epoch 2026 loss tensor(6.7771, grad_fn=<NegBackward0>)\n",
      "epoch 2027 loss tensor(6.7769, grad_fn=<NegBackward0>)\n",
      "epoch 2028 loss tensor(6.7766, grad_fn=<NegBackward0>)\n",
      "epoch 2029 loss tensor(6.7764, grad_fn=<NegBackward0>)\n",
      "epoch 2030 loss tensor(6.7762, grad_fn=<NegBackward0>)\n",
      "epoch 2031 loss tensor(6.7760, grad_fn=<NegBackward0>)\n",
      "epoch 2032 loss tensor(6.7758, grad_fn=<NegBackward0>)\n",
      "epoch 2033 loss tensor(6.7756, grad_fn=<NegBackward0>)\n",
      "epoch 2034 loss tensor(6.7753, grad_fn=<NegBackward0>)\n",
      "epoch 2035 loss tensor(6.7751, grad_fn=<NegBackward0>)\n",
      "epoch 2036 loss tensor(6.7749, grad_fn=<NegBackward0>)\n",
      "epoch 2037 loss tensor(6.7747, grad_fn=<NegBackward0>)\n",
      "epoch 2038 loss tensor(6.7745, grad_fn=<NegBackward0>)\n",
      "epoch 2039 loss tensor(6.7743, grad_fn=<NegBackward0>)\n",
      "epoch 2040 loss tensor(6.7740, grad_fn=<NegBackward0>)\n",
      "epoch 2041 loss tensor(6.7738, grad_fn=<NegBackward0>)\n",
      "epoch 2042 loss tensor(6.7736, grad_fn=<NegBackward0>)\n",
      "epoch 2043 loss tensor(6.7734, grad_fn=<NegBackward0>)\n",
      "epoch 2044 loss tensor(6.7732, grad_fn=<NegBackward0>)\n",
      "epoch 2045 loss tensor(6.7730, grad_fn=<NegBackward0>)\n",
      "epoch 2046 loss tensor(6.7727, grad_fn=<NegBackward0>)\n",
      "epoch 2047 loss tensor(6.7725, grad_fn=<NegBackward0>)\n",
      "epoch 2048 loss tensor(6.7723, grad_fn=<NegBackward0>)\n",
      "epoch 2049 loss tensor(6.7721, grad_fn=<NegBackward0>)\n",
      "epoch 2050 loss tensor(6.7719, grad_fn=<NegBackward0>)\n",
      "epoch 2051 loss tensor(6.7717, grad_fn=<NegBackward0>)\n",
      "epoch 2052 loss tensor(6.7714, grad_fn=<NegBackward0>)\n",
      "epoch 2053 loss tensor(6.7712, grad_fn=<NegBackward0>)\n",
      "epoch 2054 loss tensor(6.7710, grad_fn=<NegBackward0>)\n",
      "epoch 2055 loss tensor(6.7708, grad_fn=<NegBackward0>)\n",
      "epoch 2056 loss tensor(6.7706, grad_fn=<NegBackward0>)\n",
      "epoch 2057 loss tensor(6.7704, grad_fn=<NegBackward0>)\n",
      "epoch 2058 loss tensor(6.7701, grad_fn=<NegBackward0>)\n",
      "epoch 2059 loss tensor(6.7699, grad_fn=<NegBackward0>)\n",
      "epoch 2060 loss tensor(6.7697, grad_fn=<NegBackward0>)\n",
      "epoch 2061 loss tensor(6.7695, grad_fn=<NegBackward0>)\n",
      "epoch 2062 loss tensor(6.7693, grad_fn=<NegBackward0>)\n",
      "epoch 2063 loss tensor(6.7691, grad_fn=<NegBackward0>)\n",
      "epoch 2064 loss tensor(6.7688, grad_fn=<NegBackward0>)\n",
      "epoch 2065 loss tensor(6.7686, grad_fn=<NegBackward0>)\n",
      "epoch 2066 loss tensor(6.7684, grad_fn=<NegBackward0>)\n",
      "epoch 2067 loss tensor(6.7682, grad_fn=<NegBackward0>)\n",
      "epoch 2068 loss tensor(6.7680, grad_fn=<NegBackward0>)\n",
      "epoch 2069 loss tensor(6.7678, grad_fn=<NegBackward0>)\n",
      "epoch 2070 loss tensor(6.7676, grad_fn=<NegBackward0>)\n",
      "epoch 2071 loss tensor(6.7673, grad_fn=<NegBackward0>)\n",
      "epoch 2072 loss tensor(6.7671, grad_fn=<NegBackward0>)\n",
      "epoch 2073 loss tensor(6.7669, grad_fn=<NegBackward0>)\n",
      "epoch 2074 loss tensor(6.7667, grad_fn=<NegBackward0>)\n",
      "epoch 2075 loss tensor(6.7665, grad_fn=<NegBackward0>)\n",
      "epoch 2076 loss tensor(6.7663, grad_fn=<NegBackward0>)\n",
      "epoch 2077 loss tensor(6.7661, grad_fn=<NegBackward0>)\n",
      "epoch 2078 loss tensor(6.7658, grad_fn=<NegBackward0>)\n",
      "epoch 2079 loss tensor(6.7656, grad_fn=<NegBackward0>)\n",
      "epoch 2080 loss tensor(6.7654, grad_fn=<NegBackward0>)\n",
      "epoch 2081 loss tensor(6.7652, grad_fn=<NegBackward0>)\n",
      "epoch 2082 loss tensor(6.7650, grad_fn=<NegBackward0>)\n",
      "epoch 2083 loss tensor(6.7648, grad_fn=<NegBackward0>)\n",
      "epoch 2084 loss tensor(6.7646, grad_fn=<NegBackward0>)\n",
      "epoch 2085 loss tensor(6.7643, grad_fn=<NegBackward0>)\n",
      "epoch 2086 loss tensor(6.7641, grad_fn=<NegBackward0>)\n",
      "epoch 2087 loss tensor(6.7639, grad_fn=<NegBackward0>)\n",
      "epoch 2088 loss tensor(6.7637, grad_fn=<NegBackward0>)\n",
      "epoch 2089 loss tensor(6.7635, grad_fn=<NegBackward0>)\n",
      "epoch 2090 loss tensor(6.7633, grad_fn=<NegBackward0>)\n",
      "epoch 2091 loss tensor(6.7631, grad_fn=<NegBackward0>)\n",
      "epoch 2092 loss tensor(6.7628, grad_fn=<NegBackward0>)\n",
      "epoch 2093 loss tensor(6.7626, grad_fn=<NegBackward0>)\n",
      "epoch 2094 loss tensor(6.7624, grad_fn=<NegBackward0>)\n",
      "epoch 2095 loss tensor(6.7622, grad_fn=<NegBackward0>)\n",
      "epoch 2096 loss tensor(6.7620, grad_fn=<NegBackward0>)\n",
      "epoch 2097 loss tensor(6.7618, grad_fn=<NegBackward0>)\n",
      "epoch 2098 loss tensor(6.7616, grad_fn=<NegBackward0>)\n",
      "epoch 2099 loss tensor(6.7613, grad_fn=<NegBackward0>)\n",
      "epoch 2100 loss tensor(6.7611, grad_fn=<NegBackward0>)\n",
      "epoch 2101 loss tensor(6.7609, grad_fn=<NegBackward0>)\n",
      "epoch 2102 loss tensor(6.7607, grad_fn=<NegBackward0>)\n",
      "epoch 2103 loss tensor(6.7605, grad_fn=<NegBackward0>)\n",
      "epoch 2104 loss tensor(6.7603, grad_fn=<NegBackward0>)\n",
      "epoch 2105 loss tensor(6.7601, grad_fn=<NegBackward0>)\n",
      "epoch 2106 loss tensor(6.7599, grad_fn=<NegBackward0>)\n",
      "epoch 2107 loss tensor(6.7596, grad_fn=<NegBackward0>)\n",
      "epoch 2108 loss tensor(6.7594, grad_fn=<NegBackward0>)\n",
      "epoch 2109 loss tensor(6.7592, grad_fn=<NegBackward0>)\n",
      "epoch 2110 loss tensor(6.7590, grad_fn=<NegBackward0>)\n",
      "epoch 2111 loss tensor(6.7588, grad_fn=<NegBackward0>)\n",
      "epoch 2112 loss tensor(6.7586, grad_fn=<NegBackward0>)\n",
      "epoch 2113 loss tensor(6.7584, grad_fn=<NegBackward0>)\n",
      "epoch 2114 loss tensor(6.7582, grad_fn=<NegBackward0>)\n",
      "epoch 2115 loss tensor(6.7579, grad_fn=<NegBackward0>)\n",
      "epoch 2116 loss tensor(6.7577, grad_fn=<NegBackward0>)\n",
      "epoch 2117 loss tensor(6.7575, grad_fn=<NegBackward0>)\n",
      "epoch 2118 loss tensor(6.7573, grad_fn=<NegBackward0>)\n",
      "epoch 2119 loss tensor(6.7571, grad_fn=<NegBackward0>)\n",
      "epoch 2120 loss tensor(6.7569, grad_fn=<NegBackward0>)\n",
      "epoch 2121 loss tensor(6.7567, grad_fn=<NegBackward0>)\n",
      "epoch 2122 loss tensor(6.7565, grad_fn=<NegBackward0>)\n",
      "epoch 2123 loss tensor(6.7562, grad_fn=<NegBackward0>)\n",
      "epoch 2124 loss tensor(6.7560, grad_fn=<NegBackward0>)\n",
      "epoch 2125 loss tensor(6.7558, grad_fn=<NegBackward0>)\n",
      "epoch 2126 loss tensor(6.7556, grad_fn=<NegBackward0>)\n",
      "epoch 2127 loss tensor(6.7554, grad_fn=<NegBackward0>)\n",
      "epoch 2128 loss tensor(6.7552, grad_fn=<NegBackward0>)\n",
      "epoch 2129 loss tensor(6.7550, grad_fn=<NegBackward0>)\n",
      "epoch 2130 loss tensor(6.7548, grad_fn=<NegBackward0>)\n",
      "epoch 2131 loss tensor(6.7545, grad_fn=<NegBackward0>)\n",
      "epoch 2132 loss tensor(6.7543, grad_fn=<NegBackward0>)\n",
      "epoch 2133 loss tensor(6.7541, grad_fn=<NegBackward0>)\n",
      "epoch 2134 loss tensor(6.7539, grad_fn=<NegBackward0>)\n",
      "epoch 2135 loss tensor(6.7537, grad_fn=<NegBackward0>)\n",
      "epoch 2136 loss tensor(6.7535, grad_fn=<NegBackward0>)\n",
      "epoch 2137 loss tensor(6.7533, grad_fn=<NegBackward0>)\n",
      "epoch 2138 loss tensor(6.7531, grad_fn=<NegBackward0>)\n",
      "epoch 2139 loss tensor(6.7529, grad_fn=<NegBackward0>)\n",
      "epoch 2140 loss tensor(6.7526, grad_fn=<NegBackward0>)\n",
      "epoch 2141 loss tensor(6.7524, grad_fn=<NegBackward0>)\n",
      "epoch 2142 loss tensor(6.7522, grad_fn=<NegBackward0>)\n",
      "epoch 2143 loss tensor(6.7520, grad_fn=<NegBackward0>)\n",
      "epoch 2144 loss tensor(6.7518, grad_fn=<NegBackward0>)\n",
      "epoch 2145 loss tensor(6.7516, grad_fn=<NegBackward0>)\n",
      "epoch 2146 loss tensor(6.7514, grad_fn=<NegBackward0>)\n",
      "epoch 2147 loss tensor(6.7512, grad_fn=<NegBackward0>)\n",
      "epoch 2148 loss tensor(6.7510, grad_fn=<NegBackward0>)\n",
      "epoch 2149 loss tensor(6.7508, grad_fn=<NegBackward0>)\n",
      "epoch 2150 loss tensor(6.7505, grad_fn=<NegBackward0>)\n",
      "epoch 2151 loss tensor(6.7503, grad_fn=<NegBackward0>)\n",
      "epoch 2152 loss tensor(6.7501, grad_fn=<NegBackward0>)\n",
      "epoch 2153 loss tensor(6.7499, grad_fn=<NegBackward0>)\n",
      "epoch 2154 loss tensor(6.7497, grad_fn=<NegBackward0>)\n",
      "epoch 2155 loss tensor(6.7495, grad_fn=<NegBackward0>)\n",
      "epoch 2156 loss tensor(6.7493, grad_fn=<NegBackward0>)\n",
      "epoch 2157 loss tensor(6.7491, grad_fn=<NegBackward0>)\n",
      "epoch 2158 loss tensor(6.7489, grad_fn=<NegBackward0>)\n",
      "epoch 2159 loss tensor(6.7487, grad_fn=<NegBackward0>)\n",
      "epoch 2160 loss tensor(6.7484, grad_fn=<NegBackward0>)\n",
      "epoch 2161 loss tensor(6.7482, grad_fn=<NegBackward0>)\n",
      "epoch 2162 loss tensor(6.7480, grad_fn=<NegBackward0>)\n",
      "epoch 2163 loss tensor(6.7478, grad_fn=<NegBackward0>)\n",
      "epoch 2164 loss tensor(6.7476, grad_fn=<NegBackward0>)\n",
      "epoch 2165 loss tensor(6.7474, grad_fn=<NegBackward0>)\n",
      "epoch 2166 loss tensor(6.7472, grad_fn=<NegBackward0>)\n",
      "epoch 2167 loss tensor(6.7470, grad_fn=<NegBackward0>)\n",
      "epoch 2168 loss tensor(6.7468, grad_fn=<NegBackward0>)\n",
      "epoch 2169 loss tensor(6.7466, grad_fn=<NegBackward0>)\n",
      "epoch 2170 loss tensor(6.7463, grad_fn=<NegBackward0>)\n",
      "epoch 2171 loss tensor(6.7461, grad_fn=<NegBackward0>)\n",
      "epoch 2172 loss tensor(6.7459, grad_fn=<NegBackward0>)\n",
      "epoch 2173 loss tensor(6.7457, grad_fn=<NegBackward0>)\n",
      "epoch 2174 loss tensor(6.7455, grad_fn=<NegBackward0>)\n",
      "epoch 2175 loss tensor(6.7453, grad_fn=<NegBackward0>)\n",
      "epoch 2176 loss tensor(6.7451, grad_fn=<NegBackward0>)\n",
      "epoch 2177 loss tensor(6.7449, grad_fn=<NegBackward0>)\n",
      "epoch 2178 loss tensor(6.7447, grad_fn=<NegBackward0>)\n",
      "epoch 2179 loss tensor(6.7445, grad_fn=<NegBackward0>)\n",
      "epoch 2180 loss tensor(6.7443, grad_fn=<NegBackward0>)\n",
      "epoch 2181 loss tensor(6.7440, grad_fn=<NegBackward0>)\n",
      "epoch 2182 loss tensor(6.7438, grad_fn=<NegBackward0>)\n",
      "epoch 2183 loss tensor(6.7436, grad_fn=<NegBackward0>)\n",
      "epoch 2184 loss tensor(6.7434, grad_fn=<NegBackward0>)\n",
      "epoch 2185 loss tensor(6.7432, grad_fn=<NegBackward0>)\n",
      "epoch 2186 loss tensor(6.7430, grad_fn=<NegBackward0>)\n",
      "epoch 2187 loss tensor(6.7428, grad_fn=<NegBackward0>)\n",
      "epoch 2188 loss tensor(6.7426, grad_fn=<NegBackward0>)\n",
      "epoch 2189 loss tensor(6.7424, grad_fn=<NegBackward0>)\n",
      "epoch 2190 loss tensor(6.7422, grad_fn=<NegBackward0>)\n",
      "epoch 2191 loss tensor(6.7420, grad_fn=<NegBackward0>)\n",
      "epoch 2192 loss tensor(6.7418, grad_fn=<NegBackward0>)\n",
      "epoch 2193 loss tensor(6.7415, grad_fn=<NegBackward0>)\n",
      "epoch 2194 loss tensor(6.7413, grad_fn=<NegBackward0>)\n",
      "epoch 2195 loss tensor(6.7411, grad_fn=<NegBackward0>)\n",
      "epoch 2196 loss tensor(6.7409, grad_fn=<NegBackward0>)\n",
      "epoch 2197 loss tensor(6.7407, grad_fn=<NegBackward0>)\n",
      "epoch 2198 loss tensor(6.7405, grad_fn=<NegBackward0>)\n",
      "epoch 2199 loss tensor(6.7403, grad_fn=<NegBackward0>)\n",
      "epoch 2200 loss tensor(6.7401, grad_fn=<NegBackward0>)\n",
      "epoch 2201 loss tensor(6.7399, grad_fn=<NegBackward0>)\n",
      "epoch 2202 loss tensor(6.7397, grad_fn=<NegBackward0>)\n",
      "epoch 2203 loss tensor(6.7395, grad_fn=<NegBackward0>)\n",
      "epoch 2204 loss tensor(6.7393, grad_fn=<NegBackward0>)\n",
      "epoch 2205 loss tensor(6.7391, grad_fn=<NegBackward0>)\n",
      "epoch 2206 loss tensor(6.7389, grad_fn=<NegBackward0>)\n",
      "epoch 2207 loss tensor(6.7386, grad_fn=<NegBackward0>)\n",
      "epoch 2208 loss tensor(6.7384, grad_fn=<NegBackward0>)\n",
      "epoch 2209 loss tensor(6.7382, grad_fn=<NegBackward0>)\n",
      "epoch 2210 loss tensor(6.7380, grad_fn=<NegBackward0>)\n",
      "epoch 2211 loss tensor(6.7378, grad_fn=<NegBackward0>)\n",
      "epoch 2212 loss tensor(6.7376, grad_fn=<NegBackward0>)\n",
      "epoch 2213 loss tensor(6.7374, grad_fn=<NegBackward0>)\n",
      "epoch 2214 loss tensor(6.7372, grad_fn=<NegBackward0>)\n",
      "epoch 2215 loss tensor(6.7370, grad_fn=<NegBackward0>)\n",
      "epoch 2216 loss tensor(6.7368, grad_fn=<NegBackward0>)\n",
      "epoch 2217 loss tensor(6.7366, grad_fn=<NegBackward0>)\n",
      "epoch 2218 loss tensor(6.7364, grad_fn=<NegBackward0>)\n",
      "epoch 2219 loss tensor(6.7362, grad_fn=<NegBackward0>)\n",
      "epoch 2220 loss tensor(6.7360, grad_fn=<NegBackward0>)\n",
      "epoch 2221 loss tensor(6.7358, grad_fn=<NegBackward0>)\n",
      "epoch 2222 loss tensor(6.7355, grad_fn=<NegBackward0>)\n",
      "epoch 2223 loss tensor(6.7353, grad_fn=<NegBackward0>)\n",
      "epoch 2224 loss tensor(6.7351, grad_fn=<NegBackward0>)\n",
      "epoch 2225 loss tensor(6.7349, grad_fn=<NegBackward0>)\n",
      "epoch 2226 loss tensor(6.7347, grad_fn=<NegBackward0>)\n",
      "epoch 2227 loss tensor(6.7345, grad_fn=<NegBackward0>)\n",
      "epoch 2228 loss tensor(6.7343, grad_fn=<NegBackward0>)\n",
      "epoch 2229 loss tensor(6.7341, grad_fn=<NegBackward0>)\n",
      "epoch 2230 loss tensor(6.7339, grad_fn=<NegBackward0>)\n",
      "epoch 2231 loss tensor(6.7337, grad_fn=<NegBackward0>)\n",
      "epoch 2232 loss tensor(6.7335, grad_fn=<NegBackward0>)\n",
      "epoch 2233 loss tensor(6.7333, grad_fn=<NegBackward0>)\n",
      "epoch 2234 loss tensor(6.7331, grad_fn=<NegBackward0>)\n",
      "epoch 2235 loss tensor(6.7329, grad_fn=<NegBackward0>)\n",
      "epoch 2236 loss tensor(6.7327, grad_fn=<NegBackward0>)\n",
      "epoch 2237 loss tensor(6.7325, grad_fn=<NegBackward0>)\n",
      "epoch 2238 loss tensor(6.7323, grad_fn=<NegBackward0>)\n",
      "epoch 2239 loss tensor(6.7321, grad_fn=<NegBackward0>)\n",
      "epoch 2240 loss tensor(6.7319, grad_fn=<NegBackward0>)\n",
      "epoch 2241 loss tensor(6.7316, grad_fn=<NegBackward0>)\n",
      "epoch 2242 loss tensor(6.7314, grad_fn=<NegBackward0>)\n",
      "epoch 2243 loss tensor(6.7312, grad_fn=<NegBackward0>)\n",
      "epoch 2244 loss tensor(6.7310, grad_fn=<NegBackward0>)\n",
      "epoch 2245 loss tensor(6.7308, grad_fn=<NegBackward0>)\n",
      "epoch 2246 loss tensor(6.7306, grad_fn=<NegBackward0>)\n",
      "epoch 2247 loss tensor(6.7304, grad_fn=<NegBackward0>)\n",
      "epoch 2248 loss tensor(6.7302, grad_fn=<NegBackward0>)\n",
      "epoch 2249 loss tensor(6.7300, grad_fn=<NegBackward0>)\n",
      "epoch 2250 loss tensor(6.7298, grad_fn=<NegBackward0>)\n",
      "epoch 2251 loss tensor(6.7296, grad_fn=<NegBackward0>)\n",
      "epoch 2252 loss tensor(6.7294, grad_fn=<NegBackward0>)\n",
      "epoch 2253 loss tensor(6.7292, grad_fn=<NegBackward0>)\n",
      "epoch 2254 loss tensor(6.7290, grad_fn=<NegBackward0>)\n",
      "epoch 2255 loss tensor(6.7288, grad_fn=<NegBackward0>)\n",
      "epoch 2256 loss tensor(6.7286, grad_fn=<NegBackward0>)\n",
      "epoch 2257 loss tensor(6.7284, grad_fn=<NegBackward0>)\n",
      "epoch 2258 loss tensor(6.7282, grad_fn=<NegBackward0>)\n",
      "epoch 2259 loss tensor(6.7280, grad_fn=<NegBackward0>)\n",
      "epoch 2260 loss tensor(6.7278, grad_fn=<NegBackward0>)\n",
      "epoch 2261 loss tensor(6.7276, grad_fn=<NegBackward0>)\n",
      "epoch 2262 loss tensor(6.7274, grad_fn=<NegBackward0>)\n",
      "epoch 2263 loss tensor(6.7271, grad_fn=<NegBackward0>)\n",
      "epoch 2264 loss tensor(6.7269, grad_fn=<NegBackward0>)\n",
      "epoch 2265 loss tensor(6.7267, grad_fn=<NegBackward0>)\n",
      "epoch 2266 loss tensor(6.7265, grad_fn=<NegBackward0>)\n",
      "epoch 2267 loss tensor(6.7263, grad_fn=<NegBackward0>)\n",
      "epoch 2268 loss tensor(6.7261, grad_fn=<NegBackward0>)\n",
      "epoch 2269 loss tensor(6.7259, grad_fn=<NegBackward0>)\n",
      "epoch 2270 loss tensor(6.7257, grad_fn=<NegBackward0>)\n",
      "epoch 2271 loss tensor(6.7255, grad_fn=<NegBackward0>)\n",
      "epoch 2272 loss tensor(6.7253, grad_fn=<NegBackward0>)\n",
      "epoch 2273 loss tensor(6.7251, grad_fn=<NegBackward0>)\n",
      "epoch 2274 loss tensor(6.7249, grad_fn=<NegBackward0>)\n",
      "epoch 2275 loss tensor(6.7247, grad_fn=<NegBackward0>)\n",
      "epoch 2276 loss tensor(6.7245, grad_fn=<NegBackward0>)\n",
      "epoch 2277 loss tensor(6.7243, grad_fn=<NegBackward0>)\n",
      "epoch 2278 loss tensor(6.7241, grad_fn=<NegBackward0>)\n",
      "epoch 2279 loss tensor(6.7239, grad_fn=<NegBackward0>)\n",
      "epoch 2280 loss tensor(6.7237, grad_fn=<NegBackward0>)\n",
      "epoch 2281 loss tensor(6.7235, grad_fn=<NegBackward0>)\n",
      "epoch 2282 loss tensor(6.7233, grad_fn=<NegBackward0>)\n",
      "epoch 2283 loss tensor(6.7231, grad_fn=<NegBackward0>)\n",
      "epoch 2284 loss tensor(6.7229, grad_fn=<NegBackward0>)\n",
      "epoch 2285 loss tensor(6.7227, grad_fn=<NegBackward0>)\n",
      "epoch 2286 loss tensor(6.7225, grad_fn=<NegBackward0>)\n",
      "epoch 2287 loss tensor(6.7223, grad_fn=<NegBackward0>)\n",
      "epoch 2288 loss tensor(6.7221, grad_fn=<NegBackward0>)\n",
      "epoch 2289 loss tensor(6.7219, grad_fn=<NegBackward0>)\n",
      "epoch 2290 loss tensor(6.7217, grad_fn=<NegBackward0>)\n",
      "epoch 2291 loss tensor(6.7215, grad_fn=<NegBackward0>)\n",
      "epoch 2292 loss tensor(6.7213, grad_fn=<NegBackward0>)\n",
      "epoch 2293 loss tensor(6.7211, grad_fn=<NegBackward0>)\n",
      "epoch 2294 loss tensor(6.7209, grad_fn=<NegBackward0>)\n",
      "epoch 2295 loss tensor(6.7207, grad_fn=<NegBackward0>)\n",
      "epoch 2296 loss tensor(6.7205, grad_fn=<NegBackward0>)\n",
      "epoch 2297 loss tensor(6.7203, grad_fn=<NegBackward0>)\n",
      "epoch 2298 loss tensor(6.7201, grad_fn=<NegBackward0>)\n",
      "epoch 2299 loss tensor(6.7199, grad_fn=<NegBackward0>)\n",
      "epoch 2300 loss tensor(6.7197, grad_fn=<NegBackward0>)\n",
      "epoch 2301 loss tensor(6.7194, grad_fn=<NegBackward0>)\n",
      "epoch 2302 loss tensor(6.7192, grad_fn=<NegBackward0>)\n",
      "epoch 2303 loss tensor(6.7190, grad_fn=<NegBackward0>)\n",
      "epoch 2304 loss tensor(6.7188, grad_fn=<NegBackward0>)\n",
      "epoch 2305 loss tensor(6.7186, grad_fn=<NegBackward0>)\n",
      "epoch 2306 loss tensor(6.7184, grad_fn=<NegBackward0>)\n",
      "epoch 2307 loss tensor(6.7182, grad_fn=<NegBackward0>)\n",
      "epoch 2308 loss tensor(6.7180, grad_fn=<NegBackward0>)\n",
      "epoch 2309 loss tensor(6.7178, grad_fn=<NegBackward0>)\n",
      "epoch 2310 loss tensor(6.7176, grad_fn=<NegBackward0>)\n",
      "epoch 2311 loss tensor(6.7174, grad_fn=<NegBackward0>)\n",
      "epoch 2312 loss tensor(6.7172, grad_fn=<NegBackward0>)\n",
      "epoch 2313 loss tensor(6.7170, grad_fn=<NegBackward0>)\n",
      "epoch 2314 loss tensor(6.7168, grad_fn=<NegBackward0>)\n",
      "epoch 2315 loss tensor(6.7166, grad_fn=<NegBackward0>)\n",
      "epoch 2316 loss tensor(6.7164, grad_fn=<NegBackward0>)\n",
      "epoch 2317 loss tensor(6.7162, grad_fn=<NegBackward0>)\n",
      "epoch 2318 loss tensor(6.7160, grad_fn=<NegBackward0>)\n",
      "epoch 2319 loss tensor(6.7158, grad_fn=<NegBackward0>)\n",
      "epoch 2320 loss tensor(6.7156, grad_fn=<NegBackward0>)\n",
      "epoch 2321 loss tensor(6.7154, grad_fn=<NegBackward0>)\n",
      "epoch 2322 loss tensor(6.7152, grad_fn=<NegBackward0>)\n",
      "epoch 2323 loss tensor(6.7150, grad_fn=<NegBackward0>)\n",
      "epoch 2324 loss tensor(6.7148, grad_fn=<NegBackward0>)\n",
      "epoch 2325 loss tensor(6.7146, grad_fn=<NegBackward0>)\n",
      "epoch 2326 loss tensor(6.7144, grad_fn=<NegBackward0>)\n",
      "epoch 2327 loss tensor(6.7142, grad_fn=<NegBackward0>)\n",
      "epoch 2328 loss tensor(6.7140, grad_fn=<NegBackward0>)\n",
      "epoch 2329 loss tensor(6.7138, grad_fn=<NegBackward0>)\n",
      "epoch 2330 loss tensor(6.7136, grad_fn=<NegBackward0>)\n",
      "epoch 2331 loss tensor(6.7134, grad_fn=<NegBackward0>)\n",
      "epoch 2332 loss tensor(6.7132, grad_fn=<NegBackward0>)\n",
      "epoch 2333 loss tensor(6.7130, grad_fn=<NegBackward0>)\n",
      "epoch 2334 loss tensor(6.7128, grad_fn=<NegBackward0>)\n",
      "epoch 2335 loss tensor(6.7126, grad_fn=<NegBackward0>)\n",
      "epoch 2336 loss tensor(6.7124, grad_fn=<NegBackward0>)\n",
      "epoch 2337 loss tensor(6.7122, grad_fn=<NegBackward0>)\n",
      "epoch 2338 loss tensor(6.7120, grad_fn=<NegBackward0>)\n",
      "epoch 2339 loss tensor(6.7118, grad_fn=<NegBackward0>)\n",
      "epoch 2340 loss tensor(6.7116, grad_fn=<NegBackward0>)\n",
      "epoch 2341 loss tensor(6.7114, grad_fn=<NegBackward0>)\n",
      "epoch 2342 loss tensor(6.7112, grad_fn=<NegBackward0>)\n",
      "epoch 2343 loss tensor(6.7110, grad_fn=<NegBackward0>)\n",
      "epoch 2344 loss tensor(6.7108, grad_fn=<NegBackward0>)\n",
      "epoch 2345 loss tensor(6.7106, grad_fn=<NegBackward0>)\n",
      "epoch 2346 loss tensor(6.7104, grad_fn=<NegBackward0>)\n",
      "epoch 2347 loss tensor(6.7102, grad_fn=<NegBackward0>)\n",
      "epoch 2348 loss tensor(6.7100, grad_fn=<NegBackward0>)\n",
      "epoch 2349 loss tensor(6.7098, grad_fn=<NegBackward0>)\n",
      "epoch 2350 loss tensor(6.7096, grad_fn=<NegBackward0>)\n",
      "epoch 2351 loss tensor(6.7094, grad_fn=<NegBackward0>)\n",
      "epoch 2352 loss tensor(6.7092, grad_fn=<NegBackward0>)\n",
      "epoch 2353 loss tensor(6.7090, grad_fn=<NegBackward0>)\n",
      "epoch 2354 loss tensor(6.7088, grad_fn=<NegBackward0>)\n",
      "epoch 2355 loss tensor(6.7086, grad_fn=<NegBackward0>)\n",
      "epoch 2356 loss tensor(6.7085, grad_fn=<NegBackward0>)\n",
      "epoch 2357 loss tensor(6.7083, grad_fn=<NegBackward0>)\n",
      "epoch 2358 loss tensor(6.7081, grad_fn=<NegBackward0>)\n",
      "epoch 2359 loss tensor(6.7079, grad_fn=<NegBackward0>)\n",
      "epoch 2360 loss tensor(6.7077, grad_fn=<NegBackward0>)\n",
      "epoch 2361 loss tensor(6.7075, grad_fn=<NegBackward0>)\n",
      "epoch 2362 loss tensor(6.7073, grad_fn=<NegBackward0>)\n",
      "epoch 2363 loss tensor(6.7071, grad_fn=<NegBackward0>)\n",
      "epoch 2364 loss tensor(6.7069, grad_fn=<NegBackward0>)\n",
      "epoch 2365 loss tensor(6.7067, grad_fn=<NegBackward0>)\n",
      "epoch 2366 loss tensor(6.7065, grad_fn=<NegBackward0>)\n",
      "epoch 2367 loss tensor(6.7063, grad_fn=<NegBackward0>)\n",
      "epoch 2368 loss tensor(6.7061, grad_fn=<NegBackward0>)\n",
      "epoch 2369 loss tensor(6.7059, grad_fn=<NegBackward0>)\n",
      "epoch 2370 loss tensor(6.7057, grad_fn=<NegBackward0>)\n",
      "epoch 2371 loss tensor(6.7055, grad_fn=<NegBackward0>)\n",
      "epoch 2372 loss tensor(6.7053, grad_fn=<NegBackward0>)\n",
      "epoch 2373 loss tensor(6.7051, grad_fn=<NegBackward0>)\n",
      "epoch 2374 loss tensor(6.7049, grad_fn=<NegBackward0>)\n",
      "epoch 2375 loss tensor(6.7047, grad_fn=<NegBackward0>)\n",
      "epoch 2376 loss tensor(6.7045, grad_fn=<NegBackward0>)\n",
      "epoch 2377 loss tensor(6.7043, grad_fn=<NegBackward0>)\n",
      "epoch 2378 loss tensor(6.7041, grad_fn=<NegBackward0>)\n",
      "epoch 2379 loss tensor(6.7039, grad_fn=<NegBackward0>)\n",
      "epoch 2380 loss tensor(6.7037, grad_fn=<NegBackward0>)\n",
      "epoch 2381 loss tensor(6.7035, grad_fn=<NegBackward0>)\n",
      "epoch 2382 loss tensor(6.7033, grad_fn=<NegBackward0>)\n",
      "epoch 2383 loss tensor(6.7031, grad_fn=<NegBackward0>)\n",
      "epoch 2384 loss tensor(6.7029, grad_fn=<NegBackward0>)\n",
      "epoch 2385 loss tensor(6.7027, grad_fn=<NegBackward0>)\n",
      "epoch 2386 loss tensor(6.7025, grad_fn=<NegBackward0>)\n",
      "epoch 2387 loss tensor(6.7023, grad_fn=<NegBackward0>)\n",
      "epoch 2388 loss tensor(6.7021, grad_fn=<NegBackward0>)\n",
      "epoch 2389 loss tensor(6.7019, grad_fn=<NegBackward0>)\n",
      "epoch 2390 loss tensor(6.7017, grad_fn=<NegBackward0>)\n",
      "epoch 2391 loss tensor(6.7015, grad_fn=<NegBackward0>)\n",
      "epoch 2392 loss tensor(6.7013, grad_fn=<NegBackward0>)\n",
      "epoch 2393 loss tensor(6.7011, grad_fn=<NegBackward0>)\n",
      "epoch 2394 loss tensor(6.7010, grad_fn=<NegBackward0>)\n",
      "epoch 2395 loss tensor(6.7008, grad_fn=<NegBackward0>)\n",
      "epoch 2396 loss tensor(6.7006, grad_fn=<NegBackward0>)\n",
      "epoch 2397 loss tensor(6.7004, grad_fn=<NegBackward0>)\n",
      "epoch 2398 loss tensor(6.7002, grad_fn=<NegBackward0>)\n",
      "epoch 2399 loss tensor(6.7000, grad_fn=<NegBackward0>)\n",
      "epoch 2400 loss tensor(6.6998, grad_fn=<NegBackward0>)\n",
      "epoch 2401 loss tensor(6.6996, grad_fn=<NegBackward0>)\n",
      "epoch 2402 loss tensor(6.6994, grad_fn=<NegBackward0>)\n",
      "epoch 2403 loss tensor(6.6992, grad_fn=<NegBackward0>)\n",
      "epoch 2404 loss tensor(6.6990, grad_fn=<NegBackward0>)\n",
      "epoch 2405 loss tensor(6.6988, grad_fn=<NegBackward0>)\n",
      "epoch 2406 loss tensor(6.6986, grad_fn=<NegBackward0>)\n",
      "epoch 2407 loss tensor(6.6984, grad_fn=<NegBackward0>)\n",
      "epoch 2408 loss tensor(6.6982, grad_fn=<NegBackward0>)\n",
      "epoch 2409 loss tensor(6.6980, grad_fn=<NegBackward0>)\n",
      "epoch 2410 loss tensor(6.6978, grad_fn=<NegBackward0>)\n",
      "epoch 2411 loss tensor(6.6976, grad_fn=<NegBackward0>)\n",
      "epoch 2412 loss tensor(6.6974, grad_fn=<NegBackward0>)\n",
      "epoch 2413 loss tensor(6.6972, grad_fn=<NegBackward0>)\n",
      "epoch 2414 loss tensor(6.6970, grad_fn=<NegBackward0>)\n",
      "epoch 2415 loss tensor(6.6968, grad_fn=<NegBackward0>)\n",
      "epoch 2416 loss tensor(6.6967, grad_fn=<NegBackward0>)\n",
      "epoch 2417 loss tensor(6.6965, grad_fn=<NegBackward0>)\n",
      "epoch 2418 loss tensor(6.6963, grad_fn=<NegBackward0>)\n",
      "epoch 2419 loss tensor(6.6961, grad_fn=<NegBackward0>)\n",
      "epoch 2420 loss tensor(6.6959, grad_fn=<NegBackward0>)\n",
      "epoch 2421 loss tensor(6.6957, grad_fn=<NegBackward0>)\n",
      "epoch 2422 loss tensor(6.6955, grad_fn=<NegBackward0>)\n",
      "epoch 2423 loss tensor(6.6953, grad_fn=<NegBackward0>)\n",
      "epoch 2424 loss tensor(6.6951, grad_fn=<NegBackward0>)\n",
      "epoch 2425 loss tensor(6.6949, grad_fn=<NegBackward0>)\n",
      "epoch 2426 loss tensor(6.6947, grad_fn=<NegBackward0>)\n",
      "epoch 2427 loss tensor(6.6945, grad_fn=<NegBackward0>)\n",
      "epoch 2428 loss tensor(6.6943, grad_fn=<NegBackward0>)\n",
      "epoch 2429 loss tensor(6.6941, grad_fn=<NegBackward0>)\n",
      "epoch 2430 loss tensor(6.6939, grad_fn=<NegBackward0>)\n",
      "epoch 2431 loss tensor(6.6937, grad_fn=<NegBackward0>)\n",
      "epoch 2432 loss tensor(6.6935, grad_fn=<NegBackward0>)\n",
      "epoch 2433 loss tensor(6.6933, grad_fn=<NegBackward0>)\n",
      "epoch 2434 loss tensor(6.6931, grad_fn=<NegBackward0>)\n",
      "epoch 2435 loss tensor(6.6930, grad_fn=<NegBackward0>)\n",
      "epoch 2436 loss tensor(6.6928, grad_fn=<NegBackward0>)\n",
      "epoch 2437 loss tensor(6.6926, grad_fn=<NegBackward0>)\n",
      "epoch 2438 loss tensor(6.6924, grad_fn=<NegBackward0>)\n",
      "epoch 2439 loss tensor(6.6922, grad_fn=<NegBackward0>)\n",
      "epoch 2440 loss tensor(6.6920, grad_fn=<NegBackward0>)\n",
      "epoch 2441 loss tensor(6.6918, grad_fn=<NegBackward0>)\n",
      "epoch 2442 loss tensor(6.6916, grad_fn=<NegBackward0>)\n",
      "epoch 2443 loss tensor(6.6914, grad_fn=<NegBackward0>)\n",
      "epoch 2444 loss tensor(6.6912, grad_fn=<NegBackward0>)\n",
      "epoch 2445 loss tensor(6.6910, grad_fn=<NegBackward0>)\n",
      "epoch 2446 loss tensor(6.6908, grad_fn=<NegBackward0>)\n",
      "epoch 2447 loss tensor(6.6906, grad_fn=<NegBackward0>)\n",
      "epoch 2448 loss tensor(6.6904, grad_fn=<NegBackward0>)\n",
      "epoch 2449 loss tensor(6.6902, grad_fn=<NegBackward0>)\n",
      "epoch 2450 loss tensor(6.6901, grad_fn=<NegBackward0>)\n",
      "epoch 2451 loss tensor(6.6899, grad_fn=<NegBackward0>)\n",
      "epoch 2452 loss tensor(6.6897, grad_fn=<NegBackward0>)\n",
      "epoch 2453 loss tensor(6.6895, grad_fn=<NegBackward0>)\n",
      "epoch 2454 loss tensor(6.6893, grad_fn=<NegBackward0>)\n",
      "epoch 2455 loss tensor(6.6891, grad_fn=<NegBackward0>)\n",
      "epoch 2456 loss tensor(6.6889, grad_fn=<NegBackward0>)\n",
      "epoch 2457 loss tensor(6.6887, grad_fn=<NegBackward0>)\n",
      "epoch 2458 loss tensor(6.6885, grad_fn=<NegBackward0>)\n",
      "epoch 2459 loss tensor(6.6883, grad_fn=<NegBackward0>)\n",
      "epoch 2460 loss tensor(6.6881, grad_fn=<NegBackward0>)\n",
      "epoch 2461 loss tensor(6.6879, grad_fn=<NegBackward0>)\n",
      "epoch 2462 loss tensor(6.6877, grad_fn=<NegBackward0>)\n",
      "epoch 2463 loss tensor(6.6875, grad_fn=<NegBackward0>)\n",
      "epoch 2464 loss tensor(6.6874, grad_fn=<NegBackward0>)\n",
      "epoch 2465 loss tensor(6.6872, grad_fn=<NegBackward0>)\n",
      "epoch 2466 loss tensor(6.6870, grad_fn=<NegBackward0>)\n",
      "epoch 2467 loss tensor(6.6868, grad_fn=<NegBackward0>)\n",
      "epoch 2468 loss tensor(6.6866, grad_fn=<NegBackward0>)\n",
      "epoch 2469 loss tensor(6.6864, grad_fn=<NegBackward0>)\n",
      "epoch 2470 loss tensor(6.6862, grad_fn=<NegBackward0>)\n",
      "epoch 2471 loss tensor(6.6860, grad_fn=<NegBackward0>)\n",
      "epoch 2472 loss tensor(6.6858, grad_fn=<NegBackward0>)\n",
      "epoch 2473 loss tensor(6.6856, grad_fn=<NegBackward0>)\n",
      "epoch 2474 loss tensor(6.6854, grad_fn=<NegBackward0>)\n",
      "epoch 2475 loss tensor(6.6852, grad_fn=<NegBackward0>)\n",
      "epoch 2476 loss tensor(6.6851, grad_fn=<NegBackward0>)\n",
      "epoch 2477 loss tensor(6.6849, grad_fn=<NegBackward0>)\n",
      "epoch 2478 loss tensor(6.6847, grad_fn=<NegBackward0>)\n",
      "epoch 2479 loss tensor(6.6845, grad_fn=<NegBackward0>)\n",
      "epoch 2480 loss tensor(6.6843, grad_fn=<NegBackward0>)\n",
      "epoch 2481 loss tensor(6.6841, grad_fn=<NegBackward0>)\n",
      "epoch 2482 loss tensor(6.6839, grad_fn=<NegBackward0>)\n",
      "epoch 2483 loss tensor(6.6837, grad_fn=<NegBackward0>)\n",
      "epoch 2484 loss tensor(6.6835, grad_fn=<NegBackward0>)\n",
      "epoch 2485 loss tensor(6.6833, grad_fn=<NegBackward0>)\n",
      "epoch 2486 loss tensor(6.6831, grad_fn=<NegBackward0>)\n",
      "epoch 2487 loss tensor(6.6829, grad_fn=<NegBackward0>)\n",
      "epoch 2488 loss tensor(6.6828, grad_fn=<NegBackward0>)\n",
      "epoch 2489 loss tensor(6.6826, grad_fn=<NegBackward0>)\n",
      "epoch 2490 loss tensor(6.6824, grad_fn=<NegBackward0>)\n",
      "epoch 2491 loss tensor(6.6822, grad_fn=<NegBackward0>)\n",
      "epoch 2492 loss tensor(6.6820, grad_fn=<NegBackward0>)\n",
      "epoch 2493 loss tensor(6.6818, grad_fn=<NegBackward0>)\n",
      "epoch 2494 loss tensor(6.6816, grad_fn=<NegBackward0>)\n",
      "epoch 2495 loss tensor(6.6814, grad_fn=<NegBackward0>)\n",
      "epoch 2496 loss tensor(6.6812, grad_fn=<NegBackward0>)\n",
      "epoch 2497 loss tensor(6.6810, grad_fn=<NegBackward0>)\n",
      "epoch 2498 loss tensor(6.6808, grad_fn=<NegBackward0>)\n",
      "epoch 2499 loss tensor(6.6807, grad_fn=<NegBackward0>)\n",
      "epoch 2500 loss tensor(6.6805, grad_fn=<NegBackward0>)\n",
      "epoch 2501 loss tensor(6.6803, grad_fn=<NegBackward0>)\n",
      "epoch 2502 loss tensor(6.6801, grad_fn=<NegBackward0>)\n",
      "epoch 2503 loss tensor(6.6799, grad_fn=<NegBackward0>)\n",
      "epoch 2504 loss tensor(6.6797, grad_fn=<NegBackward0>)\n",
      "epoch 2505 loss tensor(6.6795, grad_fn=<NegBackward0>)\n",
      "epoch 2506 loss tensor(6.6793, grad_fn=<NegBackward0>)\n",
      "epoch 2507 loss tensor(6.6791, grad_fn=<NegBackward0>)\n",
      "epoch 2508 loss tensor(6.6789, grad_fn=<NegBackward0>)\n",
      "epoch 2509 loss tensor(6.6788, grad_fn=<NegBackward0>)\n",
      "epoch 2510 loss tensor(6.6786, grad_fn=<NegBackward0>)\n",
      "epoch 2511 loss tensor(6.6784, grad_fn=<NegBackward0>)\n",
      "epoch 2512 loss tensor(6.6782, grad_fn=<NegBackward0>)\n",
      "epoch 2513 loss tensor(6.6780, grad_fn=<NegBackward0>)\n",
      "epoch 2514 loss tensor(6.6778, grad_fn=<NegBackward0>)\n",
      "epoch 2515 loss tensor(6.6776, grad_fn=<NegBackward0>)\n",
      "epoch 2516 loss tensor(6.6774, grad_fn=<NegBackward0>)\n",
      "epoch 2517 loss tensor(6.6772, grad_fn=<NegBackward0>)\n",
      "epoch 2518 loss tensor(6.6771, grad_fn=<NegBackward0>)\n",
      "epoch 2519 loss tensor(6.6769, grad_fn=<NegBackward0>)\n",
      "epoch 2520 loss tensor(6.6767, grad_fn=<NegBackward0>)\n",
      "epoch 2521 loss tensor(6.6765, grad_fn=<NegBackward0>)\n",
      "epoch 2522 loss tensor(6.6763, grad_fn=<NegBackward0>)\n",
      "epoch 2523 loss tensor(6.6761, grad_fn=<NegBackward0>)\n",
      "epoch 2524 loss tensor(6.6759, grad_fn=<NegBackward0>)\n",
      "epoch 2525 loss tensor(6.6757, grad_fn=<NegBackward0>)\n",
      "epoch 2526 loss tensor(6.6755, grad_fn=<NegBackward0>)\n",
      "epoch 2527 loss tensor(6.6754, grad_fn=<NegBackward0>)\n",
      "epoch 2528 loss tensor(6.6752, grad_fn=<NegBackward0>)\n",
      "epoch 2529 loss tensor(6.6750, grad_fn=<NegBackward0>)\n",
      "epoch 2530 loss tensor(6.6748, grad_fn=<NegBackward0>)\n",
      "epoch 2531 loss tensor(6.6746, grad_fn=<NegBackward0>)\n",
      "epoch 2532 loss tensor(6.6744, grad_fn=<NegBackward0>)\n",
      "epoch 2533 loss tensor(6.6742, grad_fn=<NegBackward0>)\n",
      "epoch 2534 loss tensor(6.6740, grad_fn=<NegBackward0>)\n",
      "epoch 2535 loss tensor(6.6738, grad_fn=<NegBackward0>)\n",
      "epoch 2536 loss tensor(6.6737, grad_fn=<NegBackward0>)\n",
      "epoch 2537 loss tensor(6.6735, grad_fn=<NegBackward0>)\n",
      "epoch 2538 loss tensor(6.6733, grad_fn=<NegBackward0>)\n",
      "epoch 2539 loss tensor(6.6731, grad_fn=<NegBackward0>)\n",
      "epoch 2540 loss tensor(6.6729, grad_fn=<NegBackward0>)\n",
      "epoch 2541 loss tensor(6.6727, grad_fn=<NegBackward0>)\n",
      "epoch 2542 loss tensor(6.6725, grad_fn=<NegBackward0>)\n",
      "epoch 2543 loss tensor(6.6723, grad_fn=<NegBackward0>)\n",
      "epoch 2544 loss tensor(6.6721, grad_fn=<NegBackward0>)\n",
      "epoch 2545 loss tensor(6.6720, grad_fn=<NegBackward0>)\n",
      "epoch 2546 loss tensor(6.6718, grad_fn=<NegBackward0>)\n",
      "epoch 2547 loss tensor(6.6716, grad_fn=<NegBackward0>)\n",
      "epoch 2548 loss tensor(6.6714, grad_fn=<NegBackward0>)\n",
      "epoch 2549 loss tensor(6.6712, grad_fn=<NegBackward0>)\n",
      "epoch 2550 loss tensor(6.6710, grad_fn=<NegBackward0>)\n",
      "epoch 2551 loss tensor(6.6708, grad_fn=<NegBackward0>)\n",
      "epoch 2552 loss tensor(6.6706, grad_fn=<NegBackward0>)\n",
      "epoch 2553 loss tensor(6.6705, grad_fn=<NegBackward0>)\n",
      "epoch 2554 loss tensor(6.6703, grad_fn=<NegBackward0>)\n",
      "epoch 2555 loss tensor(6.6701, grad_fn=<NegBackward0>)\n",
      "epoch 2556 loss tensor(6.6699, grad_fn=<NegBackward0>)\n",
      "epoch 2557 loss tensor(6.6697, grad_fn=<NegBackward0>)\n",
      "epoch 2558 loss tensor(6.6695, grad_fn=<NegBackward0>)\n",
      "epoch 2559 loss tensor(6.6693, grad_fn=<NegBackward0>)\n",
      "epoch 2560 loss tensor(6.6691, grad_fn=<NegBackward0>)\n",
      "epoch 2561 loss tensor(6.6690, grad_fn=<NegBackward0>)\n",
      "epoch 2562 loss tensor(6.6688, grad_fn=<NegBackward0>)\n",
      "epoch 2563 loss tensor(6.6686, grad_fn=<NegBackward0>)\n",
      "epoch 2564 loss tensor(6.6684, grad_fn=<NegBackward0>)\n",
      "epoch 2565 loss tensor(6.6682, grad_fn=<NegBackward0>)\n",
      "epoch 2566 loss tensor(6.6680, grad_fn=<NegBackward0>)\n",
      "epoch 2567 loss tensor(6.6678, grad_fn=<NegBackward0>)\n",
      "epoch 2568 loss tensor(6.6677, grad_fn=<NegBackward0>)\n",
      "epoch 2569 loss tensor(6.6675, grad_fn=<NegBackward0>)\n",
      "epoch 2570 loss tensor(6.6673, grad_fn=<NegBackward0>)\n",
      "epoch 2571 loss tensor(6.6671, grad_fn=<NegBackward0>)\n",
      "epoch 2572 loss tensor(6.6669, grad_fn=<NegBackward0>)\n",
      "epoch 2573 loss tensor(6.6667, grad_fn=<NegBackward0>)\n",
      "epoch 2574 loss tensor(6.6665, grad_fn=<NegBackward0>)\n",
      "epoch 2575 loss tensor(6.6664, grad_fn=<NegBackward0>)\n",
      "epoch 2576 loss tensor(6.6662, grad_fn=<NegBackward0>)\n",
      "epoch 2577 loss tensor(6.6660, grad_fn=<NegBackward0>)\n",
      "epoch 2578 loss tensor(6.6658, grad_fn=<NegBackward0>)\n",
      "epoch 2579 loss tensor(6.6656, grad_fn=<NegBackward0>)\n",
      "epoch 2580 loss tensor(6.6654, grad_fn=<NegBackward0>)\n",
      "epoch 2581 loss tensor(6.6652, grad_fn=<NegBackward0>)\n",
      "epoch 2582 loss tensor(6.6650, grad_fn=<NegBackward0>)\n",
      "epoch 2583 loss tensor(6.6649, grad_fn=<NegBackward0>)\n",
      "epoch 2584 loss tensor(6.6647, grad_fn=<NegBackward0>)\n",
      "epoch 2585 loss tensor(6.6645, grad_fn=<NegBackward0>)\n",
      "epoch 2586 loss tensor(6.6643, grad_fn=<NegBackward0>)\n",
      "epoch 2587 loss tensor(6.6641, grad_fn=<NegBackward0>)\n",
      "epoch 2588 loss tensor(6.6639, grad_fn=<NegBackward0>)\n",
      "epoch 2589 loss tensor(6.6638, grad_fn=<NegBackward0>)\n",
      "epoch 2590 loss tensor(6.6636, grad_fn=<NegBackward0>)\n",
      "epoch 2591 loss tensor(6.6634, grad_fn=<NegBackward0>)\n",
      "epoch 2592 loss tensor(6.6632, grad_fn=<NegBackward0>)\n",
      "epoch 2593 loss tensor(6.6630, grad_fn=<NegBackward0>)\n",
      "epoch 2594 loss tensor(6.6628, grad_fn=<NegBackward0>)\n",
      "epoch 2595 loss tensor(6.6626, grad_fn=<NegBackward0>)\n",
      "epoch 2596 loss tensor(6.6625, grad_fn=<NegBackward0>)\n",
      "epoch 2597 loss tensor(6.6623, grad_fn=<NegBackward0>)\n",
      "epoch 2598 loss tensor(6.6621, grad_fn=<NegBackward0>)\n",
      "epoch 2599 loss tensor(6.6619, grad_fn=<NegBackward0>)\n",
      "epoch 2600 loss tensor(6.6617, grad_fn=<NegBackward0>)\n",
      "epoch 2601 loss tensor(6.6615, grad_fn=<NegBackward0>)\n",
      "epoch 2602 loss tensor(6.6613, grad_fn=<NegBackward0>)\n",
      "epoch 2603 loss tensor(6.6612, grad_fn=<NegBackward0>)\n",
      "epoch 2604 loss tensor(6.6610, grad_fn=<NegBackward0>)\n",
      "epoch 2605 loss tensor(6.6608, grad_fn=<NegBackward0>)\n",
      "epoch 2606 loss tensor(6.6606, grad_fn=<NegBackward0>)\n",
      "epoch 2607 loss tensor(6.6604, grad_fn=<NegBackward0>)\n",
      "epoch 2608 loss tensor(6.6602, grad_fn=<NegBackward0>)\n",
      "epoch 2609 loss tensor(6.6601, grad_fn=<NegBackward0>)\n",
      "epoch 2610 loss tensor(6.6599, grad_fn=<NegBackward0>)\n",
      "epoch 2611 loss tensor(6.6597, grad_fn=<NegBackward0>)\n",
      "epoch 2612 loss tensor(6.6595, grad_fn=<NegBackward0>)\n",
      "epoch 2613 loss tensor(6.6593, grad_fn=<NegBackward0>)\n",
      "epoch 2614 loss tensor(6.6591, grad_fn=<NegBackward0>)\n",
      "epoch 2615 loss tensor(6.6589, grad_fn=<NegBackward0>)\n",
      "epoch 2616 loss tensor(6.6588, grad_fn=<NegBackward0>)\n",
      "epoch 2617 loss tensor(6.6586, grad_fn=<NegBackward0>)\n",
      "epoch 2618 loss tensor(6.6584, grad_fn=<NegBackward0>)\n",
      "epoch 2619 loss tensor(6.6582, grad_fn=<NegBackward0>)\n",
      "epoch 2620 loss tensor(6.6580, grad_fn=<NegBackward0>)\n",
      "epoch 2621 loss tensor(6.6578, grad_fn=<NegBackward0>)\n",
      "epoch 2622 loss tensor(6.6577, grad_fn=<NegBackward0>)\n",
      "epoch 2623 loss tensor(6.6575, grad_fn=<NegBackward0>)\n",
      "epoch 2624 loss tensor(6.6573, grad_fn=<NegBackward0>)\n",
      "epoch 2625 loss tensor(6.6571, grad_fn=<NegBackward0>)\n",
      "epoch 2626 loss tensor(6.6569, grad_fn=<NegBackward0>)\n",
      "epoch 2627 loss tensor(6.6567, grad_fn=<NegBackward0>)\n",
      "epoch 2628 loss tensor(6.6566, grad_fn=<NegBackward0>)\n",
      "epoch 2629 loss tensor(6.6564, grad_fn=<NegBackward0>)\n",
      "epoch 2630 loss tensor(6.6562, grad_fn=<NegBackward0>)\n",
      "epoch 2631 loss tensor(6.6560, grad_fn=<NegBackward0>)\n",
      "epoch 2632 loss tensor(6.6558, grad_fn=<NegBackward0>)\n",
      "epoch 2633 loss tensor(6.6556, grad_fn=<NegBackward0>)\n",
      "epoch 2634 loss tensor(6.6555, grad_fn=<NegBackward0>)\n",
      "epoch 2635 loss tensor(6.6553, grad_fn=<NegBackward0>)\n",
      "epoch 2636 loss tensor(6.6551, grad_fn=<NegBackward0>)\n",
      "epoch 2637 loss tensor(6.6549, grad_fn=<NegBackward0>)\n",
      "epoch 2638 loss tensor(6.6547, grad_fn=<NegBackward0>)\n",
      "epoch 2639 loss tensor(6.6545, grad_fn=<NegBackward0>)\n",
      "epoch 2640 loss tensor(6.6544, grad_fn=<NegBackward0>)\n",
      "epoch 2641 loss tensor(6.6542, grad_fn=<NegBackward0>)\n",
      "epoch 2642 loss tensor(6.6540, grad_fn=<NegBackward0>)\n",
      "epoch 2643 loss tensor(6.6538, grad_fn=<NegBackward0>)\n",
      "epoch 2644 loss tensor(6.6536, grad_fn=<NegBackward0>)\n",
      "epoch 2645 loss tensor(6.6534, grad_fn=<NegBackward0>)\n",
      "epoch 2646 loss tensor(6.6533, grad_fn=<NegBackward0>)\n",
      "epoch 2647 loss tensor(6.6531, grad_fn=<NegBackward0>)\n",
      "epoch 2648 loss tensor(6.6529, grad_fn=<NegBackward0>)\n",
      "epoch 2649 loss tensor(6.6527, grad_fn=<NegBackward0>)\n",
      "epoch 2650 loss tensor(6.6525, grad_fn=<NegBackward0>)\n",
      "epoch 2651 loss tensor(6.6524, grad_fn=<NegBackward0>)\n",
      "epoch 2652 loss tensor(6.6522, grad_fn=<NegBackward0>)\n",
      "epoch 2653 loss tensor(6.6520, grad_fn=<NegBackward0>)\n",
      "epoch 2654 loss tensor(6.6518, grad_fn=<NegBackward0>)\n",
      "epoch 2655 loss tensor(6.6516, grad_fn=<NegBackward0>)\n",
      "epoch 2656 loss tensor(6.6514, grad_fn=<NegBackward0>)\n",
      "epoch 2657 loss tensor(6.6513, grad_fn=<NegBackward0>)\n",
      "epoch 2658 loss tensor(6.6511, grad_fn=<NegBackward0>)\n",
      "epoch 2659 loss tensor(6.6509, grad_fn=<NegBackward0>)\n",
      "epoch 2660 loss tensor(6.6507, grad_fn=<NegBackward0>)\n",
      "epoch 2661 loss tensor(6.6505, grad_fn=<NegBackward0>)\n",
      "epoch 2662 loss tensor(6.6504, grad_fn=<NegBackward0>)\n",
      "epoch 2663 loss tensor(6.6502, grad_fn=<NegBackward0>)\n",
      "epoch 2664 loss tensor(6.6500, grad_fn=<NegBackward0>)\n",
      "epoch 2665 loss tensor(6.6498, grad_fn=<NegBackward0>)\n",
      "epoch 2666 loss tensor(6.6496, grad_fn=<NegBackward0>)\n",
      "epoch 2667 loss tensor(6.6494, grad_fn=<NegBackward0>)\n",
      "epoch 2668 loss tensor(6.6493, grad_fn=<NegBackward0>)\n",
      "epoch 2669 loss tensor(6.6491, grad_fn=<NegBackward0>)\n",
      "epoch 2670 loss tensor(6.6489, grad_fn=<NegBackward0>)\n",
      "epoch 2671 loss tensor(6.6487, grad_fn=<NegBackward0>)\n",
      "epoch 2672 loss tensor(6.6485, grad_fn=<NegBackward0>)\n",
      "epoch 2673 loss tensor(6.6484, grad_fn=<NegBackward0>)\n",
      "epoch 2674 loss tensor(6.6482, grad_fn=<NegBackward0>)\n",
      "epoch 2675 loss tensor(6.6480, grad_fn=<NegBackward0>)\n",
      "epoch 2676 loss tensor(6.6478, grad_fn=<NegBackward0>)\n",
      "epoch 2677 loss tensor(6.6476, grad_fn=<NegBackward0>)\n",
      "epoch 2678 loss tensor(6.6475, grad_fn=<NegBackward0>)\n",
      "epoch 2679 loss tensor(6.6473, grad_fn=<NegBackward0>)\n",
      "epoch 2680 loss tensor(6.6471, grad_fn=<NegBackward0>)\n",
      "epoch 2681 loss tensor(6.6469, grad_fn=<NegBackward0>)\n",
      "epoch 2682 loss tensor(6.6467, grad_fn=<NegBackward0>)\n",
      "epoch 2683 loss tensor(6.6466, grad_fn=<NegBackward0>)\n",
      "epoch 2684 loss tensor(6.6464, grad_fn=<NegBackward0>)\n",
      "epoch 2685 loss tensor(6.6462, grad_fn=<NegBackward0>)\n",
      "epoch 2686 loss tensor(6.6460, grad_fn=<NegBackward0>)\n",
      "epoch 2687 loss tensor(6.6458, grad_fn=<NegBackward0>)\n",
      "epoch 2688 loss tensor(6.6457, grad_fn=<NegBackward0>)\n",
      "epoch 2689 loss tensor(6.6455, grad_fn=<NegBackward0>)\n",
      "epoch 2690 loss tensor(6.6453, grad_fn=<NegBackward0>)\n",
      "epoch 2691 loss tensor(6.6451, grad_fn=<NegBackward0>)\n",
      "epoch 2692 loss tensor(6.6449, grad_fn=<NegBackward0>)\n",
      "epoch 2693 loss tensor(6.6448, grad_fn=<NegBackward0>)\n",
      "epoch 2694 loss tensor(6.6446, grad_fn=<NegBackward0>)\n",
      "epoch 2695 loss tensor(6.6444, grad_fn=<NegBackward0>)\n",
      "epoch 2696 loss tensor(6.6442, grad_fn=<NegBackward0>)\n",
      "epoch 2697 loss tensor(6.6440, grad_fn=<NegBackward0>)\n",
      "epoch 2698 loss tensor(6.6439, grad_fn=<NegBackward0>)\n",
      "epoch 2699 loss tensor(6.6437, grad_fn=<NegBackward0>)\n",
      "epoch 2700 loss tensor(6.6435, grad_fn=<NegBackward0>)\n",
      "epoch 2701 loss tensor(6.6433, grad_fn=<NegBackward0>)\n",
      "epoch 2702 loss tensor(6.6431, grad_fn=<NegBackward0>)\n",
      "epoch 2703 loss tensor(6.6430, grad_fn=<NegBackward0>)\n",
      "epoch 2704 loss tensor(6.6428, grad_fn=<NegBackward0>)\n",
      "epoch 2705 loss tensor(6.6426, grad_fn=<NegBackward0>)\n",
      "epoch 2706 loss tensor(6.6424, grad_fn=<NegBackward0>)\n",
      "epoch 2707 loss tensor(6.6422, grad_fn=<NegBackward0>)\n",
      "epoch 2708 loss tensor(6.6421, grad_fn=<NegBackward0>)\n",
      "epoch 2709 loss tensor(6.6419, grad_fn=<NegBackward0>)\n",
      "epoch 2710 loss tensor(6.6417, grad_fn=<NegBackward0>)\n",
      "epoch 2711 loss tensor(6.6415, grad_fn=<NegBackward0>)\n",
      "epoch 2712 loss tensor(6.6413, grad_fn=<NegBackward0>)\n",
      "epoch 2713 loss tensor(6.6412, grad_fn=<NegBackward0>)\n",
      "epoch 2714 loss tensor(6.6410, grad_fn=<NegBackward0>)\n",
      "epoch 2715 loss tensor(6.6408, grad_fn=<NegBackward0>)\n",
      "epoch 2716 loss tensor(6.6406, grad_fn=<NegBackward0>)\n",
      "epoch 2717 loss tensor(6.6405, grad_fn=<NegBackward0>)\n",
      "epoch 2718 loss tensor(6.6403, grad_fn=<NegBackward0>)\n",
      "epoch 2719 loss tensor(6.6401, grad_fn=<NegBackward0>)\n",
      "epoch 2720 loss tensor(6.6399, grad_fn=<NegBackward0>)\n",
      "epoch 2721 loss tensor(6.6397, grad_fn=<NegBackward0>)\n",
      "epoch 2722 loss tensor(6.6396, grad_fn=<NegBackward0>)\n",
      "epoch 2723 loss tensor(6.6394, grad_fn=<NegBackward0>)\n",
      "epoch 2724 loss tensor(6.6392, grad_fn=<NegBackward0>)\n",
      "epoch 2725 loss tensor(6.6390, grad_fn=<NegBackward0>)\n",
      "epoch 2726 loss tensor(6.6388, grad_fn=<NegBackward0>)\n",
      "epoch 2727 loss tensor(6.6387, grad_fn=<NegBackward0>)\n",
      "epoch 2728 loss tensor(6.6385, grad_fn=<NegBackward0>)\n",
      "epoch 2729 loss tensor(6.6383, grad_fn=<NegBackward0>)\n",
      "epoch 2730 loss tensor(6.6381, grad_fn=<NegBackward0>)\n",
      "epoch 2731 loss tensor(6.6380, grad_fn=<NegBackward0>)\n",
      "epoch 2732 loss tensor(6.6378, grad_fn=<NegBackward0>)\n",
      "epoch 2733 loss tensor(6.6376, grad_fn=<NegBackward0>)\n",
      "epoch 2734 loss tensor(6.6374, grad_fn=<NegBackward0>)\n",
      "epoch 2735 loss tensor(6.6372, grad_fn=<NegBackward0>)\n",
      "epoch 2736 loss tensor(6.6371, grad_fn=<NegBackward0>)\n",
      "epoch 2737 loss tensor(6.6369, grad_fn=<NegBackward0>)\n",
      "epoch 2738 loss tensor(6.6367, grad_fn=<NegBackward0>)\n",
      "epoch 2739 loss tensor(6.6365, grad_fn=<NegBackward0>)\n",
      "epoch 2740 loss tensor(6.6364, grad_fn=<NegBackward0>)\n",
      "epoch 2741 loss tensor(6.6362, grad_fn=<NegBackward0>)\n",
      "epoch 2742 loss tensor(6.6360, grad_fn=<NegBackward0>)\n",
      "epoch 2743 loss tensor(6.6358, grad_fn=<NegBackward0>)\n",
      "epoch 2744 loss tensor(6.6356, grad_fn=<NegBackward0>)\n",
      "epoch 2745 loss tensor(6.6355, grad_fn=<NegBackward0>)\n",
      "epoch 2746 loss tensor(6.6353, grad_fn=<NegBackward0>)\n",
      "epoch 2747 loss tensor(6.6351, grad_fn=<NegBackward0>)\n",
      "epoch 2748 loss tensor(6.6349, grad_fn=<NegBackward0>)\n",
      "epoch 2749 loss tensor(6.6348, grad_fn=<NegBackward0>)\n",
      "epoch 2750 loss tensor(6.6346, grad_fn=<NegBackward0>)\n",
      "epoch 2751 loss tensor(6.6344, grad_fn=<NegBackward0>)\n",
      "epoch 2752 loss tensor(6.6342, grad_fn=<NegBackward0>)\n",
      "epoch 2753 loss tensor(6.6341, grad_fn=<NegBackward0>)\n",
      "epoch 2754 loss tensor(6.6339, grad_fn=<NegBackward0>)\n",
      "epoch 2755 loss tensor(6.6337, grad_fn=<NegBackward0>)\n",
      "epoch 2756 loss tensor(6.6335, grad_fn=<NegBackward0>)\n",
      "epoch 2757 loss tensor(6.6333, grad_fn=<NegBackward0>)\n",
      "epoch 2758 loss tensor(6.6332, grad_fn=<NegBackward0>)\n",
      "epoch 2759 loss tensor(6.6330, grad_fn=<NegBackward0>)\n",
      "epoch 2760 loss tensor(6.6328, grad_fn=<NegBackward0>)\n",
      "epoch 2761 loss tensor(6.6326, grad_fn=<NegBackward0>)\n",
      "epoch 2762 loss tensor(6.6325, grad_fn=<NegBackward0>)\n",
      "epoch 2763 loss tensor(6.6323, grad_fn=<NegBackward0>)\n",
      "epoch 2764 loss tensor(6.6321, grad_fn=<NegBackward0>)\n",
      "epoch 2765 loss tensor(6.6319, grad_fn=<NegBackward0>)\n",
      "epoch 2766 loss tensor(6.6318, grad_fn=<NegBackward0>)\n",
      "epoch 2767 loss tensor(6.6316, grad_fn=<NegBackward0>)\n",
      "epoch 2768 loss tensor(6.6314, grad_fn=<NegBackward0>)\n",
      "epoch 2769 loss tensor(6.6312, grad_fn=<NegBackward0>)\n",
      "epoch 2770 loss tensor(6.6311, grad_fn=<NegBackward0>)\n",
      "epoch 2771 loss tensor(6.6309, grad_fn=<NegBackward0>)\n",
      "epoch 2772 loss tensor(6.6307, grad_fn=<NegBackward0>)\n",
      "epoch 2773 loss tensor(6.6305, grad_fn=<NegBackward0>)\n",
      "epoch 2774 loss tensor(6.6304, grad_fn=<NegBackward0>)\n",
      "epoch 2775 loss tensor(6.6302, grad_fn=<NegBackward0>)\n",
      "epoch 2776 loss tensor(6.6300, grad_fn=<NegBackward0>)\n",
      "epoch 2777 loss tensor(6.6298, grad_fn=<NegBackward0>)\n",
      "epoch 2778 loss tensor(6.6297, grad_fn=<NegBackward0>)\n",
      "epoch 2779 loss tensor(6.6295, grad_fn=<NegBackward0>)\n",
      "epoch 2780 loss tensor(6.6293, grad_fn=<NegBackward0>)\n",
      "epoch 2781 loss tensor(6.6291, grad_fn=<NegBackward0>)\n",
      "epoch 2782 loss tensor(6.6290, grad_fn=<NegBackward0>)\n",
      "epoch 2783 loss tensor(6.6288, grad_fn=<NegBackward0>)\n",
      "epoch 2784 loss tensor(6.6286, grad_fn=<NegBackward0>)\n",
      "epoch 2785 loss tensor(6.6284, grad_fn=<NegBackward0>)\n",
      "epoch 2786 loss tensor(6.6283, grad_fn=<NegBackward0>)\n",
      "epoch 2787 loss tensor(6.6281, grad_fn=<NegBackward0>)\n",
      "epoch 2788 loss tensor(6.6279, grad_fn=<NegBackward0>)\n",
      "epoch 2789 loss tensor(6.6277, grad_fn=<NegBackward0>)\n",
      "epoch 2790 loss tensor(6.6276, grad_fn=<NegBackward0>)\n",
      "epoch 2791 loss tensor(6.6274, grad_fn=<NegBackward0>)\n",
      "epoch 2792 loss tensor(6.6272, grad_fn=<NegBackward0>)\n",
      "epoch 2793 loss tensor(6.6270, grad_fn=<NegBackward0>)\n",
      "epoch 2794 loss tensor(6.6269, grad_fn=<NegBackward0>)\n",
      "epoch 2795 loss tensor(6.6267, grad_fn=<NegBackward0>)\n",
      "epoch 2796 loss tensor(6.6265, grad_fn=<NegBackward0>)\n",
      "epoch 2797 loss tensor(6.6263, grad_fn=<NegBackward0>)\n",
      "epoch 2798 loss tensor(6.6262, grad_fn=<NegBackward0>)\n",
      "epoch 2799 loss tensor(6.6260, grad_fn=<NegBackward0>)\n",
      "epoch 2800 loss tensor(6.6258, grad_fn=<NegBackward0>)\n",
      "epoch 2801 loss tensor(6.6256, grad_fn=<NegBackward0>)\n",
      "epoch 2802 loss tensor(6.6255, grad_fn=<NegBackward0>)\n",
      "epoch 2803 loss tensor(6.6253, grad_fn=<NegBackward0>)\n",
      "epoch 2804 loss tensor(6.6251, grad_fn=<NegBackward0>)\n",
      "epoch 2805 loss tensor(6.6249, grad_fn=<NegBackward0>)\n",
      "epoch 2806 loss tensor(6.6248, grad_fn=<NegBackward0>)\n",
      "epoch 2807 loss tensor(6.6246, grad_fn=<NegBackward0>)\n",
      "epoch 2808 loss tensor(6.6244, grad_fn=<NegBackward0>)\n",
      "epoch 2809 loss tensor(6.6242, grad_fn=<NegBackward0>)\n",
      "epoch 2810 loss tensor(6.6241, grad_fn=<NegBackward0>)\n",
      "epoch 2811 loss tensor(6.6239, grad_fn=<NegBackward0>)\n",
      "epoch 2812 loss tensor(6.6237, grad_fn=<NegBackward0>)\n",
      "epoch 2813 loss tensor(6.6235, grad_fn=<NegBackward0>)\n",
      "epoch 2814 loss tensor(6.6234, grad_fn=<NegBackward0>)\n",
      "epoch 2815 loss tensor(6.6232, grad_fn=<NegBackward0>)\n",
      "epoch 2816 loss tensor(6.6230, grad_fn=<NegBackward0>)\n",
      "epoch 2817 loss tensor(6.6228, grad_fn=<NegBackward0>)\n",
      "epoch 2818 loss tensor(6.6227, grad_fn=<NegBackward0>)\n",
      "epoch 2819 loss tensor(6.6225, grad_fn=<NegBackward0>)\n",
      "epoch 2820 loss tensor(6.6223, grad_fn=<NegBackward0>)\n",
      "epoch 2821 loss tensor(6.6222, grad_fn=<NegBackward0>)\n",
      "epoch 2822 loss tensor(6.6220, grad_fn=<NegBackward0>)\n",
      "epoch 2823 loss tensor(6.6218, grad_fn=<NegBackward0>)\n",
      "epoch 2824 loss tensor(6.6216, grad_fn=<NegBackward0>)\n",
      "epoch 2825 loss tensor(6.6215, grad_fn=<NegBackward0>)\n",
      "epoch 2826 loss tensor(6.6213, grad_fn=<NegBackward0>)\n",
      "epoch 2827 loss tensor(6.6211, grad_fn=<NegBackward0>)\n",
      "epoch 2828 loss tensor(6.6209, grad_fn=<NegBackward0>)\n",
      "epoch 2829 loss tensor(6.6208, grad_fn=<NegBackward0>)\n",
      "epoch 2830 loss tensor(6.6206, grad_fn=<NegBackward0>)\n",
      "epoch 2831 loss tensor(6.6204, grad_fn=<NegBackward0>)\n",
      "epoch 2832 loss tensor(6.6203, grad_fn=<NegBackward0>)\n",
      "epoch 2833 loss tensor(6.6201, grad_fn=<NegBackward0>)\n",
      "epoch 2834 loss tensor(6.6199, grad_fn=<NegBackward0>)\n",
      "epoch 2835 loss tensor(6.6197, grad_fn=<NegBackward0>)\n",
      "epoch 2836 loss tensor(6.6196, grad_fn=<NegBackward0>)\n",
      "epoch 2837 loss tensor(6.6194, grad_fn=<NegBackward0>)\n",
      "epoch 2838 loss tensor(6.6192, grad_fn=<NegBackward0>)\n",
      "epoch 2839 loss tensor(6.6190, grad_fn=<NegBackward0>)\n",
      "epoch 2840 loss tensor(6.6189, grad_fn=<NegBackward0>)\n",
      "epoch 2841 loss tensor(6.6187, grad_fn=<NegBackward0>)\n",
      "epoch 2842 loss tensor(6.6185, grad_fn=<NegBackward0>)\n",
      "epoch 2843 loss tensor(6.6184, grad_fn=<NegBackward0>)\n",
      "epoch 2844 loss tensor(6.6182, grad_fn=<NegBackward0>)\n",
      "epoch 2845 loss tensor(6.6180, grad_fn=<NegBackward0>)\n",
      "epoch 2846 loss tensor(6.6178, grad_fn=<NegBackward0>)\n",
      "epoch 2847 loss tensor(6.6177, grad_fn=<NegBackward0>)\n",
      "epoch 2848 loss tensor(6.6175, grad_fn=<NegBackward0>)\n",
      "epoch 2849 loss tensor(6.6173, grad_fn=<NegBackward0>)\n",
      "epoch 2850 loss tensor(6.6172, grad_fn=<NegBackward0>)\n",
      "epoch 2851 loss tensor(6.6170, grad_fn=<NegBackward0>)\n",
      "epoch 2852 loss tensor(6.6168, grad_fn=<NegBackward0>)\n",
      "epoch 2853 loss tensor(6.6166, grad_fn=<NegBackward0>)\n",
      "epoch 2854 loss tensor(6.6165, grad_fn=<NegBackward0>)\n",
      "epoch 2855 loss tensor(6.6163, grad_fn=<NegBackward0>)\n",
      "epoch 2856 loss tensor(6.6161, grad_fn=<NegBackward0>)\n",
      "epoch 2857 loss tensor(6.6160, grad_fn=<NegBackward0>)\n",
      "epoch 2858 loss tensor(6.6158, grad_fn=<NegBackward0>)\n",
      "epoch 2859 loss tensor(6.6156, grad_fn=<NegBackward0>)\n",
      "epoch 2860 loss tensor(6.6154, grad_fn=<NegBackward0>)\n",
      "epoch 2861 loss tensor(6.6153, grad_fn=<NegBackward0>)\n",
      "epoch 2862 loss tensor(6.6151, grad_fn=<NegBackward0>)\n",
      "epoch 2863 loss tensor(6.6149, grad_fn=<NegBackward0>)\n",
      "epoch 2864 loss tensor(6.6148, grad_fn=<NegBackward0>)\n",
      "epoch 2865 loss tensor(6.6146, grad_fn=<NegBackward0>)\n",
      "epoch 2866 loss tensor(6.6144, grad_fn=<NegBackward0>)\n",
      "epoch 2867 loss tensor(6.6142, grad_fn=<NegBackward0>)\n",
      "epoch 2868 loss tensor(6.6141, grad_fn=<NegBackward0>)\n",
      "epoch 2869 loss tensor(6.6139, grad_fn=<NegBackward0>)\n",
      "epoch 2870 loss tensor(6.6137, grad_fn=<NegBackward0>)\n",
      "epoch 2871 loss tensor(6.6136, grad_fn=<NegBackward0>)\n",
      "epoch 2872 loss tensor(6.6134, grad_fn=<NegBackward0>)\n",
      "epoch 2873 loss tensor(6.6132, grad_fn=<NegBackward0>)\n",
      "epoch 2874 loss tensor(6.6130, grad_fn=<NegBackward0>)\n",
      "epoch 2875 loss tensor(6.6129, grad_fn=<NegBackward0>)\n",
      "epoch 2876 loss tensor(6.6127, grad_fn=<NegBackward0>)\n",
      "epoch 2877 loss tensor(6.6125, grad_fn=<NegBackward0>)\n",
      "epoch 2878 loss tensor(6.6124, grad_fn=<NegBackward0>)\n",
      "epoch 2879 loss tensor(6.6122, grad_fn=<NegBackward0>)\n",
      "epoch 2880 loss tensor(6.6120, grad_fn=<NegBackward0>)\n",
      "epoch 2881 loss tensor(6.6119, grad_fn=<NegBackward0>)\n",
      "epoch 2882 loss tensor(6.6117, grad_fn=<NegBackward0>)\n",
      "epoch 2883 loss tensor(6.6115, grad_fn=<NegBackward0>)\n",
      "epoch 2884 loss tensor(6.6113, grad_fn=<NegBackward0>)\n",
      "epoch 2885 loss tensor(6.6112, grad_fn=<NegBackward0>)\n",
      "epoch 2886 loss tensor(6.6110, grad_fn=<NegBackward0>)\n",
      "epoch 2887 loss tensor(6.6108, grad_fn=<NegBackward0>)\n",
      "epoch 2888 loss tensor(6.6107, grad_fn=<NegBackward0>)\n",
      "epoch 2889 loss tensor(6.6105, grad_fn=<NegBackward0>)\n",
      "epoch 2890 loss tensor(6.6103, grad_fn=<NegBackward0>)\n",
      "epoch 2891 loss tensor(6.6102, grad_fn=<NegBackward0>)\n",
      "epoch 2892 loss tensor(6.6100, grad_fn=<NegBackward0>)\n",
      "epoch 2893 loss tensor(6.6098, grad_fn=<NegBackward0>)\n",
      "epoch 2894 loss tensor(6.6096, grad_fn=<NegBackward0>)\n",
      "epoch 2895 loss tensor(6.6095, grad_fn=<NegBackward0>)\n",
      "epoch 2896 loss tensor(6.6093, grad_fn=<NegBackward0>)\n",
      "epoch 2897 loss tensor(6.6091, grad_fn=<NegBackward0>)\n",
      "epoch 2898 loss tensor(6.6090, grad_fn=<NegBackward0>)\n",
      "epoch 2899 loss tensor(6.6088, grad_fn=<NegBackward0>)\n",
      "epoch 2900 loss tensor(6.6086, grad_fn=<NegBackward0>)\n",
      "epoch 2901 loss tensor(6.6085, grad_fn=<NegBackward0>)\n",
      "epoch 2902 loss tensor(6.6083, grad_fn=<NegBackward0>)\n",
      "epoch 2903 loss tensor(6.6081, grad_fn=<NegBackward0>)\n",
      "epoch 2904 loss tensor(6.6080, grad_fn=<NegBackward0>)\n",
      "epoch 2905 loss tensor(6.6078, grad_fn=<NegBackward0>)\n",
      "epoch 2906 loss tensor(6.6076, grad_fn=<NegBackward0>)\n",
      "epoch 2907 loss tensor(6.6075, grad_fn=<NegBackward0>)\n",
      "epoch 2908 loss tensor(6.6073, grad_fn=<NegBackward0>)\n",
      "epoch 2909 loss tensor(6.6071, grad_fn=<NegBackward0>)\n",
      "epoch 2910 loss tensor(6.6069, grad_fn=<NegBackward0>)\n",
      "epoch 2911 loss tensor(6.6068, grad_fn=<NegBackward0>)\n",
      "epoch 2912 loss tensor(6.6066, grad_fn=<NegBackward0>)\n",
      "epoch 2913 loss tensor(6.6064, grad_fn=<NegBackward0>)\n",
      "epoch 2914 loss tensor(6.6063, grad_fn=<NegBackward0>)\n",
      "epoch 2915 loss tensor(6.6061, grad_fn=<NegBackward0>)\n",
      "epoch 2916 loss tensor(6.6059, grad_fn=<NegBackward0>)\n",
      "epoch 2917 loss tensor(6.6058, grad_fn=<NegBackward0>)\n",
      "epoch 2918 loss tensor(6.6056, grad_fn=<NegBackward0>)\n",
      "epoch 2919 loss tensor(6.6054, grad_fn=<NegBackward0>)\n",
      "epoch 2920 loss tensor(6.6053, grad_fn=<NegBackward0>)\n",
      "epoch 2921 loss tensor(6.6051, grad_fn=<NegBackward0>)\n",
      "epoch 2922 loss tensor(6.6049, grad_fn=<NegBackward0>)\n",
      "epoch 2923 loss tensor(6.6048, grad_fn=<NegBackward0>)\n",
      "epoch 2924 loss tensor(6.6046, grad_fn=<NegBackward0>)\n",
      "epoch 2925 loss tensor(6.6044, grad_fn=<NegBackward0>)\n",
      "epoch 2926 loss tensor(6.6043, grad_fn=<NegBackward0>)\n",
      "epoch 2927 loss tensor(6.6041, grad_fn=<NegBackward0>)\n",
      "epoch 2928 loss tensor(6.6039, grad_fn=<NegBackward0>)\n",
      "epoch 2929 loss tensor(6.6037, grad_fn=<NegBackward0>)\n",
      "epoch 2930 loss tensor(6.6036, grad_fn=<NegBackward0>)\n",
      "epoch 2931 loss tensor(6.6034, grad_fn=<NegBackward0>)\n",
      "epoch 2932 loss tensor(6.6032, grad_fn=<NegBackward0>)\n",
      "epoch 2933 loss tensor(6.6031, grad_fn=<NegBackward0>)\n",
      "epoch 2934 loss tensor(6.6029, grad_fn=<NegBackward0>)\n",
      "epoch 2935 loss tensor(6.6027, grad_fn=<NegBackward0>)\n",
      "epoch 2936 loss tensor(6.6026, grad_fn=<NegBackward0>)\n",
      "epoch 2937 loss tensor(6.6024, grad_fn=<NegBackward0>)\n",
      "epoch 2938 loss tensor(6.6022, grad_fn=<NegBackward0>)\n",
      "epoch 2939 loss tensor(6.6021, grad_fn=<NegBackward0>)\n",
      "epoch 2940 loss tensor(6.6019, grad_fn=<NegBackward0>)\n",
      "epoch 2941 loss tensor(6.6017, grad_fn=<NegBackward0>)\n",
      "epoch 2942 loss tensor(6.6016, grad_fn=<NegBackward0>)\n",
      "epoch 2943 loss tensor(6.6014, grad_fn=<NegBackward0>)\n",
      "epoch 2944 loss tensor(6.6012, grad_fn=<NegBackward0>)\n",
      "epoch 2945 loss tensor(6.6011, grad_fn=<NegBackward0>)\n",
      "epoch 2946 loss tensor(6.6009, grad_fn=<NegBackward0>)\n",
      "epoch 2947 loss tensor(6.6007, grad_fn=<NegBackward0>)\n",
      "epoch 2948 loss tensor(6.6006, grad_fn=<NegBackward0>)\n",
      "epoch 2949 loss tensor(6.6004, grad_fn=<NegBackward0>)\n",
      "epoch 2950 loss tensor(6.6002, grad_fn=<NegBackward0>)\n",
      "epoch 2951 loss tensor(6.6001, grad_fn=<NegBackward0>)\n",
      "epoch 2952 loss tensor(6.5999, grad_fn=<NegBackward0>)\n",
      "epoch 2953 loss tensor(6.5997, grad_fn=<NegBackward0>)\n",
      "epoch 2954 loss tensor(6.5996, grad_fn=<NegBackward0>)\n",
      "epoch 2955 loss tensor(6.5994, grad_fn=<NegBackward0>)\n",
      "epoch 2956 loss tensor(6.5992, grad_fn=<NegBackward0>)\n",
      "epoch 2957 loss tensor(6.5991, grad_fn=<NegBackward0>)\n",
      "epoch 2958 loss tensor(6.5989, grad_fn=<NegBackward0>)\n",
      "epoch 2959 loss tensor(6.5987, grad_fn=<NegBackward0>)\n",
      "epoch 2960 loss tensor(6.5986, grad_fn=<NegBackward0>)\n",
      "epoch 2961 loss tensor(6.5984, grad_fn=<NegBackward0>)\n",
      "epoch 2962 loss tensor(6.5982, grad_fn=<NegBackward0>)\n",
      "epoch 2963 loss tensor(6.5981, grad_fn=<NegBackward0>)\n",
      "epoch 2964 loss tensor(6.5979, grad_fn=<NegBackward0>)\n",
      "epoch 2965 loss tensor(6.5977, grad_fn=<NegBackward0>)\n",
      "epoch 2966 loss tensor(6.5976, grad_fn=<NegBackward0>)\n",
      "epoch 2967 loss tensor(6.5974, grad_fn=<NegBackward0>)\n",
      "epoch 2968 loss tensor(6.5972, grad_fn=<NegBackward0>)\n",
      "epoch 2969 loss tensor(6.5971, grad_fn=<NegBackward0>)\n",
      "epoch 2970 loss tensor(6.5969, grad_fn=<NegBackward0>)\n",
      "epoch 2971 loss tensor(6.5967, grad_fn=<NegBackward0>)\n",
      "epoch 2972 loss tensor(6.5966, grad_fn=<NegBackward0>)\n",
      "epoch 2973 loss tensor(6.5964, grad_fn=<NegBackward0>)\n",
      "epoch 2974 loss tensor(6.5963, grad_fn=<NegBackward0>)\n",
      "epoch 2975 loss tensor(6.5961, grad_fn=<NegBackward0>)\n",
      "epoch 2976 loss tensor(6.5959, grad_fn=<NegBackward0>)\n",
      "epoch 2977 loss tensor(6.5958, grad_fn=<NegBackward0>)\n",
      "epoch 2978 loss tensor(6.5956, grad_fn=<NegBackward0>)\n",
      "epoch 2979 loss tensor(6.5954, grad_fn=<NegBackward0>)\n",
      "epoch 2980 loss tensor(6.5953, grad_fn=<NegBackward0>)\n",
      "epoch 2981 loss tensor(6.5951, grad_fn=<NegBackward0>)\n",
      "epoch 2982 loss tensor(6.5949, grad_fn=<NegBackward0>)\n",
      "epoch 2983 loss tensor(6.5948, grad_fn=<NegBackward0>)\n",
      "epoch 2984 loss tensor(6.5946, grad_fn=<NegBackward0>)\n",
      "epoch 2985 loss tensor(6.5944, grad_fn=<NegBackward0>)\n",
      "epoch 2986 loss tensor(6.5943, grad_fn=<NegBackward0>)\n",
      "epoch 2987 loss tensor(6.5941, grad_fn=<NegBackward0>)\n",
      "epoch 2988 loss tensor(6.5939, grad_fn=<NegBackward0>)\n",
      "epoch 2989 loss tensor(6.5938, grad_fn=<NegBackward0>)\n",
      "epoch 2990 loss tensor(6.5936, grad_fn=<NegBackward0>)\n",
      "epoch 2991 loss tensor(6.5934, grad_fn=<NegBackward0>)\n",
      "epoch 2992 loss tensor(6.5933, grad_fn=<NegBackward0>)\n",
      "epoch 2993 loss tensor(6.5931, grad_fn=<NegBackward0>)\n",
      "epoch 2994 loss tensor(6.5929, grad_fn=<NegBackward0>)\n",
      "epoch 2995 loss tensor(6.5928, grad_fn=<NegBackward0>)\n",
      "epoch 2996 loss tensor(6.5926, grad_fn=<NegBackward0>)\n",
      "epoch 2997 loss tensor(6.5925, grad_fn=<NegBackward0>)\n",
      "epoch 2998 loss tensor(6.5923, grad_fn=<NegBackward0>)\n",
      "epoch 2999 loss tensor(6.5921, grad_fn=<NegBackward0>)\n",
      "epoch 3000 loss tensor(6.5920, grad_fn=<NegBackward0>)\n",
      "epoch 3001 loss tensor(6.5918, grad_fn=<NegBackward0>)\n",
      "epoch 3002 loss tensor(6.5916, grad_fn=<NegBackward0>)\n",
      "epoch 3003 loss tensor(6.5915, grad_fn=<NegBackward0>)\n",
      "epoch 3004 loss tensor(6.5913, grad_fn=<NegBackward0>)\n",
      "epoch 3005 loss tensor(6.5911, grad_fn=<NegBackward0>)\n",
      "epoch 3006 loss tensor(6.5910, grad_fn=<NegBackward0>)\n",
      "epoch 3007 loss tensor(6.5908, grad_fn=<NegBackward0>)\n",
      "epoch 3008 loss tensor(6.5907, grad_fn=<NegBackward0>)\n",
      "epoch 3009 loss tensor(6.5905, grad_fn=<NegBackward0>)\n",
      "epoch 3010 loss tensor(6.5903, grad_fn=<NegBackward0>)\n",
      "epoch 3011 loss tensor(6.5902, grad_fn=<NegBackward0>)\n",
      "epoch 3012 loss tensor(6.5900, grad_fn=<NegBackward0>)\n",
      "epoch 3013 loss tensor(6.5898, grad_fn=<NegBackward0>)\n",
      "epoch 3014 loss tensor(6.5897, grad_fn=<NegBackward0>)\n",
      "epoch 3015 loss tensor(6.5895, grad_fn=<NegBackward0>)\n",
      "epoch 3016 loss tensor(6.5893, grad_fn=<NegBackward0>)\n",
      "epoch 3017 loss tensor(6.5892, grad_fn=<NegBackward0>)\n",
      "epoch 3018 loss tensor(6.5890, grad_fn=<NegBackward0>)\n",
      "epoch 3019 loss tensor(6.5889, grad_fn=<NegBackward0>)\n",
      "epoch 3020 loss tensor(6.5887, grad_fn=<NegBackward0>)\n",
      "epoch 3021 loss tensor(6.5885, grad_fn=<NegBackward0>)\n",
      "epoch 3022 loss tensor(6.5884, grad_fn=<NegBackward0>)\n",
      "epoch 3023 loss tensor(6.5882, grad_fn=<NegBackward0>)\n",
      "epoch 3024 loss tensor(6.5880, grad_fn=<NegBackward0>)\n",
      "epoch 3025 loss tensor(6.5879, grad_fn=<NegBackward0>)\n",
      "epoch 3026 loss tensor(6.5877, grad_fn=<NegBackward0>)\n",
      "epoch 3027 loss tensor(6.5875, grad_fn=<NegBackward0>)\n",
      "epoch 3028 loss tensor(6.5874, grad_fn=<NegBackward0>)\n",
      "epoch 3029 loss tensor(6.5872, grad_fn=<NegBackward0>)\n",
      "epoch 3030 loss tensor(6.5871, grad_fn=<NegBackward0>)\n",
      "epoch 3031 loss tensor(6.5869, grad_fn=<NegBackward0>)\n",
      "epoch 3032 loss tensor(6.5867, grad_fn=<NegBackward0>)\n",
      "epoch 3033 loss tensor(6.5866, grad_fn=<NegBackward0>)\n",
      "epoch 3034 loss tensor(6.5864, grad_fn=<NegBackward0>)\n",
      "epoch 3035 loss tensor(6.5862, grad_fn=<NegBackward0>)\n",
      "epoch 3036 loss tensor(6.5861, grad_fn=<NegBackward0>)\n",
      "epoch 3037 loss tensor(6.5859, grad_fn=<NegBackward0>)\n",
      "epoch 3038 loss tensor(6.5858, grad_fn=<NegBackward0>)\n",
      "epoch 3039 loss tensor(6.5856, grad_fn=<NegBackward0>)\n",
      "epoch 3040 loss tensor(6.5854, grad_fn=<NegBackward0>)\n",
      "epoch 3041 loss tensor(6.5853, grad_fn=<NegBackward0>)\n",
      "epoch 3042 loss tensor(6.5851, grad_fn=<NegBackward0>)\n",
      "epoch 3043 loss tensor(6.5849, grad_fn=<NegBackward0>)\n",
      "epoch 3044 loss tensor(6.5848, grad_fn=<NegBackward0>)\n",
      "epoch 3045 loss tensor(6.5846, grad_fn=<NegBackward0>)\n",
      "epoch 3046 loss tensor(6.5845, grad_fn=<NegBackward0>)\n",
      "epoch 3047 loss tensor(6.5843, grad_fn=<NegBackward0>)\n",
      "epoch 3048 loss tensor(6.5841, grad_fn=<NegBackward0>)\n",
      "epoch 3049 loss tensor(6.5840, grad_fn=<NegBackward0>)\n",
      "epoch 3050 loss tensor(6.5838, grad_fn=<NegBackward0>)\n",
      "epoch 3051 loss tensor(6.5837, grad_fn=<NegBackward0>)\n",
      "epoch 3052 loss tensor(6.5835, grad_fn=<NegBackward0>)\n",
      "epoch 3053 loss tensor(6.5833, grad_fn=<NegBackward0>)\n",
      "epoch 3054 loss tensor(6.5832, grad_fn=<NegBackward0>)\n",
      "epoch 3055 loss tensor(6.5830, grad_fn=<NegBackward0>)\n",
      "epoch 3056 loss tensor(6.5828, grad_fn=<NegBackward0>)\n",
      "epoch 3057 loss tensor(6.5827, grad_fn=<NegBackward0>)\n",
      "epoch 3058 loss tensor(6.5825, grad_fn=<NegBackward0>)\n",
      "epoch 3059 loss tensor(6.5824, grad_fn=<NegBackward0>)\n",
      "epoch 3060 loss tensor(6.5822, grad_fn=<NegBackward0>)\n",
      "epoch 3061 loss tensor(6.5820, grad_fn=<NegBackward0>)\n",
      "epoch 3062 loss tensor(6.5819, grad_fn=<NegBackward0>)\n",
      "epoch 3063 loss tensor(6.5817, grad_fn=<NegBackward0>)\n",
      "epoch 3064 loss tensor(6.5816, grad_fn=<NegBackward0>)\n",
      "epoch 3065 loss tensor(6.5814, grad_fn=<NegBackward0>)\n",
      "epoch 3066 loss tensor(6.5812, grad_fn=<NegBackward0>)\n",
      "epoch 3067 loss tensor(6.5811, grad_fn=<NegBackward0>)\n",
      "epoch 3068 loss tensor(6.5809, grad_fn=<NegBackward0>)\n",
      "epoch 3069 loss tensor(6.5807, grad_fn=<NegBackward0>)\n",
      "epoch 3070 loss tensor(6.5806, grad_fn=<NegBackward0>)\n",
      "epoch 3071 loss tensor(6.5804, grad_fn=<NegBackward0>)\n",
      "epoch 3072 loss tensor(6.5803, grad_fn=<NegBackward0>)\n",
      "epoch 3073 loss tensor(6.5801, grad_fn=<NegBackward0>)\n",
      "epoch 3074 loss tensor(6.5799, grad_fn=<NegBackward0>)\n",
      "epoch 3075 loss tensor(6.5798, grad_fn=<NegBackward0>)\n",
      "epoch 3076 loss tensor(6.5796, grad_fn=<NegBackward0>)\n",
      "epoch 3077 loss tensor(6.5795, grad_fn=<NegBackward0>)\n",
      "epoch 3078 loss tensor(6.5793, grad_fn=<NegBackward0>)\n",
      "epoch 3079 loss tensor(6.5791, grad_fn=<NegBackward0>)\n",
      "epoch 3080 loss tensor(6.5790, grad_fn=<NegBackward0>)\n",
      "epoch 3081 loss tensor(6.5788, grad_fn=<NegBackward0>)\n",
      "epoch 3082 loss tensor(6.5787, grad_fn=<NegBackward0>)\n",
      "epoch 3083 loss tensor(6.5785, grad_fn=<NegBackward0>)\n",
      "epoch 3084 loss tensor(6.5783, grad_fn=<NegBackward0>)\n",
      "epoch 3085 loss tensor(6.5782, grad_fn=<NegBackward0>)\n",
      "epoch 3086 loss tensor(6.5780, grad_fn=<NegBackward0>)\n",
      "epoch 3087 loss tensor(6.5779, grad_fn=<NegBackward0>)\n",
      "epoch 3088 loss tensor(6.5777, grad_fn=<NegBackward0>)\n",
      "epoch 3089 loss tensor(6.5775, grad_fn=<NegBackward0>)\n",
      "epoch 3090 loss tensor(6.5774, grad_fn=<NegBackward0>)\n",
      "epoch 3091 loss tensor(6.5772, grad_fn=<NegBackward0>)\n",
      "epoch 3092 loss tensor(6.5771, grad_fn=<NegBackward0>)\n",
      "epoch 3093 loss tensor(6.5769, grad_fn=<NegBackward0>)\n",
      "epoch 3094 loss tensor(6.5767, grad_fn=<NegBackward0>)\n",
      "epoch 3095 loss tensor(6.5766, grad_fn=<NegBackward0>)\n",
      "epoch 3096 loss tensor(6.5764, grad_fn=<NegBackward0>)\n",
      "epoch 3097 loss tensor(6.5763, grad_fn=<NegBackward0>)\n",
      "epoch 3098 loss tensor(6.5761, grad_fn=<NegBackward0>)\n",
      "epoch 3099 loss tensor(6.5759, grad_fn=<NegBackward0>)\n",
      "epoch 3100 loss tensor(6.5758, grad_fn=<NegBackward0>)\n",
      "epoch 3101 loss tensor(6.5756, grad_fn=<NegBackward0>)\n",
      "epoch 3102 loss tensor(6.5755, grad_fn=<NegBackward0>)\n",
      "epoch 3103 loss tensor(6.5753, grad_fn=<NegBackward0>)\n",
      "epoch 3104 loss tensor(6.5751, grad_fn=<NegBackward0>)\n",
      "epoch 3105 loss tensor(6.5750, grad_fn=<NegBackward0>)\n",
      "epoch 3106 loss tensor(6.5748, grad_fn=<NegBackward0>)\n",
      "epoch 3107 loss tensor(6.5747, grad_fn=<NegBackward0>)\n",
      "epoch 3108 loss tensor(6.5745, grad_fn=<NegBackward0>)\n",
      "epoch 3109 loss tensor(6.5744, grad_fn=<NegBackward0>)\n",
      "epoch 3110 loss tensor(6.5742, grad_fn=<NegBackward0>)\n",
      "epoch 3111 loss tensor(6.5740, grad_fn=<NegBackward0>)\n",
      "epoch 3112 loss tensor(6.5739, grad_fn=<NegBackward0>)\n",
      "epoch 3113 loss tensor(6.5737, grad_fn=<NegBackward0>)\n",
      "epoch 3114 loss tensor(6.5736, grad_fn=<NegBackward0>)\n",
      "epoch 3115 loss tensor(6.5734, grad_fn=<NegBackward0>)\n",
      "epoch 3116 loss tensor(6.5732, grad_fn=<NegBackward0>)\n",
      "epoch 3117 loss tensor(6.5731, grad_fn=<NegBackward0>)\n",
      "epoch 3118 loss tensor(6.5729, grad_fn=<NegBackward0>)\n",
      "epoch 3119 loss tensor(6.5728, grad_fn=<NegBackward0>)\n",
      "epoch 3120 loss tensor(6.5726, grad_fn=<NegBackward0>)\n",
      "epoch 3121 loss tensor(6.5724, grad_fn=<NegBackward0>)\n",
      "epoch 3122 loss tensor(6.5723, grad_fn=<NegBackward0>)\n",
      "epoch 3123 loss tensor(6.5721, grad_fn=<NegBackward0>)\n",
      "epoch 3124 loss tensor(6.5720, grad_fn=<NegBackward0>)\n",
      "epoch 3125 loss tensor(6.5718, grad_fn=<NegBackward0>)\n",
      "epoch 3126 loss tensor(6.5717, grad_fn=<NegBackward0>)\n",
      "epoch 3127 loss tensor(6.5715, grad_fn=<NegBackward0>)\n",
      "epoch 3128 loss tensor(6.5713, grad_fn=<NegBackward0>)\n",
      "epoch 3129 loss tensor(6.5712, grad_fn=<NegBackward0>)\n",
      "epoch 3130 loss tensor(6.5710, grad_fn=<NegBackward0>)\n",
      "epoch 3131 loss tensor(6.5709, grad_fn=<NegBackward0>)\n",
      "epoch 3132 loss tensor(6.5707, grad_fn=<NegBackward0>)\n",
      "epoch 3133 loss tensor(6.5706, grad_fn=<NegBackward0>)\n",
      "epoch 3134 loss tensor(6.5704, grad_fn=<NegBackward0>)\n",
      "epoch 3135 loss tensor(6.5702, grad_fn=<NegBackward0>)\n",
      "epoch 3136 loss tensor(6.5701, grad_fn=<NegBackward0>)\n",
      "epoch 3137 loss tensor(6.5699, grad_fn=<NegBackward0>)\n",
      "epoch 3138 loss tensor(6.5698, grad_fn=<NegBackward0>)\n",
      "epoch 3139 loss tensor(6.5696, grad_fn=<NegBackward0>)\n",
      "epoch 3140 loss tensor(6.5694, grad_fn=<NegBackward0>)\n",
      "epoch 3141 loss tensor(6.5693, grad_fn=<NegBackward0>)\n",
      "epoch 3142 loss tensor(6.5691, grad_fn=<NegBackward0>)\n",
      "epoch 3143 loss tensor(6.5690, grad_fn=<NegBackward0>)\n",
      "epoch 3144 loss tensor(6.5688, grad_fn=<NegBackward0>)\n",
      "epoch 3145 loss tensor(6.5687, grad_fn=<NegBackward0>)\n",
      "epoch 3146 loss tensor(6.5685, grad_fn=<NegBackward0>)\n",
      "epoch 3147 loss tensor(6.5683, grad_fn=<NegBackward0>)\n",
      "epoch 3148 loss tensor(6.5682, grad_fn=<NegBackward0>)\n",
      "epoch 3149 loss tensor(6.5680, grad_fn=<NegBackward0>)\n",
      "epoch 3150 loss tensor(6.5679, grad_fn=<NegBackward0>)\n",
      "epoch 3151 loss tensor(6.5677, grad_fn=<NegBackward0>)\n",
      "epoch 3152 loss tensor(6.5676, grad_fn=<NegBackward0>)\n",
      "epoch 3153 loss tensor(6.5674, grad_fn=<NegBackward0>)\n",
      "epoch 3154 loss tensor(6.5672, grad_fn=<NegBackward0>)\n",
      "epoch 3155 loss tensor(6.5671, grad_fn=<NegBackward0>)\n",
      "epoch 3156 loss tensor(6.5669, grad_fn=<NegBackward0>)\n",
      "epoch 3157 loss tensor(6.5668, grad_fn=<NegBackward0>)\n",
      "epoch 3158 loss tensor(6.5666, grad_fn=<NegBackward0>)\n",
      "epoch 3159 loss tensor(6.5665, grad_fn=<NegBackward0>)\n",
      "epoch 3160 loss tensor(6.5663, grad_fn=<NegBackward0>)\n",
      "epoch 3161 loss tensor(6.5662, grad_fn=<NegBackward0>)\n",
      "epoch 3162 loss tensor(6.5660, grad_fn=<NegBackward0>)\n",
      "epoch 3163 loss tensor(6.5658, grad_fn=<NegBackward0>)\n",
      "epoch 3164 loss tensor(6.5657, grad_fn=<NegBackward0>)\n",
      "epoch 3165 loss tensor(6.5655, grad_fn=<NegBackward0>)\n",
      "epoch 3166 loss tensor(6.5654, grad_fn=<NegBackward0>)\n",
      "epoch 3167 loss tensor(6.5652, grad_fn=<NegBackward0>)\n",
      "epoch 3168 loss tensor(6.5651, grad_fn=<NegBackward0>)\n",
      "epoch 3169 loss tensor(6.5649, grad_fn=<NegBackward0>)\n",
      "epoch 3170 loss tensor(6.5647, grad_fn=<NegBackward0>)\n",
      "epoch 3171 loss tensor(6.5646, grad_fn=<NegBackward0>)\n",
      "epoch 3172 loss tensor(6.5644, grad_fn=<NegBackward0>)\n",
      "epoch 3173 loss tensor(6.5643, grad_fn=<NegBackward0>)\n",
      "epoch 3174 loss tensor(6.5641, grad_fn=<NegBackward0>)\n",
      "epoch 3175 loss tensor(6.5640, grad_fn=<NegBackward0>)\n",
      "epoch 3176 loss tensor(6.5638, grad_fn=<NegBackward0>)\n",
      "epoch 3177 loss tensor(6.5637, grad_fn=<NegBackward0>)\n",
      "epoch 3178 loss tensor(6.5635, grad_fn=<NegBackward0>)\n",
      "epoch 3179 loss tensor(6.5633, grad_fn=<NegBackward0>)\n",
      "epoch 3180 loss tensor(6.5632, grad_fn=<NegBackward0>)\n",
      "epoch 3181 loss tensor(6.5630, grad_fn=<NegBackward0>)\n",
      "epoch 3182 loss tensor(6.5629, grad_fn=<NegBackward0>)\n",
      "epoch 3183 loss tensor(6.5627, grad_fn=<NegBackward0>)\n",
      "epoch 3184 loss tensor(6.5626, grad_fn=<NegBackward0>)\n",
      "epoch 3185 loss tensor(6.5624, grad_fn=<NegBackward0>)\n",
      "epoch 3186 loss tensor(6.5623, grad_fn=<NegBackward0>)\n",
      "epoch 3187 loss tensor(6.5621, grad_fn=<NegBackward0>)\n",
      "epoch 3188 loss tensor(6.5619, grad_fn=<NegBackward0>)\n",
      "epoch 3189 loss tensor(6.5618, grad_fn=<NegBackward0>)\n",
      "epoch 3190 loss tensor(6.5616, grad_fn=<NegBackward0>)\n",
      "epoch 3191 loss tensor(6.5615, grad_fn=<NegBackward0>)\n",
      "epoch 3192 loss tensor(6.5613, grad_fn=<NegBackward0>)\n",
      "epoch 3193 loss tensor(6.5612, grad_fn=<NegBackward0>)\n",
      "epoch 3194 loss tensor(6.5610, grad_fn=<NegBackward0>)\n",
      "epoch 3195 loss tensor(6.5609, grad_fn=<NegBackward0>)\n",
      "epoch 3196 loss tensor(6.5607, grad_fn=<NegBackward0>)\n",
      "epoch 3197 loss tensor(6.5605, grad_fn=<NegBackward0>)\n",
      "epoch 3198 loss tensor(6.5604, grad_fn=<NegBackward0>)\n",
      "epoch 3199 loss tensor(6.5602, grad_fn=<NegBackward0>)\n",
      "epoch 3200 loss tensor(6.5601, grad_fn=<NegBackward0>)\n",
      "epoch 3201 loss tensor(6.5599, grad_fn=<NegBackward0>)\n",
      "epoch 3202 loss tensor(6.5598, grad_fn=<NegBackward0>)\n",
      "epoch 3203 loss tensor(6.5596, grad_fn=<NegBackward0>)\n",
      "epoch 3204 loss tensor(6.5595, grad_fn=<NegBackward0>)\n",
      "epoch 3205 loss tensor(6.5593, grad_fn=<NegBackward0>)\n",
      "epoch 3206 loss tensor(6.5592, grad_fn=<NegBackward0>)\n",
      "epoch 3207 loss tensor(6.5590, grad_fn=<NegBackward0>)\n",
      "epoch 3208 loss tensor(6.5588, grad_fn=<NegBackward0>)\n",
      "epoch 3209 loss tensor(6.5587, grad_fn=<NegBackward0>)\n",
      "epoch 3210 loss tensor(6.5585, grad_fn=<NegBackward0>)\n",
      "epoch 3211 loss tensor(6.5584, grad_fn=<NegBackward0>)\n",
      "epoch 3212 loss tensor(6.5582, grad_fn=<NegBackward0>)\n",
      "epoch 3213 loss tensor(6.5581, grad_fn=<NegBackward0>)\n",
      "epoch 3214 loss tensor(6.5579, grad_fn=<NegBackward0>)\n",
      "epoch 3215 loss tensor(6.5578, grad_fn=<NegBackward0>)\n",
      "epoch 3216 loss tensor(6.5576, grad_fn=<NegBackward0>)\n",
      "epoch 3217 loss tensor(6.5575, grad_fn=<NegBackward0>)\n",
      "epoch 3218 loss tensor(6.5573, grad_fn=<NegBackward0>)\n",
      "epoch 3219 loss tensor(6.5572, grad_fn=<NegBackward0>)\n",
      "epoch 3220 loss tensor(6.5570, grad_fn=<NegBackward0>)\n",
      "epoch 3221 loss tensor(6.5568, grad_fn=<NegBackward0>)\n",
      "epoch 3222 loss tensor(6.5567, grad_fn=<NegBackward0>)\n",
      "epoch 3223 loss tensor(6.5565, grad_fn=<NegBackward0>)\n",
      "epoch 3224 loss tensor(6.5564, grad_fn=<NegBackward0>)\n",
      "epoch 3225 loss tensor(6.5562, grad_fn=<NegBackward0>)\n",
      "epoch 3226 loss tensor(6.5561, grad_fn=<NegBackward0>)\n",
      "epoch 3227 loss tensor(6.5559, grad_fn=<NegBackward0>)\n",
      "epoch 3228 loss tensor(6.5558, grad_fn=<NegBackward0>)\n",
      "epoch 3229 loss tensor(6.5556, grad_fn=<NegBackward0>)\n",
      "epoch 3230 loss tensor(6.5555, grad_fn=<NegBackward0>)\n",
      "epoch 3231 loss tensor(6.5553, grad_fn=<NegBackward0>)\n",
      "epoch 3232 loss tensor(6.5552, grad_fn=<NegBackward0>)\n",
      "epoch 3233 loss tensor(6.5550, grad_fn=<NegBackward0>)\n",
      "epoch 3234 loss tensor(6.5549, grad_fn=<NegBackward0>)\n",
      "epoch 3235 loss tensor(6.5547, grad_fn=<NegBackward0>)\n",
      "epoch 3236 loss tensor(6.5545, grad_fn=<NegBackward0>)\n",
      "epoch 3237 loss tensor(6.5544, grad_fn=<NegBackward0>)\n",
      "epoch 3238 loss tensor(6.5542, grad_fn=<NegBackward0>)\n",
      "epoch 3239 loss tensor(6.5541, grad_fn=<NegBackward0>)\n",
      "epoch 3240 loss tensor(6.5539, grad_fn=<NegBackward0>)\n",
      "epoch 3241 loss tensor(6.5538, grad_fn=<NegBackward0>)\n",
      "epoch 3242 loss tensor(6.5536, grad_fn=<NegBackward0>)\n",
      "epoch 3243 loss tensor(6.5535, grad_fn=<NegBackward0>)\n",
      "epoch 3244 loss tensor(6.5533, grad_fn=<NegBackward0>)\n",
      "epoch 3245 loss tensor(6.5532, grad_fn=<NegBackward0>)\n",
      "epoch 3246 loss tensor(6.5530, grad_fn=<NegBackward0>)\n",
      "epoch 3247 loss tensor(6.5529, grad_fn=<NegBackward0>)\n",
      "epoch 3248 loss tensor(6.5527, grad_fn=<NegBackward0>)\n",
      "epoch 3249 loss tensor(6.5526, grad_fn=<NegBackward0>)\n",
      "epoch 3250 loss tensor(6.5524, grad_fn=<NegBackward0>)\n",
      "epoch 3251 loss tensor(6.5523, grad_fn=<NegBackward0>)\n",
      "epoch 3252 loss tensor(6.5521, grad_fn=<NegBackward0>)\n",
      "epoch 3253 loss tensor(6.5520, grad_fn=<NegBackward0>)\n",
      "epoch 3254 loss tensor(6.5518, grad_fn=<NegBackward0>)\n",
      "epoch 3255 loss tensor(6.5516, grad_fn=<NegBackward0>)\n",
      "epoch 3256 loss tensor(6.5515, grad_fn=<NegBackward0>)\n",
      "epoch 3257 loss tensor(6.5513, grad_fn=<NegBackward0>)\n",
      "epoch 3258 loss tensor(6.5512, grad_fn=<NegBackward0>)\n",
      "epoch 3259 loss tensor(6.5510, grad_fn=<NegBackward0>)\n",
      "epoch 3260 loss tensor(6.5509, grad_fn=<NegBackward0>)\n",
      "epoch 3261 loss tensor(6.5507, grad_fn=<NegBackward0>)\n",
      "epoch 3262 loss tensor(6.5506, grad_fn=<NegBackward0>)\n",
      "epoch 3263 loss tensor(6.5504, grad_fn=<NegBackward0>)\n",
      "epoch 3264 loss tensor(6.5503, grad_fn=<NegBackward0>)\n",
      "epoch 3265 loss tensor(6.5501, grad_fn=<NegBackward0>)\n",
      "epoch 3266 loss tensor(6.5500, grad_fn=<NegBackward0>)\n",
      "epoch 3267 loss tensor(6.5498, grad_fn=<NegBackward0>)\n",
      "epoch 3268 loss tensor(6.5497, grad_fn=<NegBackward0>)\n",
      "epoch 3269 loss tensor(6.5495, grad_fn=<NegBackward0>)\n",
      "epoch 3270 loss tensor(6.5494, grad_fn=<NegBackward0>)\n",
      "epoch 3271 loss tensor(6.5492, grad_fn=<NegBackward0>)\n",
      "epoch 3272 loss tensor(6.5491, grad_fn=<NegBackward0>)\n",
      "epoch 3273 loss tensor(6.5489, grad_fn=<NegBackward0>)\n",
      "epoch 3274 loss tensor(6.5488, grad_fn=<NegBackward0>)\n",
      "epoch 3275 loss tensor(6.5486, grad_fn=<NegBackward0>)\n",
      "epoch 3276 loss tensor(6.5485, grad_fn=<NegBackward0>)\n",
      "epoch 3277 loss tensor(6.5483, grad_fn=<NegBackward0>)\n",
      "epoch 3278 loss tensor(6.5482, grad_fn=<NegBackward0>)\n",
      "epoch 3279 loss tensor(6.5480, grad_fn=<NegBackward0>)\n",
      "epoch 3280 loss tensor(6.5479, grad_fn=<NegBackward0>)\n",
      "epoch 3281 loss tensor(6.5477, grad_fn=<NegBackward0>)\n",
      "epoch 3282 loss tensor(6.5476, grad_fn=<NegBackward0>)\n",
      "epoch 3283 loss tensor(6.5474, grad_fn=<NegBackward0>)\n",
      "epoch 3284 loss tensor(6.5473, grad_fn=<NegBackward0>)\n",
      "epoch 3285 loss tensor(6.5471, grad_fn=<NegBackward0>)\n",
      "epoch 3286 loss tensor(6.5470, grad_fn=<NegBackward0>)\n",
      "epoch 3287 loss tensor(6.5468, grad_fn=<NegBackward0>)\n",
      "epoch 3288 loss tensor(6.5467, grad_fn=<NegBackward0>)\n",
      "epoch 3289 loss tensor(6.5465, grad_fn=<NegBackward0>)\n",
      "epoch 3290 loss tensor(6.5464, grad_fn=<NegBackward0>)\n",
      "epoch 3291 loss tensor(6.5462, grad_fn=<NegBackward0>)\n",
      "epoch 3292 loss tensor(6.5461, grad_fn=<NegBackward0>)\n",
      "epoch 3293 loss tensor(6.5459, grad_fn=<NegBackward0>)\n",
      "epoch 3294 loss tensor(6.5458, grad_fn=<NegBackward0>)\n",
      "epoch 3295 loss tensor(6.5456, grad_fn=<NegBackward0>)\n",
      "epoch 3296 loss tensor(6.5455, grad_fn=<NegBackward0>)\n",
      "epoch 3297 loss tensor(6.5453, grad_fn=<NegBackward0>)\n",
      "epoch 3298 loss tensor(6.5451, grad_fn=<NegBackward0>)\n",
      "epoch 3299 loss tensor(6.5450, grad_fn=<NegBackward0>)\n",
      "epoch 3300 loss tensor(6.5448, grad_fn=<NegBackward0>)\n",
      "epoch 3301 loss tensor(6.5447, grad_fn=<NegBackward0>)\n",
      "epoch 3302 loss tensor(6.5445, grad_fn=<NegBackward0>)\n",
      "epoch 3303 loss tensor(6.5444, grad_fn=<NegBackward0>)\n",
      "epoch 3304 loss tensor(6.5443, grad_fn=<NegBackward0>)\n",
      "epoch 3305 loss tensor(6.5441, grad_fn=<NegBackward0>)\n",
      "epoch 3306 loss tensor(6.5440, grad_fn=<NegBackward0>)\n",
      "epoch 3307 loss tensor(6.5438, grad_fn=<NegBackward0>)\n",
      "epoch 3308 loss tensor(6.5437, grad_fn=<NegBackward0>)\n",
      "epoch 3309 loss tensor(6.5435, grad_fn=<NegBackward0>)\n",
      "epoch 3310 loss tensor(6.5434, grad_fn=<NegBackward0>)\n",
      "epoch 3311 loss tensor(6.5432, grad_fn=<NegBackward0>)\n",
      "epoch 3312 loss tensor(6.5431, grad_fn=<NegBackward0>)\n",
      "epoch 3313 loss tensor(6.5429, grad_fn=<NegBackward0>)\n",
      "epoch 3314 loss tensor(6.5428, grad_fn=<NegBackward0>)\n",
      "epoch 3315 loss tensor(6.5426, grad_fn=<NegBackward0>)\n",
      "epoch 3316 loss tensor(6.5425, grad_fn=<NegBackward0>)\n",
      "epoch 3317 loss tensor(6.5423, grad_fn=<NegBackward0>)\n",
      "epoch 3318 loss tensor(6.5422, grad_fn=<NegBackward0>)\n",
      "epoch 3319 loss tensor(6.5420, grad_fn=<NegBackward0>)\n",
      "epoch 3320 loss tensor(6.5419, grad_fn=<NegBackward0>)\n",
      "epoch 3321 loss tensor(6.5417, grad_fn=<NegBackward0>)\n",
      "epoch 3322 loss tensor(6.5416, grad_fn=<NegBackward0>)\n",
      "epoch 3323 loss tensor(6.5414, grad_fn=<NegBackward0>)\n",
      "epoch 3324 loss tensor(6.5413, grad_fn=<NegBackward0>)\n",
      "epoch 3325 loss tensor(6.5411, grad_fn=<NegBackward0>)\n",
      "epoch 3326 loss tensor(6.5410, grad_fn=<NegBackward0>)\n",
      "epoch 3327 loss tensor(6.5408, grad_fn=<NegBackward0>)\n",
      "epoch 3328 loss tensor(6.5407, grad_fn=<NegBackward0>)\n",
      "epoch 3329 loss tensor(6.5405, grad_fn=<NegBackward0>)\n",
      "epoch 3330 loss tensor(6.5404, grad_fn=<NegBackward0>)\n",
      "epoch 3331 loss tensor(6.5402, grad_fn=<NegBackward0>)\n",
      "epoch 3332 loss tensor(6.5401, grad_fn=<NegBackward0>)\n",
      "epoch 3333 loss tensor(6.5399, grad_fn=<NegBackward0>)\n",
      "epoch 3334 loss tensor(6.5398, grad_fn=<NegBackward0>)\n",
      "epoch 3335 loss tensor(6.5396, grad_fn=<NegBackward0>)\n",
      "epoch 3336 loss tensor(6.5395, grad_fn=<NegBackward0>)\n",
      "epoch 3337 loss tensor(6.5393, grad_fn=<NegBackward0>)\n",
      "epoch 3338 loss tensor(6.5392, grad_fn=<NegBackward0>)\n",
      "epoch 3339 loss tensor(6.5390, grad_fn=<NegBackward0>)\n",
      "epoch 3340 loss tensor(6.5389, grad_fn=<NegBackward0>)\n",
      "epoch 3341 loss tensor(6.5387, grad_fn=<NegBackward0>)\n",
      "epoch 3342 loss tensor(6.5386, grad_fn=<NegBackward0>)\n",
      "epoch 3343 loss tensor(6.5384, grad_fn=<NegBackward0>)\n",
      "epoch 3344 loss tensor(6.5383, grad_fn=<NegBackward0>)\n",
      "epoch 3345 loss tensor(6.5381, grad_fn=<NegBackward0>)\n",
      "epoch 3346 loss tensor(6.5380, grad_fn=<NegBackward0>)\n",
      "epoch 3347 loss tensor(6.5378, grad_fn=<NegBackward0>)\n",
      "epoch 3348 loss tensor(6.5377, grad_fn=<NegBackward0>)\n",
      "epoch 3349 loss tensor(6.5376, grad_fn=<NegBackward0>)\n",
      "epoch 3350 loss tensor(6.5374, grad_fn=<NegBackward0>)\n",
      "epoch 3351 loss tensor(6.5373, grad_fn=<NegBackward0>)\n",
      "epoch 3352 loss tensor(6.5371, grad_fn=<NegBackward0>)\n",
      "epoch 3353 loss tensor(6.5370, grad_fn=<NegBackward0>)\n",
      "epoch 3354 loss tensor(6.5368, grad_fn=<NegBackward0>)\n",
      "epoch 3355 loss tensor(6.5367, grad_fn=<NegBackward0>)\n",
      "epoch 3356 loss tensor(6.5365, grad_fn=<NegBackward0>)\n",
      "epoch 3357 loss tensor(6.5364, grad_fn=<NegBackward0>)\n",
      "epoch 3358 loss tensor(6.5362, grad_fn=<NegBackward0>)\n",
      "epoch 3359 loss tensor(6.5361, grad_fn=<NegBackward0>)\n",
      "epoch 3360 loss tensor(6.5359, grad_fn=<NegBackward0>)\n",
      "epoch 3361 loss tensor(6.5358, grad_fn=<NegBackward0>)\n",
      "epoch 3362 loss tensor(6.5356, grad_fn=<NegBackward0>)\n",
      "epoch 3363 loss tensor(6.5355, grad_fn=<NegBackward0>)\n",
      "epoch 3364 loss tensor(6.5353, grad_fn=<NegBackward0>)\n",
      "epoch 3365 loss tensor(6.5352, grad_fn=<NegBackward0>)\n",
      "epoch 3366 loss tensor(6.5350, grad_fn=<NegBackward0>)\n",
      "epoch 3367 loss tensor(6.5349, grad_fn=<NegBackward0>)\n",
      "epoch 3368 loss tensor(6.5348, grad_fn=<NegBackward0>)\n",
      "epoch 3369 loss tensor(6.5346, grad_fn=<NegBackward0>)\n",
      "epoch 3370 loss tensor(6.5345, grad_fn=<NegBackward0>)\n",
      "epoch 3371 loss tensor(6.5343, grad_fn=<NegBackward0>)\n",
      "epoch 3372 loss tensor(6.5342, grad_fn=<NegBackward0>)\n",
      "epoch 3373 loss tensor(6.5340, grad_fn=<NegBackward0>)\n",
      "epoch 3374 loss tensor(6.5339, grad_fn=<NegBackward0>)\n",
      "epoch 3375 loss tensor(6.5337, grad_fn=<NegBackward0>)\n",
      "epoch 3376 loss tensor(6.5336, grad_fn=<NegBackward0>)\n",
      "epoch 3377 loss tensor(6.5334, grad_fn=<NegBackward0>)\n",
      "epoch 3378 loss tensor(6.5333, grad_fn=<NegBackward0>)\n",
      "epoch 3379 loss tensor(6.5331, grad_fn=<NegBackward0>)\n",
      "epoch 3380 loss tensor(6.5330, grad_fn=<NegBackward0>)\n",
      "epoch 3381 loss tensor(6.5329, grad_fn=<NegBackward0>)\n",
      "epoch 3382 loss tensor(6.5327, grad_fn=<NegBackward0>)\n",
      "epoch 3383 loss tensor(6.5326, grad_fn=<NegBackward0>)\n",
      "epoch 3384 loss tensor(6.5324, grad_fn=<NegBackward0>)\n",
      "epoch 3385 loss tensor(6.5323, grad_fn=<NegBackward0>)\n",
      "epoch 3386 loss tensor(6.5321, grad_fn=<NegBackward0>)\n",
      "epoch 3387 loss tensor(6.5320, grad_fn=<NegBackward0>)\n",
      "epoch 3388 loss tensor(6.5318, grad_fn=<NegBackward0>)\n",
      "epoch 3389 loss tensor(6.5317, grad_fn=<NegBackward0>)\n",
      "epoch 3390 loss tensor(6.5315, grad_fn=<NegBackward0>)\n",
      "epoch 3391 loss tensor(6.5314, grad_fn=<NegBackward0>)\n",
      "epoch 3392 loss tensor(6.5312, grad_fn=<NegBackward0>)\n",
      "epoch 3393 loss tensor(6.5311, grad_fn=<NegBackward0>)\n",
      "epoch 3394 loss tensor(6.5310, grad_fn=<NegBackward0>)\n",
      "epoch 3395 loss tensor(6.5308, grad_fn=<NegBackward0>)\n",
      "epoch 3396 loss tensor(6.5307, grad_fn=<NegBackward0>)\n",
      "epoch 3397 loss tensor(6.5305, grad_fn=<NegBackward0>)\n",
      "epoch 3398 loss tensor(6.5304, grad_fn=<NegBackward0>)\n",
      "epoch 3399 loss tensor(6.5302, grad_fn=<NegBackward0>)\n",
      "epoch 3400 loss tensor(6.5301, grad_fn=<NegBackward0>)\n",
      "epoch 3401 loss tensor(6.5299, grad_fn=<NegBackward0>)\n",
      "epoch 3402 loss tensor(6.5298, grad_fn=<NegBackward0>)\n",
      "epoch 3403 loss tensor(6.5296, grad_fn=<NegBackward0>)\n",
      "epoch 3404 loss tensor(6.5295, grad_fn=<NegBackward0>)\n",
      "epoch 3405 loss tensor(6.5294, grad_fn=<NegBackward0>)\n",
      "epoch 3406 loss tensor(6.5292, grad_fn=<NegBackward0>)\n",
      "epoch 3407 loss tensor(6.5291, grad_fn=<NegBackward0>)\n",
      "epoch 3408 loss tensor(6.5289, grad_fn=<NegBackward0>)\n",
      "epoch 3409 loss tensor(6.5288, grad_fn=<NegBackward0>)\n",
      "epoch 3410 loss tensor(6.5286, grad_fn=<NegBackward0>)\n",
      "epoch 3411 loss tensor(6.5285, grad_fn=<NegBackward0>)\n",
      "epoch 3412 loss tensor(6.5283, grad_fn=<NegBackward0>)\n",
      "epoch 3413 loss tensor(6.5282, grad_fn=<NegBackward0>)\n",
      "epoch 3414 loss tensor(6.5280, grad_fn=<NegBackward0>)\n",
      "epoch 3415 loss tensor(6.5279, grad_fn=<NegBackward0>)\n",
      "epoch 3416 loss tensor(6.5278, grad_fn=<NegBackward0>)\n",
      "epoch 3417 loss tensor(6.5276, grad_fn=<NegBackward0>)\n",
      "epoch 3418 loss tensor(6.5275, grad_fn=<NegBackward0>)\n",
      "epoch 3419 loss tensor(6.5273, grad_fn=<NegBackward0>)\n",
      "epoch 3420 loss tensor(6.5272, grad_fn=<NegBackward0>)\n",
      "epoch 3421 loss tensor(6.5270, grad_fn=<NegBackward0>)\n",
      "epoch 3422 loss tensor(6.5269, grad_fn=<NegBackward0>)\n",
      "epoch 3423 loss tensor(6.5267, grad_fn=<NegBackward0>)\n",
      "epoch 3424 loss tensor(6.5266, grad_fn=<NegBackward0>)\n",
      "epoch 3425 loss tensor(6.5265, grad_fn=<NegBackward0>)\n",
      "epoch 3426 loss tensor(6.5263, grad_fn=<NegBackward0>)\n",
      "epoch 3427 loss tensor(6.5262, grad_fn=<NegBackward0>)\n",
      "epoch 3428 loss tensor(6.5260, grad_fn=<NegBackward0>)\n",
      "epoch 3429 loss tensor(6.5259, grad_fn=<NegBackward0>)\n",
      "epoch 3430 loss tensor(6.5257, grad_fn=<NegBackward0>)\n",
      "epoch 3431 loss tensor(6.5256, grad_fn=<NegBackward0>)\n",
      "epoch 3432 loss tensor(6.5254, grad_fn=<NegBackward0>)\n",
      "epoch 3433 loss tensor(6.5253, grad_fn=<NegBackward0>)\n",
      "epoch 3434 loss tensor(6.5252, grad_fn=<NegBackward0>)\n",
      "epoch 3435 loss tensor(6.5250, grad_fn=<NegBackward0>)\n",
      "epoch 3436 loss tensor(6.5249, grad_fn=<NegBackward0>)\n",
      "epoch 3437 loss tensor(6.5247, grad_fn=<NegBackward0>)\n",
      "epoch 3438 loss tensor(6.5246, grad_fn=<NegBackward0>)\n",
      "epoch 3439 loss tensor(6.5244, grad_fn=<NegBackward0>)\n",
      "epoch 3440 loss tensor(6.5243, grad_fn=<NegBackward0>)\n",
      "epoch 3441 loss tensor(6.5242, grad_fn=<NegBackward0>)\n",
      "epoch 3442 loss tensor(6.5240, grad_fn=<NegBackward0>)\n",
      "epoch 3443 loss tensor(6.5239, grad_fn=<NegBackward0>)\n",
      "epoch 3444 loss tensor(6.5237, grad_fn=<NegBackward0>)\n",
      "epoch 3445 loss tensor(6.5236, grad_fn=<NegBackward0>)\n",
      "epoch 3446 loss tensor(6.5234, grad_fn=<NegBackward0>)\n",
      "epoch 3447 loss tensor(6.5233, grad_fn=<NegBackward0>)\n",
      "epoch 3448 loss tensor(6.5231, grad_fn=<NegBackward0>)\n",
      "epoch 3449 loss tensor(6.5230, grad_fn=<NegBackward0>)\n",
      "epoch 3450 loss tensor(6.5229, grad_fn=<NegBackward0>)\n",
      "epoch 3451 loss tensor(6.5227, grad_fn=<NegBackward0>)\n",
      "epoch 3452 loss tensor(6.5226, grad_fn=<NegBackward0>)\n",
      "epoch 3453 loss tensor(6.5224, grad_fn=<NegBackward0>)\n",
      "epoch 3454 loss tensor(6.5223, grad_fn=<NegBackward0>)\n",
      "epoch 3455 loss tensor(6.5221, grad_fn=<NegBackward0>)\n",
      "epoch 3456 loss tensor(6.5220, grad_fn=<NegBackward0>)\n",
      "epoch 3457 loss tensor(6.5219, grad_fn=<NegBackward0>)\n",
      "epoch 3458 loss tensor(6.5217, grad_fn=<NegBackward0>)\n",
      "epoch 3459 loss tensor(6.5216, grad_fn=<NegBackward0>)\n",
      "epoch 3460 loss tensor(6.5214, grad_fn=<NegBackward0>)\n",
      "epoch 3461 loss tensor(6.5213, grad_fn=<NegBackward0>)\n",
      "epoch 3462 loss tensor(6.5211, grad_fn=<NegBackward0>)\n",
      "epoch 3463 loss tensor(6.5210, grad_fn=<NegBackward0>)\n",
      "epoch 3464 loss tensor(6.5209, grad_fn=<NegBackward0>)\n",
      "epoch 3465 loss tensor(6.5207, grad_fn=<NegBackward0>)\n",
      "epoch 3466 loss tensor(6.5206, grad_fn=<NegBackward0>)\n",
      "epoch 3467 loss tensor(6.5204, grad_fn=<NegBackward0>)\n",
      "epoch 3468 loss tensor(6.5203, grad_fn=<NegBackward0>)\n",
      "epoch 3469 loss tensor(6.5202, grad_fn=<NegBackward0>)\n",
      "epoch 3470 loss tensor(6.5200, grad_fn=<NegBackward0>)\n",
      "epoch 3471 loss tensor(6.5199, grad_fn=<NegBackward0>)\n",
      "epoch 3472 loss tensor(6.5197, grad_fn=<NegBackward0>)\n",
      "epoch 3473 loss tensor(6.5196, grad_fn=<NegBackward0>)\n",
      "epoch 3474 loss tensor(6.5194, grad_fn=<NegBackward0>)\n",
      "epoch 3475 loss tensor(6.5193, grad_fn=<NegBackward0>)\n",
      "epoch 3476 loss tensor(6.5192, grad_fn=<NegBackward0>)\n",
      "epoch 3477 loss tensor(6.5190, grad_fn=<NegBackward0>)\n",
      "epoch 3478 loss tensor(6.5189, grad_fn=<NegBackward0>)\n",
      "epoch 3479 loss tensor(6.5187, grad_fn=<NegBackward0>)\n",
      "epoch 3480 loss tensor(6.5186, grad_fn=<NegBackward0>)\n",
      "epoch 3481 loss tensor(6.5184, grad_fn=<NegBackward0>)\n",
      "epoch 3482 loss tensor(6.5183, grad_fn=<NegBackward0>)\n",
      "epoch 3483 loss tensor(6.5182, grad_fn=<NegBackward0>)\n",
      "epoch 3484 loss tensor(6.5180, grad_fn=<NegBackward0>)\n",
      "epoch 3485 loss tensor(6.5179, grad_fn=<NegBackward0>)\n",
      "epoch 3486 loss tensor(6.5177, grad_fn=<NegBackward0>)\n",
      "epoch 3487 loss tensor(6.5176, grad_fn=<NegBackward0>)\n",
      "epoch 3488 loss tensor(6.5175, grad_fn=<NegBackward0>)\n",
      "epoch 3489 loss tensor(6.5173, grad_fn=<NegBackward0>)\n",
      "epoch 3490 loss tensor(6.5172, grad_fn=<NegBackward0>)\n",
      "epoch 3491 loss tensor(6.5170, grad_fn=<NegBackward0>)\n",
      "epoch 3492 loss tensor(6.5169, grad_fn=<NegBackward0>)\n",
      "epoch 3493 loss tensor(6.5167, grad_fn=<NegBackward0>)\n",
      "epoch 3494 loss tensor(6.5166, grad_fn=<NegBackward0>)\n",
      "epoch 3495 loss tensor(6.5165, grad_fn=<NegBackward0>)\n",
      "epoch 3496 loss tensor(6.5163, grad_fn=<NegBackward0>)\n",
      "epoch 3497 loss tensor(6.5162, grad_fn=<NegBackward0>)\n",
      "epoch 3498 loss tensor(6.5160, grad_fn=<NegBackward0>)\n",
      "epoch 3499 loss tensor(6.5159, grad_fn=<NegBackward0>)\n",
      "epoch 3500 loss tensor(6.5158, grad_fn=<NegBackward0>)\n",
      "epoch 3501 loss tensor(6.5156, grad_fn=<NegBackward0>)\n",
      "epoch 3502 loss tensor(6.5155, grad_fn=<NegBackward0>)\n",
      "epoch 3503 loss tensor(6.5153, grad_fn=<NegBackward0>)\n",
      "epoch 3504 loss tensor(6.5152, grad_fn=<NegBackward0>)\n",
      "epoch 3505 loss tensor(6.5151, grad_fn=<NegBackward0>)\n",
      "epoch 3506 loss tensor(6.5149, grad_fn=<NegBackward0>)\n",
      "epoch 3507 loss tensor(6.5148, grad_fn=<NegBackward0>)\n",
      "epoch 3508 loss tensor(6.5146, grad_fn=<NegBackward0>)\n",
      "epoch 3509 loss tensor(6.5145, grad_fn=<NegBackward0>)\n",
      "epoch 3510 loss tensor(6.5144, grad_fn=<NegBackward0>)\n",
      "epoch 3511 loss tensor(6.5142, grad_fn=<NegBackward0>)\n",
      "epoch 3512 loss tensor(6.5141, grad_fn=<NegBackward0>)\n",
      "epoch 3513 loss tensor(6.5139, grad_fn=<NegBackward0>)\n",
      "epoch 3514 loss tensor(6.5138, grad_fn=<NegBackward0>)\n",
      "epoch 3515 loss tensor(6.5136, grad_fn=<NegBackward0>)\n",
      "epoch 3516 loss tensor(6.5135, grad_fn=<NegBackward0>)\n",
      "epoch 3517 loss tensor(6.5134, grad_fn=<NegBackward0>)\n",
      "epoch 3518 loss tensor(6.5132, grad_fn=<NegBackward0>)\n",
      "epoch 3519 loss tensor(6.5131, grad_fn=<NegBackward0>)\n",
      "epoch 3520 loss tensor(6.5129, grad_fn=<NegBackward0>)\n",
      "epoch 3521 loss tensor(6.5128, grad_fn=<NegBackward0>)\n",
      "epoch 3522 loss tensor(6.5127, grad_fn=<NegBackward0>)\n",
      "epoch 3523 loss tensor(6.5125, grad_fn=<NegBackward0>)\n",
      "epoch 3524 loss tensor(6.5124, grad_fn=<NegBackward0>)\n",
      "epoch 3525 loss tensor(6.5122, grad_fn=<NegBackward0>)\n",
      "epoch 3526 loss tensor(6.5121, grad_fn=<NegBackward0>)\n",
      "epoch 3527 loss tensor(6.5120, grad_fn=<NegBackward0>)\n",
      "epoch 3528 loss tensor(6.5118, grad_fn=<NegBackward0>)\n",
      "epoch 3529 loss tensor(6.5117, grad_fn=<NegBackward0>)\n",
      "epoch 3530 loss tensor(6.5115, grad_fn=<NegBackward0>)\n",
      "epoch 3531 loss tensor(6.5114, grad_fn=<NegBackward0>)\n",
      "epoch 3532 loss tensor(6.5113, grad_fn=<NegBackward0>)\n",
      "epoch 3533 loss tensor(6.5111, grad_fn=<NegBackward0>)\n",
      "epoch 3534 loss tensor(6.5110, grad_fn=<NegBackward0>)\n",
      "epoch 3535 loss tensor(6.5108, grad_fn=<NegBackward0>)\n",
      "epoch 3536 loss tensor(6.5107, grad_fn=<NegBackward0>)\n",
      "epoch 3537 loss tensor(6.5106, grad_fn=<NegBackward0>)\n",
      "epoch 3538 loss tensor(6.5104, grad_fn=<NegBackward0>)\n",
      "epoch 3539 loss tensor(6.5103, grad_fn=<NegBackward0>)\n",
      "epoch 3540 loss tensor(6.5102, grad_fn=<NegBackward0>)\n",
      "epoch 3541 loss tensor(6.5100, grad_fn=<NegBackward0>)\n",
      "epoch 3542 loss tensor(6.5099, grad_fn=<NegBackward0>)\n",
      "epoch 3543 loss tensor(6.5097, grad_fn=<NegBackward0>)\n",
      "epoch 3544 loss tensor(6.5096, grad_fn=<NegBackward0>)\n",
      "epoch 3545 loss tensor(6.5095, grad_fn=<NegBackward0>)\n",
      "epoch 3546 loss tensor(6.5093, grad_fn=<NegBackward0>)\n",
      "epoch 3547 loss tensor(6.5092, grad_fn=<NegBackward0>)\n",
      "epoch 3548 loss tensor(6.5090, grad_fn=<NegBackward0>)\n",
      "epoch 3549 loss tensor(6.5089, grad_fn=<NegBackward0>)\n",
      "epoch 3550 loss tensor(6.5088, grad_fn=<NegBackward0>)\n",
      "epoch 3551 loss tensor(6.5086, grad_fn=<NegBackward0>)\n",
      "epoch 3552 loss tensor(6.5085, grad_fn=<NegBackward0>)\n",
      "epoch 3553 loss tensor(6.5083, grad_fn=<NegBackward0>)\n",
      "epoch 3554 loss tensor(6.5082, grad_fn=<NegBackward0>)\n",
      "epoch 3555 loss tensor(6.5081, grad_fn=<NegBackward0>)\n",
      "epoch 3556 loss tensor(6.5079, grad_fn=<NegBackward0>)\n",
      "epoch 3557 loss tensor(6.5078, grad_fn=<NegBackward0>)\n",
      "epoch 3558 loss tensor(6.5077, grad_fn=<NegBackward0>)\n",
      "epoch 3559 loss tensor(6.5075, grad_fn=<NegBackward0>)\n",
      "epoch 3560 loss tensor(6.5074, grad_fn=<NegBackward0>)\n",
      "epoch 3561 loss tensor(6.5072, grad_fn=<NegBackward0>)\n",
      "epoch 3562 loss tensor(6.5071, grad_fn=<NegBackward0>)\n",
      "epoch 3563 loss tensor(6.5070, grad_fn=<NegBackward0>)\n",
      "epoch 3564 loss tensor(6.5068, grad_fn=<NegBackward0>)\n",
      "epoch 3565 loss tensor(6.5067, grad_fn=<NegBackward0>)\n",
      "epoch 3566 loss tensor(6.5065, grad_fn=<NegBackward0>)\n",
      "epoch 3567 loss tensor(6.5064, grad_fn=<NegBackward0>)\n",
      "epoch 3568 loss tensor(6.5063, grad_fn=<NegBackward0>)\n",
      "epoch 3569 loss tensor(6.5061, grad_fn=<NegBackward0>)\n",
      "epoch 3570 loss tensor(6.5060, grad_fn=<NegBackward0>)\n",
      "epoch 3571 loss tensor(6.5059, grad_fn=<NegBackward0>)\n",
      "epoch 3572 loss tensor(6.5057, grad_fn=<NegBackward0>)\n",
      "epoch 3573 loss tensor(6.5056, grad_fn=<NegBackward0>)\n",
      "epoch 3574 loss tensor(6.5054, grad_fn=<NegBackward0>)\n",
      "epoch 3575 loss tensor(6.5053, grad_fn=<NegBackward0>)\n",
      "epoch 3576 loss tensor(6.5052, grad_fn=<NegBackward0>)\n",
      "epoch 3577 loss tensor(6.5050, grad_fn=<NegBackward0>)\n",
      "epoch 3578 loss tensor(6.5049, grad_fn=<NegBackward0>)\n",
      "epoch 3579 loss tensor(6.5048, grad_fn=<NegBackward0>)\n",
      "epoch 3580 loss tensor(6.5046, grad_fn=<NegBackward0>)\n",
      "epoch 3581 loss tensor(6.5045, grad_fn=<NegBackward0>)\n",
      "epoch 3582 loss tensor(6.5043, grad_fn=<NegBackward0>)\n",
      "epoch 3583 loss tensor(6.5042, grad_fn=<NegBackward0>)\n",
      "epoch 3584 loss tensor(6.5041, grad_fn=<NegBackward0>)\n",
      "epoch 3585 loss tensor(6.5039, grad_fn=<NegBackward0>)\n",
      "epoch 3586 loss tensor(6.5038, grad_fn=<NegBackward0>)\n",
      "epoch 3587 loss tensor(6.5037, grad_fn=<NegBackward0>)\n",
      "epoch 3588 loss tensor(6.5035, grad_fn=<NegBackward0>)\n",
      "epoch 3589 loss tensor(6.5034, grad_fn=<NegBackward0>)\n",
      "epoch 3590 loss tensor(6.5032, grad_fn=<NegBackward0>)\n",
      "epoch 3591 loss tensor(6.5031, grad_fn=<NegBackward0>)\n",
      "epoch 3592 loss tensor(6.5030, grad_fn=<NegBackward0>)\n",
      "epoch 3593 loss tensor(6.5028, grad_fn=<NegBackward0>)\n",
      "epoch 3594 loss tensor(6.5027, grad_fn=<NegBackward0>)\n",
      "epoch 3595 loss tensor(6.5026, grad_fn=<NegBackward0>)\n",
      "epoch 3596 loss tensor(6.5024, grad_fn=<NegBackward0>)\n",
      "epoch 3597 loss tensor(6.5023, grad_fn=<NegBackward0>)\n",
      "epoch 3598 loss tensor(6.5022, grad_fn=<NegBackward0>)\n",
      "epoch 3599 loss tensor(6.5020, grad_fn=<NegBackward0>)\n",
      "epoch 3600 loss tensor(6.5019, grad_fn=<NegBackward0>)\n",
      "epoch 3601 loss tensor(6.5017, grad_fn=<NegBackward0>)\n",
      "epoch 3602 loss tensor(6.5016, grad_fn=<NegBackward0>)\n",
      "epoch 3603 loss tensor(6.5015, grad_fn=<NegBackward0>)\n",
      "epoch 3604 loss tensor(6.5013, grad_fn=<NegBackward0>)\n",
      "epoch 3605 loss tensor(6.5012, grad_fn=<NegBackward0>)\n",
      "epoch 3606 loss tensor(6.5011, grad_fn=<NegBackward0>)\n",
      "epoch 3607 loss tensor(6.5009, grad_fn=<NegBackward0>)\n",
      "epoch 3608 loss tensor(6.5008, grad_fn=<NegBackward0>)\n",
      "epoch 3609 loss tensor(6.5006, grad_fn=<NegBackward0>)\n",
      "epoch 3610 loss tensor(6.5005, grad_fn=<NegBackward0>)\n",
      "epoch 3611 loss tensor(6.5004, grad_fn=<NegBackward0>)\n",
      "epoch 3612 loss tensor(6.5002, grad_fn=<NegBackward0>)\n",
      "epoch 3613 loss tensor(6.5001, grad_fn=<NegBackward0>)\n",
      "epoch 3614 loss tensor(6.5000, grad_fn=<NegBackward0>)\n",
      "epoch 3615 loss tensor(6.4998, grad_fn=<NegBackward0>)\n",
      "epoch 3616 loss tensor(6.4997, grad_fn=<NegBackward0>)\n",
      "epoch 3617 loss tensor(6.4996, grad_fn=<NegBackward0>)\n",
      "epoch 3618 loss tensor(6.4994, grad_fn=<NegBackward0>)\n",
      "epoch 3619 loss tensor(6.4993, grad_fn=<NegBackward0>)\n",
      "epoch 3620 loss tensor(6.4992, grad_fn=<NegBackward0>)\n",
      "epoch 3621 loss tensor(6.4990, grad_fn=<NegBackward0>)\n",
      "epoch 3622 loss tensor(6.4989, grad_fn=<NegBackward0>)\n",
      "epoch 3623 loss tensor(6.4987, grad_fn=<NegBackward0>)\n",
      "epoch 3624 loss tensor(6.4986, grad_fn=<NegBackward0>)\n",
      "epoch 3625 loss tensor(6.4985, grad_fn=<NegBackward0>)\n",
      "epoch 3626 loss tensor(6.4983, grad_fn=<NegBackward0>)\n",
      "epoch 3627 loss tensor(6.4982, grad_fn=<NegBackward0>)\n",
      "epoch 3628 loss tensor(6.4981, grad_fn=<NegBackward0>)\n",
      "epoch 3629 loss tensor(6.4979, grad_fn=<NegBackward0>)\n",
      "epoch 3630 loss tensor(6.4978, grad_fn=<NegBackward0>)\n",
      "epoch 3631 loss tensor(6.4977, grad_fn=<NegBackward0>)\n",
      "epoch 3632 loss tensor(6.4975, grad_fn=<NegBackward0>)\n",
      "epoch 3633 loss tensor(6.4974, grad_fn=<NegBackward0>)\n",
      "epoch 3634 loss tensor(6.4973, grad_fn=<NegBackward0>)\n",
      "epoch 3635 loss tensor(6.4971, grad_fn=<NegBackward0>)\n",
      "epoch 3636 loss tensor(6.4970, grad_fn=<NegBackward0>)\n",
      "epoch 3637 loss tensor(6.4969, grad_fn=<NegBackward0>)\n",
      "epoch 3638 loss tensor(6.4967, grad_fn=<NegBackward0>)\n",
      "epoch 3639 loss tensor(6.4966, grad_fn=<NegBackward0>)\n",
      "epoch 3640 loss tensor(6.4964, grad_fn=<NegBackward0>)\n",
      "epoch 3641 loss tensor(6.4963, grad_fn=<NegBackward0>)\n",
      "epoch 3642 loss tensor(6.4962, grad_fn=<NegBackward0>)\n",
      "epoch 3643 loss tensor(6.4960, grad_fn=<NegBackward0>)\n",
      "epoch 3644 loss tensor(6.4959, grad_fn=<NegBackward0>)\n",
      "epoch 3645 loss tensor(6.4958, grad_fn=<NegBackward0>)\n",
      "epoch 3646 loss tensor(6.4956, grad_fn=<NegBackward0>)\n",
      "epoch 3647 loss tensor(6.4955, grad_fn=<NegBackward0>)\n",
      "epoch 3648 loss tensor(6.4954, grad_fn=<NegBackward0>)\n",
      "epoch 3649 loss tensor(6.4952, grad_fn=<NegBackward0>)\n",
      "epoch 3650 loss tensor(6.4951, grad_fn=<NegBackward0>)\n",
      "epoch 3651 loss tensor(6.4950, grad_fn=<NegBackward0>)\n",
      "epoch 3652 loss tensor(6.4948, grad_fn=<NegBackward0>)\n",
      "epoch 3653 loss tensor(6.4947, grad_fn=<NegBackward0>)\n",
      "epoch 3654 loss tensor(6.4946, grad_fn=<NegBackward0>)\n",
      "epoch 3655 loss tensor(6.4944, grad_fn=<NegBackward0>)\n",
      "epoch 3656 loss tensor(6.4943, grad_fn=<NegBackward0>)\n",
      "epoch 3657 loss tensor(6.4942, grad_fn=<NegBackward0>)\n",
      "epoch 3658 loss tensor(6.4940, grad_fn=<NegBackward0>)\n",
      "epoch 3659 loss tensor(6.4939, grad_fn=<NegBackward0>)\n",
      "epoch 3660 loss tensor(6.4938, grad_fn=<NegBackward0>)\n",
      "epoch 3661 loss tensor(6.4936, grad_fn=<NegBackward0>)\n",
      "epoch 3662 loss tensor(6.4935, grad_fn=<NegBackward0>)\n",
      "epoch 3663 loss tensor(6.4934, grad_fn=<NegBackward0>)\n",
      "epoch 3664 loss tensor(6.4932, grad_fn=<NegBackward0>)\n",
      "epoch 3665 loss tensor(6.4931, grad_fn=<NegBackward0>)\n",
      "epoch 3666 loss tensor(6.4930, grad_fn=<NegBackward0>)\n",
      "epoch 3667 loss tensor(6.4928, grad_fn=<NegBackward0>)\n",
      "epoch 3668 loss tensor(6.4927, grad_fn=<NegBackward0>)\n",
      "epoch 3669 loss tensor(6.4926, grad_fn=<NegBackward0>)\n",
      "epoch 3670 loss tensor(6.4924, grad_fn=<NegBackward0>)\n",
      "epoch 3671 loss tensor(6.4923, grad_fn=<NegBackward0>)\n",
      "epoch 3672 loss tensor(6.4922, grad_fn=<NegBackward0>)\n",
      "epoch 3673 loss tensor(6.4920, grad_fn=<NegBackward0>)\n",
      "epoch 3674 loss tensor(6.4919, grad_fn=<NegBackward0>)\n",
      "epoch 3675 loss tensor(6.4918, grad_fn=<NegBackward0>)\n",
      "epoch 3676 loss tensor(6.4916, grad_fn=<NegBackward0>)\n",
      "epoch 3677 loss tensor(6.4915, grad_fn=<NegBackward0>)\n",
      "epoch 3678 loss tensor(6.4914, grad_fn=<NegBackward0>)\n",
      "epoch 3679 loss tensor(6.4912, grad_fn=<NegBackward0>)\n",
      "epoch 3680 loss tensor(6.4911, grad_fn=<NegBackward0>)\n",
      "epoch 3681 loss tensor(6.4910, grad_fn=<NegBackward0>)\n",
      "epoch 3682 loss tensor(6.4908, grad_fn=<NegBackward0>)\n",
      "epoch 3683 loss tensor(6.4907, grad_fn=<NegBackward0>)\n",
      "epoch 3684 loss tensor(6.4906, grad_fn=<NegBackward0>)\n",
      "epoch 3685 loss tensor(6.4904, grad_fn=<NegBackward0>)\n",
      "epoch 3686 loss tensor(6.4903, grad_fn=<NegBackward0>)\n",
      "epoch 3687 loss tensor(6.4902, grad_fn=<NegBackward0>)\n",
      "epoch 3688 loss tensor(6.4900, grad_fn=<NegBackward0>)\n",
      "epoch 3689 loss tensor(6.4899, grad_fn=<NegBackward0>)\n",
      "epoch 3690 loss tensor(6.4898, grad_fn=<NegBackward0>)\n",
      "epoch 3691 loss tensor(6.4896, grad_fn=<NegBackward0>)\n",
      "epoch 3692 loss tensor(6.4895, grad_fn=<NegBackward0>)\n",
      "epoch 3693 loss tensor(6.4894, grad_fn=<NegBackward0>)\n",
      "epoch 3694 loss tensor(6.4892, grad_fn=<NegBackward0>)\n",
      "epoch 3695 loss tensor(6.4891, grad_fn=<NegBackward0>)\n",
      "epoch 3696 loss tensor(6.4890, grad_fn=<NegBackward0>)\n",
      "epoch 3697 loss tensor(6.4888, grad_fn=<NegBackward0>)\n",
      "epoch 3698 loss tensor(6.4887, grad_fn=<NegBackward0>)\n",
      "epoch 3699 loss tensor(6.4886, grad_fn=<NegBackward0>)\n",
      "epoch 3700 loss tensor(6.4884, grad_fn=<NegBackward0>)\n",
      "epoch 3701 loss tensor(6.4883, grad_fn=<NegBackward0>)\n",
      "epoch 3702 loss tensor(6.4882, grad_fn=<NegBackward0>)\n",
      "epoch 3703 loss tensor(6.4880, grad_fn=<NegBackward0>)\n",
      "epoch 3704 loss tensor(6.4879, grad_fn=<NegBackward0>)\n",
      "epoch 3705 loss tensor(6.4878, grad_fn=<NegBackward0>)\n",
      "epoch 3706 loss tensor(6.4876, grad_fn=<NegBackward0>)\n",
      "epoch 3707 loss tensor(6.4875, grad_fn=<NegBackward0>)\n",
      "epoch 3708 loss tensor(6.4874, grad_fn=<NegBackward0>)\n",
      "epoch 3709 loss tensor(6.4872, grad_fn=<NegBackward0>)\n",
      "epoch 3710 loss tensor(6.4871, grad_fn=<NegBackward0>)\n",
      "epoch 3711 loss tensor(6.4870, grad_fn=<NegBackward0>)\n",
      "epoch 3712 loss tensor(6.4868, grad_fn=<NegBackward0>)\n",
      "epoch 3713 loss tensor(6.4867, grad_fn=<NegBackward0>)\n",
      "epoch 3714 loss tensor(6.4866, grad_fn=<NegBackward0>)\n",
      "epoch 3715 loss tensor(6.4865, grad_fn=<NegBackward0>)\n",
      "epoch 3716 loss tensor(6.4863, grad_fn=<NegBackward0>)\n",
      "epoch 3717 loss tensor(6.4862, grad_fn=<NegBackward0>)\n",
      "epoch 3718 loss tensor(6.4861, grad_fn=<NegBackward0>)\n",
      "epoch 3719 loss tensor(6.4859, grad_fn=<NegBackward0>)\n",
      "epoch 3720 loss tensor(6.4858, grad_fn=<NegBackward0>)\n",
      "epoch 3721 loss tensor(6.4857, grad_fn=<NegBackward0>)\n",
      "epoch 3722 loss tensor(6.4855, grad_fn=<NegBackward0>)\n",
      "epoch 3723 loss tensor(6.4854, grad_fn=<NegBackward0>)\n",
      "epoch 3724 loss tensor(6.4853, grad_fn=<NegBackward0>)\n",
      "epoch 3725 loss tensor(6.4851, grad_fn=<NegBackward0>)\n",
      "epoch 3726 loss tensor(6.4850, grad_fn=<NegBackward0>)\n",
      "epoch 3727 loss tensor(6.4849, grad_fn=<NegBackward0>)\n",
      "epoch 3728 loss tensor(6.4847, grad_fn=<NegBackward0>)\n",
      "epoch 3729 loss tensor(6.4846, grad_fn=<NegBackward0>)\n",
      "epoch 3730 loss tensor(6.4845, grad_fn=<NegBackward0>)\n",
      "epoch 3731 loss tensor(6.4844, grad_fn=<NegBackward0>)\n",
      "epoch 3732 loss tensor(6.4842, grad_fn=<NegBackward0>)\n",
      "epoch 3733 loss tensor(6.4841, grad_fn=<NegBackward0>)\n",
      "epoch 3734 loss tensor(6.4840, grad_fn=<NegBackward0>)\n",
      "epoch 3735 loss tensor(6.4838, grad_fn=<NegBackward0>)\n",
      "epoch 3736 loss tensor(6.4837, grad_fn=<NegBackward0>)\n",
      "epoch 3737 loss tensor(6.4836, grad_fn=<NegBackward0>)\n",
      "epoch 3738 loss tensor(6.4834, grad_fn=<NegBackward0>)\n",
      "epoch 3739 loss tensor(6.4833, grad_fn=<NegBackward0>)\n",
      "epoch 3740 loss tensor(6.4832, grad_fn=<NegBackward0>)\n",
      "epoch 3741 loss tensor(6.4830, grad_fn=<NegBackward0>)\n",
      "epoch 3742 loss tensor(6.4829, grad_fn=<NegBackward0>)\n",
      "epoch 3743 loss tensor(6.4828, grad_fn=<NegBackward0>)\n",
      "epoch 3744 loss tensor(6.4827, grad_fn=<NegBackward0>)\n",
      "epoch 3745 loss tensor(6.4825, grad_fn=<NegBackward0>)\n",
      "epoch 3746 loss tensor(6.4824, grad_fn=<NegBackward0>)\n",
      "epoch 3747 loss tensor(6.4823, grad_fn=<NegBackward0>)\n",
      "epoch 3748 loss tensor(6.4821, grad_fn=<NegBackward0>)\n",
      "epoch 3749 loss tensor(6.4820, grad_fn=<NegBackward0>)\n",
      "epoch 3750 loss tensor(6.4819, grad_fn=<NegBackward0>)\n",
      "epoch 3751 loss tensor(6.4817, grad_fn=<NegBackward0>)\n",
      "epoch 3752 loss tensor(6.4816, grad_fn=<NegBackward0>)\n",
      "epoch 3753 loss tensor(6.4815, grad_fn=<NegBackward0>)\n",
      "epoch 3754 loss tensor(6.4813, grad_fn=<NegBackward0>)\n",
      "epoch 3755 loss tensor(6.4812, grad_fn=<NegBackward0>)\n",
      "epoch 3756 loss tensor(6.4811, grad_fn=<NegBackward0>)\n",
      "epoch 3757 loss tensor(6.4810, grad_fn=<NegBackward0>)\n",
      "epoch 3758 loss tensor(6.4808, grad_fn=<NegBackward0>)\n",
      "epoch 3759 loss tensor(6.4807, grad_fn=<NegBackward0>)\n",
      "epoch 3760 loss tensor(6.4806, grad_fn=<NegBackward0>)\n",
      "epoch 3761 loss tensor(6.4804, grad_fn=<NegBackward0>)\n",
      "epoch 3762 loss tensor(6.4803, grad_fn=<NegBackward0>)\n",
      "epoch 3763 loss tensor(6.4802, grad_fn=<NegBackward0>)\n",
      "epoch 3764 loss tensor(6.4801, grad_fn=<NegBackward0>)\n",
      "epoch 3765 loss tensor(6.4799, grad_fn=<NegBackward0>)\n",
      "epoch 3766 loss tensor(6.4798, grad_fn=<NegBackward0>)\n",
      "epoch 3767 loss tensor(6.4797, grad_fn=<NegBackward0>)\n",
      "epoch 3768 loss tensor(6.4795, grad_fn=<NegBackward0>)\n",
      "epoch 3769 loss tensor(6.4794, grad_fn=<NegBackward0>)\n",
      "epoch 3770 loss tensor(6.4793, grad_fn=<NegBackward0>)\n",
      "epoch 3771 loss tensor(6.4791, grad_fn=<NegBackward0>)\n",
      "epoch 3772 loss tensor(6.4790, grad_fn=<NegBackward0>)\n",
      "epoch 3773 loss tensor(6.4789, grad_fn=<NegBackward0>)\n",
      "epoch 3774 loss tensor(6.4788, grad_fn=<NegBackward0>)\n",
      "epoch 3775 loss tensor(6.4786, grad_fn=<NegBackward0>)\n",
      "epoch 3776 loss tensor(6.4785, grad_fn=<NegBackward0>)\n",
      "epoch 3777 loss tensor(6.4784, grad_fn=<NegBackward0>)\n",
      "epoch 3778 loss tensor(6.4782, grad_fn=<NegBackward0>)\n",
      "epoch 3779 loss tensor(6.4781, grad_fn=<NegBackward0>)\n",
      "epoch 3780 loss tensor(6.4780, grad_fn=<NegBackward0>)\n",
      "epoch 3781 loss tensor(6.4779, grad_fn=<NegBackward0>)\n",
      "epoch 3782 loss tensor(6.4777, grad_fn=<NegBackward0>)\n",
      "epoch 3783 loss tensor(6.4776, grad_fn=<NegBackward0>)\n",
      "epoch 3784 loss tensor(6.4775, grad_fn=<NegBackward0>)\n",
      "epoch 3785 loss tensor(6.4773, grad_fn=<NegBackward0>)\n",
      "epoch 3786 loss tensor(6.4772, grad_fn=<NegBackward0>)\n",
      "epoch 3787 loss tensor(6.4771, grad_fn=<NegBackward0>)\n",
      "epoch 3788 loss tensor(6.4770, grad_fn=<NegBackward0>)\n",
      "epoch 3789 loss tensor(6.4768, grad_fn=<NegBackward0>)\n",
      "epoch 3790 loss tensor(6.4767, grad_fn=<NegBackward0>)\n",
      "epoch 3791 loss tensor(6.4766, grad_fn=<NegBackward0>)\n",
      "epoch 3792 loss tensor(6.4764, grad_fn=<NegBackward0>)\n",
      "epoch 3793 loss tensor(6.4763, grad_fn=<NegBackward0>)\n",
      "epoch 3794 loss tensor(6.4762, grad_fn=<NegBackward0>)\n",
      "epoch 3795 loss tensor(6.4761, grad_fn=<NegBackward0>)\n",
      "epoch 3796 loss tensor(6.4759, grad_fn=<NegBackward0>)\n",
      "epoch 3797 loss tensor(6.4758, grad_fn=<NegBackward0>)\n",
      "epoch 3798 loss tensor(6.4757, grad_fn=<NegBackward0>)\n",
      "epoch 3799 loss tensor(6.4755, grad_fn=<NegBackward0>)\n",
      "epoch 3800 loss tensor(6.4754, grad_fn=<NegBackward0>)\n",
      "epoch 3801 loss tensor(6.4753, grad_fn=<NegBackward0>)\n",
      "epoch 3802 loss tensor(6.4752, grad_fn=<NegBackward0>)\n",
      "epoch 3803 loss tensor(6.4750, grad_fn=<NegBackward0>)\n",
      "epoch 3804 loss tensor(6.4749, grad_fn=<NegBackward0>)\n",
      "epoch 3805 loss tensor(6.4748, grad_fn=<NegBackward0>)\n",
      "epoch 3806 loss tensor(6.4746, grad_fn=<NegBackward0>)\n",
      "epoch 3807 loss tensor(6.4745, grad_fn=<NegBackward0>)\n",
      "epoch 3808 loss tensor(6.4744, grad_fn=<NegBackward0>)\n",
      "epoch 3809 loss tensor(6.4743, grad_fn=<NegBackward0>)\n",
      "epoch 3810 loss tensor(6.4741, grad_fn=<NegBackward0>)\n",
      "epoch 3811 loss tensor(6.4740, grad_fn=<NegBackward0>)\n",
      "epoch 3812 loss tensor(6.4739, grad_fn=<NegBackward0>)\n",
      "epoch 3813 loss tensor(6.4738, grad_fn=<NegBackward0>)\n",
      "epoch 3814 loss tensor(6.4736, grad_fn=<NegBackward0>)\n",
      "epoch 3815 loss tensor(6.4735, grad_fn=<NegBackward0>)\n",
      "epoch 3816 loss tensor(6.4734, grad_fn=<NegBackward0>)\n",
      "epoch 3817 loss tensor(6.4732, grad_fn=<NegBackward0>)\n",
      "epoch 3818 loss tensor(6.4731, grad_fn=<NegBackward0>)\n",
      "epoch 3819 loss tensor(6.4730, grad_fn=<NegBackward0>)\n",
      "epoch 3820 loss tensor(6.4729, grad_fn=<NegBackward0>)\n",
      "epoch 3821 loss tensor(6.4727, grad_fn=<NegBackward0>)\n",
      "epoch 3822 loss tensor(6.4726, grad_fn=<NegBackward0>)\n",
      "epoch 3823 loss tensor(6.4725, grad_fn=<NegBackward0>)\n",
      "epoch 3824 loss tensor(6.4723, grad_fn=<NegBackward0>)\n",
      "epoch 3825 loss tensor(6.4722, grad_fn=<NegBackward0>)\n",
      "epoch 3826 loss tensor(6.4721, grad_fn=<NegBackward0>)\n",
      "epoch 3827 loss tensor(6.4720, grad_fn=<NegBackward0>)\n",
      "epoch 3828 loss tensor(6.4718, grad_fn=<NegBackward0>)\n",
      "epoch 3829 loss tensor(6.4717, grad_fn=<NegBackward0>)\n",
      "epoch 3830 loss tensor(6.4716, grad_fn=<NegBackward0>)\n",
      "epoch 3831 loss tensor(6.4715, grad_fn=<NegBackward0>)\n",
      "epoch 3832 loss tensor(6.4713, grad_fn=<NegBackward0>)\n",
      "epoch 3833 loss tensor(6.4712, grad_fn=<NegBackward0>)\n",
      "epoch 3834 loss tensor(6.4711, grad_fn=<NegBackward0>)\n",
      "epoch 3835 loss tensor(6.4710, grad_fn=<NegBackward0>)\n",
      "epoch 3836 loss tensor(6.4708, grad_fn=<NegBackward0>)\n",
      "epoch 3837 loss tensor(6.4707, grad_fn=<NegBackward0>)\n",
      "epoch 3838 loss tensor(6.4706, grad_fn=<NegBackward0>)\n",
      "epoch 3839 loss tensor(6.4704, grad_fn=<NegBackward0>)\n",
      "epoch 3840 loss tensor(6.4703, grad_fn=<NegBackward0>)\n",
      "epoch 3841 loss tensor(6.4702, grad_fn=<NegBackward0>)\n",
      "epoch 3842 loss tensor(6.4701, grad_fn=<NegBackward0>)\n",
      "epoch 3843 loss tensor(6.4699, grad_fn=<NegBackward0>)\n",
      "epoch 3844 loss tensor(6.4698, grad_fn=<NegBackward0>)\n",
      "epoch 3845 loss tensor(6.4697, grad_fn=<NegBackward0>)\n",
      "epoch 3846 loss tensor(6.4696, grad_fn=<NegBackward0>)\n",
      "epoch 3847 loss tensor(6.4694, grad_fn=<NegBackward0>)\n",
      "epoch 3848 loss tensor(6.4693, grad_fn=<NegBackward0>)\n",
      "epoch 3849 loss tensor(6.4692, grad_fn=<NegBackward0>)\n",
      "epoch 3850 loss tensor(6.4691, grad_fn=<NegBackward0>)\n",
      "epoch 3851 loss tensor(6.4689, grad_fn=<NegBackward0>)\n",
      "epoch 3852 loss tensor(6.4688, grad_fn=<NegBackward0>)\n",
      "epoch 3853 loss tensor(6.4687, grad_fn=<NegBackward0>)\n",
      "epoch 3854 loss tensor(6.4686, grad_fn=<NegBackward0>)\n",
      "epoch 3855 loss tensor(6.4684, grad_fn=<NegBackward0>)\n",
      "epoch 3856 loss tensor(6.4683, grad_fn=<NegBackward0>)\n",
      "epoch 3857 loss tensor(6.4682, grad_fn=<NegBackward0>)\n",
      "epoch 3858 loss tensor(6.4681, grad_fn=<NegBackward0>)\n",
      "epoch 3859 loss tensor(6.4679, grad_fn=<NegBackward0>)\n",
      "epoch 3860 loss tensor(6.4678, grad_fn=<NegBackward0>)\n",
      "epoch 3861 loss tensor(6.4677, grad_fn=<NegBackward0>)\n",
      "epoch 3862 loss tensor(6.4675, grad_fn=<NegBackward0>)\n",
      "epoch 3863 loss tensor(6.4674, grad_fn=<NegBackward0>)\n",
      "epoch 3864 loss tensor(6.4673, grad_fn=<NegBackward0>)\n",
      "epoch 3865 loss tensor(6.4672, grad_fn=<NegBackward0>)\n",
      "epoch 3866 loss tensor(6.4670, grad_fn=<NegBackward0>)\n",
      "epoch 3867 loss tensor(6.4669, grad_fn=<NegBackward0>)\n",
      "epoch 3868 loss tensor(6.4668, grad_fn=<NegBackward0>)\n",
      "epoch 3869 loss tensor(6.4667, grad_fn=<NegBackward0>)\n",
      "epoch 3870 loss tensor(6.4665, grad_fn=<NegBackward0>)\n",
      "epoch 3871 loss tensor(6.4664, grad_fn=<NegBackward0>)\n",
      "epoch 3872 loss tensor(6.4663, grad_fn=<NegBackward0>)\n",
      "epoch 3873 loss tensor(6.4662, grad_fn=<NegBackward0>)\n",
      "epoch 3874 loss tensor(6.4660, grad_fn=<NegBackward0>)\n",
      "epoch 3875 loss tensor(6.4659, grad_fn=<NegBackward0>)\n",
      "epoch 3876 loss tensor(6.4658, grad_fn=<NegBackward0>)\n",
      "epoch 3877 loss tensor(6.4657, grad_fn=<NegBackward0>)\n",
      "epoch 3878 loss tensor(6.4655, grad_fn=<NegBackward0>)\n",
      "epoch 3879 loss tensor(6.4654, grad_fn=<NegBackward0>)\n",
      "epoch 3880 loss tensor(6.4653, grad_fn=<NegBackward0>)\n",
      "epoch 3881 loss tensor(6.4652, grad_fn=<NegBackward0>)\n",
      "epoch 3882 loss tensor(6.4650, grad_fn=<NegBackward0>)\n",
      "epoch 3883 loss tensor(6.4649, grad_fn=<NegBackward0>)\n",
      "epoch 3884 loss tensor(6.4648, grad_fn=<NegBackward0>)\n",
      "epoch 3885 loss tensor(6.4647, grad_fn=<NegBackward0>)\n",
      "epoch 3886 loss tensor(6.4645, grad_fn=<NegBackward0>)\n",
      "epoch 3887 loss tensor(6.4644, grad_fn=<NegBackward0>)\n",
      "epoch 3888 loss tensor(6.4643, grad_fn=<NegBackward0>)\n",
      "epoch 3889 loss tensor(6.4642, grad_fn=<NegBackward0>)\n",
      "epoch 3890 loss tensor(6.4640, grad_fn=<NegBackward0>)\n",
      "epoch 3891 loss tensor(6.4639, grad_fn=<NegBackward0>)\n",
      "epoch 3892 loss tensor(6.4638, grad_fn=<NegBackward0>)\n",
      "epoch 3893 loss tensor(6.4637, grad_fn=<NegBackward0>)\n",
      "epoch 3894 loss tensor(6.4636, grad_fn=<NegBackward0>)\n",
      "epoch 3895 loss tensor(6.4634, grad_fn=<NegBackward0>)\n",
      "epoch 3896 loss tensor(6.4633, grad_fn=<NegBackward0>)\n",
      "epoch 3897 loss tensor(6.4632, grad_fn=<NegBackward0>)\n",
      "epoch 3898 loss tensor(6.4631, grad_fn=<NegBackward0>)\n",
      "epoch 3899 loss tensor(6.4629, grad_fn=<NegBackward0>)\n",
      "epoch 3900 loss tensor(6.4628, grad_fn=<NegBackward0>)\n",
      "epoch 3901 loss tensor(6.4627, grad_fn=<NegBackward0>)\n",
      "epoch 3902 loss tensor(6.4626, grad_fn=<NegBackward0>)\n",
      "epoch 3903 loss tensor(6.4624, grad_fn=<NegBackward0>)\n",
      "epoch 3904 loss tensor(6.4623, grad_fn=<NegBackward0>)\n",
      "epoch 3905 loss tensor(6.4622, grad_fn=<NegBackward0>)\n",
      "epoch 3906 loss tensor(6.4621, grad_fn=<NegBackward0>)\n",
      "epoch 3907 loss tensor(6.4619, grad_fn=<NegBackward0>)\n",
      "epoch 3908 loss tensor(6.4618, grad_fn=<NegBackward0>)\n",
      "epoch 3909 loss tensor(6.4617, grad_fn=<NegBackward0>)\n",
      "epoch 3910 loss tensor(6.4616, grad_fn=<NegBackward0>)\n",
      "epoch 3911 loss tensor(6.4614, grad_fn=<NegBackward0>)\n",
      "epoch 3912 loss tensor(6.4613, grad_fn=<NegBackward0>)\n",
      "epoch 3913 loss tensor(6.4612, grad_fn=<NegBackward0>)\n",
      "epoch 3914 loss tensor(6.4611, grad_fn=<NegBackward0>)\n",
      "epoch 3915 loss tensor(6.4609, grad_fn=<NegBackward0>)\n",
      "epoch 3916 loss tensor(6.4608, grad_fn=<NegBackward0>)\n",
      "epoch 3917 loss tensor(6.4607, grad_fn=<NegBackward0>)\n",
      "epoch 3918 loss tensor(6.4606, grad_fn=<NegBackward0>)\n",
      "epoch 3919 loss tensor(6.4605, grad_fn=<NegBackward0>)\n",
      "epoch 3920 loss tensor(6.4603, grad_fn=<NegBackward0>)\n",
      "epoch 3921 loss tensor(6.4602, grad_fn=<NegBackward0>)\n",
      "epoch 3922 loss tensor(6.4601, grad_fn=<NegBackward0>)\n",
      "epoch 3923 loss tensor(6.4600, grad_fn=<NegBackward0>)\n",
      "epoch 3924 loss tensor(6.4598, grad_fn=<NegBackward0>)\n",
      "epoch 3925 loss tensor(6.4597, grad_fn=<NegBackward0>)\n",
      "epoch 3926 loss tensor(6.4596, grad_fn=<NegBackward0>)\n",
      "epoch 3927 loss tensor(6.4595, grad_fn=<NegBackward0>)\n",
      "epoch 3928 loss tensor(6.4593, grad_fn=<NegBackward0>)\n",
      "epoch 3929 loss tensor(6.4592, grad_fn=<NegBackward0>)\n",
      "epoch 3930 loss tensor(6.4591, grad_fn=<NegBackward0>)\n",
      "epoch 3931 loss tensor(6.4590, grad_fn=<NegBackward0>)\n",
      "epoch 3932 loss tensor(6.4589, grad_fn=<NegBackward0>)\n",
      "epoch 3933 loss tensor(6.4587, grad_fn=<NegBackward0>)\n",
      "epoch 3934 loss tensor(6.4586, grad_fn=<NegBackward0>)\n",
      "epoch 3935 loss tensor(6.4585, grad_fn=<NegBackward0>)\n",
      "epoch 3936 loss tensor(6.4584, grad_fn=<NegBackward0>)\n",
      "epoch 3937 loss tensor(6.4582, grad_fn=<NegBackward0>)\n",
      "epoch 3938 loss tensor(6.4581, grad_fn=<NegBackward0>)\n",
      "epoch 3939 loss tensor(6.4580, grad_fn=<NegBackward0>)\n",
      "epoch 3940 loss tensor(6.4579, grad_fn=<NegBackward0>)\n",
      "epoch 3941 loss tensor(6.4578, grad_fn=<NegBackward0>)\n",
      "epoch 3942 loss tensor(6.4576, grad_fn=<NegBackward0>)\n",
      "epoch 3943 loss tensor(6.4575, grad_fn=<NegBackward0>)\n",
      "epoch 3944 loss tensor(6.4574, grad_fn=<NegBackward0>)\n",
      "epoch 3945 loss tensor(6.4573, grad_fn=<NegBackward0>)\n",
      "epoch 3946 loss tensor(6.4571, grad_fn=<NegBackward0>)\n",
      "epoch 3947 loss tensor(6.4570, grad_fn=<NegBackward0>)\n",
      "epoch 3948 loss tensor(6.4569, grad_fn=<NegBackward0>)\n",
      "epoch 3949 loss tensor(6.4568, grad_fn=<NegBackward0>)\n",
      "epoch 3950 loss tensor(6.4567, grad_fn=<NegBackward0>)\n",
      "epoch 3951 loss tensor(6.4565, grad_fn=<NegBackward0>)\n",
      "epoch 3952 loss tensor(6.4564, grad_fn=<NegBackward0>)\n",
      "epoch 3953 loss tensor(6.4563, grad_fn=<NegBackward0>)\n",
      "epoch 3954 loss tensor(6.4562, grad_fn=<NegBackward0>)\n",
      "epoch 3955 loss tensor(6.4560, grad_fn=<NegBackward0>)\n",
      "epoch 3956 loss tensor(6.4559, grad_fn=<NegBackward0>)\n",
      "epoch 3957 loss tensor(6.4558, grad_fn=<NegBackward0>)\n",
      "epoch 3958 loss tensor(6.4557, grad_fn=<NegBackward0>)\n",
      "epoch 3959 loss tensor(6.4556, grad_fn=<NegBackward0>)\n",
      "epoch 3960 loss tensor(6.4554, grad_fn=<NegBackward0>)\n",
      "epoch 3961 loss tensor(6.4553, grad_fn=<NegBackward0>)\n",
      "epoch 3962 loss tensor(6.4552, grad_fn=<NegBackward0>)\n",
      "epoch 3963 loss tensor(6.4551, grad_fn=<NegBackward0>)\n",
      "epoch 3964 loss tensor(6.4550, grad_fn=<NegBackward0>)\n",
      "epoch 3965 loss tensor(6.4548, grad_fn=<NegBackward0>)\n",
      "epoch 3966 loss tensor(6.4547, grad_fn=<NegBackward0>)\n",
      "epoch 3967 loss tensor(6.4546, grad_fn=<NegBackward0>)\n",
      "epoch 3968 loss tensor(6.4545, grad_fn=<NegBackward0>)\n",
      "epoch 3969 loss tensor(6.4543, grad_fn=<NegBackward0>)\n",
      "epoch 3970 loss tensor(6.4542, grad_fn=<NegBackward0>)\n",
      "epoch 3971 loss tensor(6.4541, grad_fn=<NegBackward0>)\n",
      "epoch 3972 loss tensor(6.4540, grad_fn=<NegBackward0>)\n",
      "epoch 3973 loss tensor(6.4539, grad_fn=<NegBackward0>)\n",
      "epoch 3974 loss tensor(6.4537, grad_fn=<NegBackward0>)\n",
      "epoch 3975 loss tensor(6.4536, grad_fn=<NegBackward0>)\n",
      "epoch 3976 loss tensor(6.4535, grad_fn=<NegBackward0>)\n",
      "epoch 3977 loss tensor(6.4534, grad_fn=<NegBackward0>)\n",
      "epoch 3978 loss tensor(6.4533, grad_fn=<NegBackward0>)\n",
      "epoch 3979 loss tensor(6.4531, grad_fn=<NegBackward0>)\n",
      "epoch 3980 loss tensor(6.4530, grad_fn=<NegBackward0>)\n",
      "epoch 3981 loss tensor(6.4529, grad_fn=<NegBackward0>)\n",
      "epoch 3982 loss tensor(6.4528, grad_fn=<NegBackward0>)\n",
      "epoch 3983 loss tensor(6.4527, grad_fn=<NegBackward0>)\n",
      "epoch 3984 loss tensor(6.4525, grad_fn=<NegBackward0>)\n",
      "epoch 3985 loss tensor(6.4524, grad_fn=<NegBackward0>)\n",
      "epoch 3986 loss tensor(6.4523, grad_fn=<NegBackward0>)\n",
      "epoch 3987 loss tensor(6.4522, grad_fn=<NegBackward0>)\n",
      "epoch 3988 loss tensor(6.4521, grad_fn=<NegBackward0>)\n",
      "epoch 3989 loss tensor(6.4519, grad_fn=<NegBackward0>)\n",
      "epoch 3990 loss tensor(6.4518, grad_fn=<NegBackward0>)\n",
      "epoch 3991 loss tensor(6.4517, grad_fn=<NegBackward0>)\n",
      "epoch 3992 loss tensor(6.4516, grad_fn=<NegBackward0>)\n",
      "epoch 3993 loss tensor(6.4514, grad_fn=<NegBackward0>)\n",
      "epoch 3994 loss tensor(6.4513, grad_fn=<NegBackward0>)\n",
      "epoch 3995 loss tensor(6.4512, grad_fn=<NegBackward0>)\n",
      "epoch 3996 loss tensor(6.4511, grad_fn=<NegBackward0>)\n",
      "epoch 3997 loss tensor(6.4510, grad_fn=<NegBackward0>)\n",
      "epoch 3998 loss tensor(6.4508, grad_fn=<NegBackward0>)\n",
      "epoch 3999 loss tensor(6.4507, grad_fn=<NegBackward0>)\n",
      "epoch 4000 loss tensor(6.4506, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Create the flow\n",
    "flow = Flow(transformations, base_distribution) #encapsules the entire flow model in a more structured way\n",
    "\n",
    "#Training loop\n",
    "#The training loop remains similar,\n",
    "#but with flow.log_prob(y) directly calculating the log probability using the nflows implementation.\n",
    "opt = torch.optim.Adam(flow.parameters(), lr=5e-4)\n",
    "\n",
    "#print(base_distribution)\n",
    "#print(flow)\n",
    "\n",
    "last_loss = np.inf\n",
    "patience = 0\n",
    "n_epochs = 4001\n",
    "\n",
    "for idx in range(n_epochs):\n",
    "    opt.zero_grad()\n",
    "\n",
    "    # Minimize KL(p || q)\n",
    "    loss = -flow.log_prob(y).mean()\n",
    "\n",
    "    if idx % 1 == 0:\n",
    "        print('epoch', idx, 'loss', loss)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    # Early stopping based on patience\n",
    "    #patience mechanism keeps track of how many epochs have passed without improvement in loss. \n",
    "    #If the loss does not improve for 5 consecutive epochs, the training stops early to prevent overfitting.\n",
    "\n",
    "    if loss > last_loss:\n",
    "        patience += 1\n",
    "    if patience >= 5:\n",
    "        break\n",
    "    last_loss = loss\n",
    "\n",
    "    opt.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sampling: done using the provided methods from nflows\n",
    "# Sample points from the base distribution\n",
    "prior = base_distribution.sample(1000).numpy()  # Sample 1000 points with 2 features each\n",
    "\n",
    "# Sample points from the trained flow\n",
    "trained = flow.sample(1000).detach().numpy()  # Sample 1000 points with 2 features each\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(prior[:, 0], prior[:, 1], color='gray', label='Base/Prior distribution')\n",
    "plt.scatter(bkg_coord[:10000, 0], bkg_coord[:10000, 1], color='blue', label='Background/Target distribution')\n",
    "plt.scatter(trained[:, 0], trained[:, 1], color='green', label='Trained distribution')\n",
    "plt.xlabel(\"Latent b-tagging score\")\n",
    "plt.ylabel(\"Energy [GeV]\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
